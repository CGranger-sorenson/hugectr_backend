{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "008356b7",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Parameter server end-to-end sample (Training part)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a729106",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In HugeCTR version 3.3, we finished the whole pipeline of parameter server, including \n",
    "1. The parameter dumping interface from training to kafka.\n",
    "2. Redis as a level2 cache.\n",
    "3. RocksDB as a persistence storage.\n",
    "4. Embedding cache update mechanism.\n",
    "\n",
    "\n",
    "The purpose of this notebook is to give you a brief idea of how parameter server works in terms of the flow of parameters. \n",
    "\n",
    "## Table of Contents\n",
    "-  [Data Preparation](#1)\n",
    "-  [Data Preprocessing using NVTabular](#2)\n",
    "-  [Kafka broker start](#3)\n",
    "-  [Wide&Deep Continuous Training Demo](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c404238",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2958007b",
   "metadata": {},
   "source": [
    "1. Firstly, we made a folder to store our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7d6059",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir criteo_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc11c97",
   "metadata": {},
   "source": [
    "2. Download Criteo Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800c3ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://azuremlsampleexperiments.blob.core.windows.net/criteo/day_1.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330153e1",
   "metadata": {},
   "source": [
    "**NOTE**: Replace `1` with a value from [0, 23] to use a different day.\n",
    "\n",
    "During preprocessing, the amount of data, which is used to speed up the preprocessing, fill missing values, and remove the feature values that are considered rare, is further reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556eb24c",
   "metadata": {},
   "source": [
    "3. Thirdly, preprocess the data and split the data into 6 parts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5803485a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocess.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocess.sh\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "DST_DATA_DIR=$2\n",
    "\n",
    "echo \"Getting the first few examples from the uncompressed dataset...\"\n",
    "\n",
    "head -n 420000 day_$1 > $DST_DATA_DIR/day_$1_small\n",
    "if [ $? -ne 0 ]; then\n",
    "\techo \"Warning: fallback to find original compressed data day_$1.gz...\"\n",
    "\techo \"Decompressing day_$1.gz...\"\n",
    "\tgzip -d -c day_$1.gz > day_$1\n",
    "\tif [ $? -ne 0 ]; then\n",
    "\t\techo \"Error: failed to decompress the file.\"\n",
    "\t\texit 2\n",
    "\tfi\n",
    "\thead -n 420000 day_$1 > $DST_DATA_DIR/day_$1_small\n",
    "\tif [ $? -ne 0 ]; then\n",
    "\t\techo \"Error: day_$1 file\"\n",
    "\t\texit 2\n",
    "\tfi\n",
    "fi\n",
    "\n",
    "echo \"Counting the number of samples in day_$1 dataset...\"\n",
    "total_count=$(wc -l $DST_DATA_DIR/day_$1_small)\n",
    "total_count=(${total_count})\n",
    "echo \"The first $total_count examples will be used in day_$1 dataset.\"\n",
    "\n",
    "echo \"Shuffling dataset...\"\n",
    "shuf $DST_DATA_DIR/day_$1_small > $DST_DATA_DIR/day_$1_shuf\n",
    "\n",
    "echo \"Dividing data into 6 parts...\"\n",
    "cd $DST_DATA_DIR\n",
    "split -n l/6 day_$1_shuf\n",
    "\n",
    "echo \"Done!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2b5956",
   "metadata": {},
   "source": [
    "4. Run the preprocess script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bad7952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the first few examples from the uncompressed dataset...\n",
      "Counting the number of samples in day_1 dataset...\n",
      "The first 420000 examples will be used in day_1 dataset.\n",
      "Shuffling dataset...\n",
      "Dividing data into 6 parts...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "!bash preprocess.sh 1 criteo_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bccad7f",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTES**: \n",
    "- The first argument represents the dataset postfix.  For instance, if `day_1` is used, the postfix is `1`.\n",
    "- The second argument, `criteo_data`, is where the preprocessed data is stored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e65522",
   "metadata": {},
   "source": [
    "## Preprocess using NVTabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2ab3848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocess.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import concurrent.futures as cf\n",
    "from concurrent.futures import as_completed\n",
    "import shutil\n",
    "\n",
    "import dask_cudf\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "from dask.utils import parse_bytes\n",
    "from dask.delayed import delayed\n",
    "\n",
    "import cudf\n",
    "import rmm\n",
    "import nvtabular as nvt\n",
    "from nvtabular.io import Shuffle\n",
    "from nvtabular.utils import device_mem_size\n",
    "from nvtabular.ops import Categorify, Clip, FillMissing, HashBucket, LambdaOp, Normalize, Rename, Operator, get_embedding_sizes\n",
    "#%load_ext memory_profiler\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s %(message)s')\n",
    "logging.root.setLevel(logging.NOTSET)\n",
    "logging.getLogger('numba').setLevel(logging.WARNING)\n",
    "logging.getLogger('asyncio').setLevel(logging.WARNING)\n",
    "\n",
    "# define dataset schema\n",
    "CATEGORICAL_COLUMNS=[\"C\" + str(x) for x in range(1, 27)]\n",
    "CONTINUOUS_COLUMNS=[\"I\" + str(x) for x in range(1, 14)]\n",
    "LABEL_COLUMNS = ['label']\n",
    "COLUMNS =  LABEL_COLUMNS + CONTINUOUS_COLUMNS +  CATEGORICAL_COLUMNS\n",
    "#/samples/criteo mode doesn't have dense features\n",
    "criteo_COLUMN=LABEL_COLUMNS +  CATEGORICAL_COLUMNS\n",
    "#For new feature cross columns\n",
    "CROSS_COLUMNS = []\n",
    "\n",
    "\n",
    "NUM_INTEGER_COLUMNS = 13\n",
    "NUM_CATEGORICAL_COLUMNS = 26\n",
    "NUM_TOTAL_COLUMNS = 1 + NUM_INTEGER_COLUMNS + NUM_CATEGORICAL_COLUMNS\n",
    "\n",
    "\n",
    "# Initialize RMM pool on ALL workers\n",
    "def setup_rmm_pool(client, pool_size):\n",
    "    client.run(rmm.reinitialize, pool_allocator=True, initial_pool_size=pool_size)\n",
    "    return None\n",
    "\n",
    "#compute the partition size with GB\n",
    "def bytesto(bytes, to, bsize=1024):\n",
    "    a = {'k' : 1, 'm': 2, 'g' : 3, 't' : 4, 'p' : 5, 'e' : 6 }\n",
    "    r = float(bytes)\n",
    "    return bytes / (bsize ** a[to])\n",
    "\n",
    "class FeatureCross(Operator):\n",
    "    def __init__(self, dependency):\n",
    "        self.dependency = dependency\n",
    "\n",
    "    def transform(self, columns, gdf):\n",
    "        new_df = type(gdf)()\n",
    "        for col in columns:\n",
    "            new_df[col] = gdf[col] + gdf[self.dependency]\n",
    "        return new_df\n",
    "\n",
    "    def dependencies(self):\n",
    "        return [self.dependency]\n",
    "\n",
    "#process the data with NVTabular\n",
    "def process_NVT(args):\n",
    "\n",
    "    if args.feature_cross_list:\n",
    "        feature_pairs = [pair.split(\"_\") for pair in args.feature_cross_list.split(\",\")]\n",
    "        for pair in feature_pairs:\n",
    "            CROSS_COLUMNS.append(pair[0]+'_'+pair[1])\n",
    "\n",
    "\n",
    "    logging.info('NVTabular processing')\n",
    "    \n",
    "    ### TODO\n",
    "    files = [\"day_1_shuf\", \"xaa\", \"xab\", \"xac\", \"xad\", \"xae\", \"xaf\"]\n",
    "    inputs = [os.path.join(args.data_path, file) for file in files]\n",
    "\n",
    "    ## Get Dask Client\n",
    "\n",
    "    # Deploy a Single-Machine Multi-GPU Cluster\n",
    "    device_size = device_mem_size(kind=\"total\")\n",
    "    cluster = None\n",
    "    if args.protocol == \"ucx\":\n",
    "        UCX_TLS = os.environ.get(\"UCX_TLS\", \"tcp,cuda_copy,cuda_ipc,sockcm\")\n",
    "        os.environ[\"UCX_TLS\"] = UCX_TLS\n",
    "        cluster = LocalCUDACluster(\n",
    "            protocol = args.protocol,\n",
    "            CUDA_VISIBLE_DEVICES = args.devices,\n",
    "            n_workers = len(args.devices.split(\",\")),\n",
    "            enable_nvlink=True,\n",
    "            device_memory_limit = int(device_size * args.device_limit_frac),\n",
    "            dashboard_address=\":\" + args.dashboard_port\n",
    "        )\n",
    "    else:\n",
    "        cluster = LocalCUDACluster(\n",
    "            protocol = args.protocol,\n",
    "            n_workers = len(args.devices.split(\",\")),\n",
    "            CUDA_VISIBLE_DEVICES = args.devices,\n",
    "            device_memory_limit = int(device_size * args.device_limit_frac),\n",
    "            dashboard_address=\":\" + args.dashboard_port\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    # Create the distributed client\n",
    "    client = Client(cluster)\n",
    "    if args.device_pool_frac > 0.01:\n",
    "        setup_rmm_pool(client, int(args.device_pool_frac*device_size))\n",
    "\n",
    "\n",
    "    #calculate the total processing time\n",
    "    runtime = time.time()\n",
    "\n",
    "    #test dataset without the label feature\n",
    "    if args.dataset_type == 'test':\n",
    "        global LABEL_COLUMNS\n",
    "        LABEL_COLUMNS = []\n",
    "\n",
    "    ##-----------------------------------##\n",
    "    # Dask rapids converts txt to parquet\n",
    "    # Dask cudf dataframe = ddf\n",
    "\n",
    "    for i in inputs:\n",
    "\n",
    "        ddf = dask_cudf.read_csv(i, sep='\\t',names=LABEL_COLUMNS + CONTINUOUS_COLUMNS + CATEGORICAL_COLUMNS)\n",
    "\n",
    "        ## Convert label col to FP32\n",
    "        if args.parquet_format and args.dataset_type == 'train':\n",
    "            ddf[\"label\"] = ddf['label'].astype('float32')\n",
    "\n",
    "        # Save it as parquet format for better memory usage\n",
    "        ddf.to_parquet(i + \"_temp\", header=True)\n",
    "        ##-----------------------------------##\n",
    "\n",
    "    COLUMNS =  LABEL_COLUMNS + CONTINUOUS_COLUMNS + CROSS_COLUMNS + CATEGORICAL_COLUMNS\n",
    "    \n",
    "    \n",
    "    paths = glob.glob(os.path.join(args.data_path, \"*_temp\"))\n",
    "\n",
    "    categorify_op = Categorify(freq_threshold=args.freq_limit)\n",
    "    cat_features = CATEGORICAL_COLUMNS >> categorify_op\n",
    "    cont_features = CONTINUOUS_COLUMNS >> FillMissing() >> Clip(min_value=0) >> Normalize()\n",
    "    cross_cat_op = Categorify(freq_threshold=args.freq_limit)\n",
    "\n",
    "    features = LABEL_COLUMNS\n",
    "    \n",
    "    if args.criteo_mode == 0:\n",
    "        features += cont_features\n",
    "        if args.feature_cross_list:\n",
    "            feature_pairs = [pair.split(\"_\") for pair in args.feature_cross_list.split(\",\")]\n",
    "            for pair in feature_pairs:\n",
    "                col0 = pair[0]\n",
    "                col1 = pair[1]\n",
    "                features += col0 >> FeatureCross(col1)  >> Rename(postfix=\"_\"+col1) >> cross_cat_op\n",
    "            \n",
    "    features += cat_features\n",
    "\n",
    "    workflow = nvt.Workflow(features, client=client)\n",
    "\n",
    "    logging.info(\"Preprocessing\")\n",
    "\n",
    "    output_format = 'hugectr'\n",
    "    if args.parquet_format:\n",
    "        output_format = 'parquet'\n",
    "\n",
    "    # just for /samples/criteo model\n",
    "    iterators = []\n",
    "    for path in paths:\n",
    "        iterators.append(nvt.Dataset(path, engine='parquet', part_size=int(args.part_mem_frac * device_size)))\n",
    "\n",
    "    shuffle = None\n",
    "    if args.shuffle == \"PER_WORKER\":\n",
    "        shuffle = nvt.io.Shuffle.PER_WORKER\n",
    "    elif args.shuffle == \"PER_PARTITION\":\n",
    "        shuffle = nvt.io.Shuffle.PER_PARTITION\n",
    "\n",
    "    logging.info('Preprocessing.....')\n",
    "\n",
    "    dict_dtypes = {}\n",
    "    for col in CATEGORICAL_COLUMNS:\n",
    "        dict_dtypes[col] = np.int64\n",
    "    if not args.criteo_mode:\n",
    "        for col in CONTINUOUS_COLUMNS:\n",
    "            dict_dtypes[col] = np.float32\n",
    "    for col in CROSS_COLUMNS:\n",
    "        dict_dtypes[col] = np.int64\n",
    "    for col in LABEL_COLUMNS:\n",
    "        dict_dtypes[col] = np.float32\n",
    "    \n",
    "    conts = CONTINUOUS_COLUMNS if not args.criteo_mode else []\n",
    "    \n",
    "    workflow.fit(iterators[0])\n",
    "     \n",
    "    i=0\n",
    "    for iterator in iterators[1:]:\n",
    "        if output_format == 'hugectr':\n",
    "            workflow.transform(iterator).to_hugectr(\n",
    "                    cats=CATEGORICAL_COLUMNS + CROSS_COLUMNS,\n",
    "                    conts=conts,\n",
    "                    labels=LABEL_COLUMNS,\n",
    "                    output_path=args.out_path + '/file' + str(i),\n",
    "                    shuffle=shuffle,\n",
    "                    out_files_per_proc=args.out_files_per_proc,\n",
    "                    num_threads=args.num_io_threads,\n",
    "                    write_hugectr_keyset=True)\n",
    "        else:\n",
    "            workflow.transform(iterator).to_parquet(\n",
    "                    output_path=args.out_path + '/file' + str(i),\n",
    "                    dtypes=dict_dtypes,\n",
    "                    cats=CATEGORICAL_COLUMNS + CROSS_COLUMNS,\n",
    "                    conts=conts,\n",
    "                    labels=LABEL_COLUMNS,\n",
    "                    shuffle=shuffle,\n",
    "                    out_files_per_proc=args.out_files_per_proc,\n",
    "                    num_threads=args.num_io_threads,\n",
    "                    write_hugectr_keyset=True)\n",
    "        i += 1\n",
    "\n",
    "        embeddings_dict_cat = categorify_op.get_embedding_sizes(CATEGORICAL_COLUMNS)\n",
    "        embeddings_dict_cross = cross_cat_op.get_embedding_sizes(CROSS_COLUMNS)\n",
    "        embeddings = [embeddings_dict_cross[c][0] for c in CROSS_COLUMNS] + [embeddings_dict_cat[c][0] for c in CATEGORICAL_COLUMNS]\n",
    "\n",
    "        print(embeddings)\n",
    "    ##--------------------##\n",
    "\n",
    "    ## Shutdown clusters\n",
    "    client.close()\n",
    "    logging.info('NVTabular processing done')\n",
    "\n",
    "    runtime = time.time() - runtime\n",
    "\n",
    "    print(\"\\nDask-NVTabular Criteo Preprocessing\")\n",
    "    print(\"--------------------------------------\")\n",
    "    print(f\"data_path          | {args.data_path}\")\n",
    "    print(f\"output_path        | {args.out_path}\")\n",
    "    print(f\"partition size     | {'%.2f GB'%bytesto(int(args.part_mem_frac * device_size),'g')}\")\n",
    "    print(f\"protocol           | {args.protocol}\")\n",
    "    print(f\"device(s)          | {args.devices}\")\n",
    "    print(f\"rmm-pool-frac      | {(args.device_pool_frac)}\")\n",
    "    print(f\"out-files-per-proc | {args.out_files_per_proc}\")\n",
    "    print(f\"num_io_threads     | {args.num_io_threads}\")\n",
    "    print(f\"shuffle            | {args.shuffle}\")\n",
    "    print(\"======================================\")\n",
    "    print(f\"Runtime[s]         | {runtime}\")\n",
    "    print(\"======================================\\n\")\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=(\"Multi-GPU Criteo Preprocessing\"))\n",
    "\n",
    "    #\n",
    "    # System Options\n",
    "    #\n",
    "\n",
    "    parser.add_argument(\"--data_path\", type=str, help=\"Input dataset path (Required)\")\n",
    "    parser.add_argument(\"--out_path\", type=str, help=\"Directory path to write output (Required)\")\n",
    "    parser.add_argument(\n",
    "        \"-d\",\n",
    "        \"--devices\",\n",
    "        default=os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"0\"),\n",
    "        type=str,\n",
    "        help='Comma-separated list of visible devices (e.g. \"0,1,2,3\"). '\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-p\",\n",
    "        \"--protocol\",\n",
    "        choices=[\"tcp\", \"ucx\"],\n",
    "        default=\"tcp\",\n",
    "        type=str,\n",
    "        help=\"Communication protocol to use (Default 'tcp')\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--device_limit_frac\",\n",
    "        default=0.5,\n",
    "        type=float,\n",
    "        help=\"Worker device-memory limit as a fraction of GPU capacity (Default 0.8). \"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--device_pool_frac\",\n",
    "        default=0.9,\n",
    "        type=float,\n",
    "        help=\"RMM pool size for each worker  as a fraction of GPU capacity (Default 0.9). \"\n",
    "        \"The RMM pool frac is the same for all GPUs, make sure each one has enough memory size\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_io_threads\",\n",
    "        default=0,\n",
    "        type=int,\n",
    "        help=\"Number of threads to use when writing output data (Default 0). \"\n",
    "        \"If 0 is specified, multi-threading will not be used for IO.\",\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Data-Decomposition Parameters\n",
    "    #\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--part_mem_frac\",\n",
    "        default=0.125,\n",
    "        type=float,\n",
    "        help=\"Maximum size desired for dataset partitions as a fraction \"\n",
    "        \"of GPU capacity (Default 0.125)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--out_files_per_proc\",\n",
    "        default=8,\n",
    "        type=int,\n",
    "        help=\"Number of output files to write on each worker (Default 8)\",\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Preprocessing Options\n",
    "    #\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"-f\",\n",
    "        \"--freq_limit\",\n",
    "        default=0,\n",
    "        type=int,\n",
    "        help=\"Frequency limit for categorical encoding (Default 0)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-s\",\n",
    "        \"--shuffle\",\n",
    "        choices=[\"PER_WORKER\", \"PER_PARTITION\", \"NONE\"],\n",
    "        default=\"PER_PARTITION\",\n",
    "        help=\"Shuffle algorithm to use when writing output data to disk (Default PER_PARTITION)\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--feature_cross_list\", default=None, type=str, help=\"List of feature crossing cols (e.g. C1_C2, C3_C4)\"\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Diagnostics Options\n",
    "    #\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--profile\",\n",
    "        metavar=\"PATH\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        help=\"Specify a file path to export a Dask profile report (E.g. dask-report.html).\"\n",
    "        \"If this option is excluded from the command, not profile will be exported\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dashboard_port\",\n",
    "        default=\"8787\",\n",
    "        type=str,\n",
    "        help=\"Specify the desired port of Dask's diagnostics-dashboard (Default `3787`). \"\n",
    "        \"The dashboard will be hosted at http://<IP>:<PORT>/status\",\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Format\n",
    "    #\n",
    "\n",
    "    parser.add_argument('--criteo_mode', type=int, default=0)\n",
    "    parser.add_argument('--parquet_format', type=int, default=1)\n",
    "    parser.add_argument('--dataset_type', type=str, default='train')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    args.n_workers = len(args.devices.split(\",\"))\n",
    "    return args\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    args = parse_args()\n",
    "\n",
    "    process_NVT(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60c2ab04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-25 03:28:18,637 NVTabular processing\n",
      "distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "2021-11-25 03:28:25,834 Preprocessing\n",
      "2021-11-25 03:28:26,329 Preprocessing.....\n",
      "/usr/local/lib/python3.8/dist-packages/numba/cuda/compiler.py:865: NumbaPerformanceWarning: Grid size (1) < 2 * SM count (160) will likely result in GPU under utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "[517, 1172, 2057, 393, 688, 3, 1604, 482, 21, 623, 654, 1687, 10, 267, 869, 28, 4, 119, 13, 496, 698, 547, 577, 1683, 27, 24]\n",
      "[517, 1172, 2057, 393, 688, 3, 1604, 482, 21, 623, 654, 1687, 10, 267, 869, 28, 4, 119, 13, 496, 698, 547, 577, 1683, 27, 24]\n",
      "[517, 1172, 2057, 393, 688, 3, 1604, 482, 21, 623, 654, 1687, 10, 267, 869, 28, 4, 119, 13, 496, 698, 547, 577, 1683, 27, 24]\n",
      "[517, 1172, 2057, 393, 688, 3, 1604, 482, 21, 623, 654, 1687, 10, 267, 869, 28, 4, 119, 13, 496, 698, 547, 577, 1683, 27, 24]\n",
      "[517, 1172, 2057, 393, 688, 3, 1604, 482, 21, 623, 654, 1687, 10, 267, 869, 28, 4, 119, 13, 496, 698, 547, 577, 1683, 27, 24]\n",
      "[517, 1172, 2057, 393, 688, 3, 1604, 482, 21, 623, 654, 1687, 10, 267, 869, 28, 4, 119, 13, 496, 698, 547, 577, 1683, 27, 24]\n",
      "2021-11-25 03:28:51,574 NVTabular processing done\n",
      "\n",
      "Dask-NVTabular Criteo Preprocessing\n",
      "--------------------------------------\n",
      "data_path          | ./criteo_data\n",
      "output_path        | ./criteo_data\n",
      "partition size     | 3.97 GB\n",
      "protocol           | tcp\n",
      "device(s)          | 0\n",
      "rmm-pool-frac      | 0.5\n",
      "out-files-per-proc | 8\n",
      "num_io_threads     | 2\n",
      "shuffle            | PER_PARTITION\n",
      "======================================\n",
      "Runtime[s]         | 29.41077160835266\n",
      "======================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python preprocess.py --data_path ./criteo_data --out_path ./criteo_data --freq_limit 6 --device_limit_frac 0.5 --device_pool_frac 0.5 --out_files_per_proc 8 --devices \"0\"  --num_io_threads 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b994e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    " !ls -l ./criteo_data/file*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429f8968",
   "metadata": {},
   "source": [
    "## Kafka broker setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5c1737",
   "metadata": {},
   "source": [
    "Install Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e667496c",
   "metadata": {},
   "source": [
    "```\n",
    "$ cd /usr/local\n",
    "$ wget https://dlcdn.apache.org/kafka/3.0.0/kafka_2.12-3.0.0.tgz\n",
    "$ tar -zxvf kafka_2.12-3.0.0.tgz\n",
    "$ mv kafka_2.12-3.0.0 kafka\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6eecb0",
   "metadata": {},
   "source": [
    "Install Zookeeper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c831483",
   "metadata": {},
   "source": [
    "```\n",
    "$ cd /usr/local\n",
    "$ wget https://dlcdn.apache.org/zookeeper/zookeeper-3.7.0/apache-zookeeper-3.7.0-bin.tar.gz\n",
    "$ tar -zxvf apache-zookeeper-3.7.0-bin.tar.gz\n",
    "$ mv apache-zookeeper-3.7.0-bin zookeeper\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cd340d",
   "metadata": {},
   "source": [
    "Configure Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89b6e68",
   "metadata": {},
   "source": [
    "**Open /usr/local/kafka/config/server.properties and decomment the following:**\n",
    "```\n",
    "listeners = PLAINTEXT://your.host.name:9092\n",
    "```\n",
    "**Input your host name and port, this should be consistent with what you input in the training script below.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fb7684",
   "metadata": {},
   "source": [
    "Start Zookeeper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9cd597",
   "metadata": {},
   "source": [
    "```\n",
    "$ cd /usr/local/zookeeper/bin\n",
    "$ bash zkServer.sh start\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949fab27",
   "metadata": {},
   "source": [
    "Start Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7207923",
   "metadata": {},
   "source": [
    "```\n",
    "$ cd /usr/local/kafka\n",
    "$ bin/kafka-server-start.sh config/server.properties\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b58b76",
   "metadata": {},
   "source": [
    "## Wide&Deep Model Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f7c8af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wdl_demo.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wdl_demo.py\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "solver = hugectr.CreateSolver(model_name = \"wdl\",\n",
    "                              max_eval_batches = 5000,\n",
    "                              batchsize_eval = 1024,\n",
    "                              batchsize = 1024,\n",
    "                              lr = 0.001,\n",
    "                              vvgpu = [[0]],\n",
    "                              i64_input_key = True,\n",
    "                              use_mixed_precision = False,\n",
    "                              repeat_dataset = False,\n",
    "                              use_cuda_graph = True,\n",
    "                              kafka_brockers = \"10.23.137.25:9093\")\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Parquet,\n",
    "                          source = [\"criteo_data/file0/_file_list.txt\", \"criteo_data/file1/_file_list.txt\"],\n",
    "                          keyset = [\"criteo_data/file0/_hugectr.keyset\", \"criteo_data/file1/_hugectr.keyset\"],\n",
    "                          slot_size_array = [517, 1172, 2057, 393, 688, 3, 1604, 482, 21, 623, 654, 1687, 10, 267, 869, 28, 4, 119, 13, 496, 698, 547, 577, 1683, 27, 24],\n",
    "                          eval_source = \"criteo_data/file2/_file_list.txt\",\n",
    "                          check_type = hugectr.Check_t.Sum)\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.Adam)\n",
    "hc_config = hugectr.CreateHMemCache(2, 0.5, 0)\n",
    "etc = hugectr.CreateETC(ps_types = [hugectr.TrainPSType_t.Staged, hugectr.TrainPSType_t.Cached],\\\n",
    "                        sparse_models = [\"./wdl_0_sparse_model\", \"./wdl_1_sparse_model\"],\\\n",
    "                        local_paths = [\"raid/md1/temp_dir\"], hmem_cache_configs = [hc_config])\n",
    "model = hugectr.Model(solver, reader, optimizer, etc)\n",
    "model.add(hugectr.Input(label_dim = 1, label_name = \"label\",\n",
    "                        dense_dim = 13, dense_name = \"dense\",\n",
    "                        data_reader_sparse_param_array = \n",
    "                        [hugectr.DataReaderSparseParam(\"wide_data\", 1, True, 1),\n",
    "                        hugectr.DataReaderSparseParam(\"deep_data\", 1, True, 25)]))\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash, \n",
    "                            workspace_size_per_gpu_in_mb = 23,\n",
    "                            embedding_vec_size = 1,\n",
    "                            combiner = \"sum\",\n",
    "                            sparse_embedding_name = \"sparse_embedding2\",\n",
    "                            bottom_name = \"wide_data\",\n",
    "                            optimizer = optimizer))\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash, \n",
    "                            workspace_size_per_gpu_in_mb = 358,\n",
    "                            embedding_vec_size = 16,\n",
    "                            combiner = \"sum\",\n",
    "                            sparse_embedding_name = \"sparse_embedding1\",\n",
    "                            bottom_name = \"deep_data\",\n",
    "                            optimizer = optimizer))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding1\"],\n",
    "                            top_names = [\"reshape1\"],\n",
    "                            leading_dim=400))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding2\"],\n",
    "                            top_names = [\"reshape2\"],\n",
    "                            leading_dim=1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Concat,\n",
    "                            bottom_names = [\"reshape1\", \"dense\"], top_names = [\"concat1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"concat1\"],\n",
    "                            top_names = [\"fc1\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc1\"],\n",
    "                            top_names = [\"relu1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu1\"],\n",
    "                            top_names = [\"dropout1\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dropout1\"],\n",
    "                            top_names = [\"fc2\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc2\"],\n",
    "                            top_names = [\"relu2\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu2\"],\n",
    "                            top_names = [\"dropout2\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dropout2\"],\n",
    "                            top_names = [\"fc3\"],\n",
    "                            num_output=1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Add,\n",
    "                            bottom_names = [\"fc3\", \"reshape2\"],\n",
    "                            top_names = [\"add1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "                            bottom_names = [\"add1\", \"label\"],\n",
    "                            top_names = [\"loss\"]))\n",
    "model.compile()\n",
    "model.summary()\n",
    "model.graph_to_json(graph_config_file = \"wdl.json\")\n",
    "model.fit(max_iter = 1100, display = 200, eval_interval = 500, snapshot = 1000000, snapshot_prefix = \"wdl\")\n",
    "# Get the updated embedding features in model.fit()\n",
    "# updated_model = model.get_incremental_model()\n",
    "# User defined operations to the updated_model\n",
    "# ...+\n",
    "model.dump_parameters_2kafka()\n",
    "model.set_source(source = [\"criteo_data/file3/_file_list.txt\", \"criteo_data/file4/_file_list.txt\"], keyset = [\"criteo_data/file3/_hugectr.keyset\", \"criteo_data/file4/_hugectr.keyset\"], eval_source = \"criteo_data/file5/_file_list.txt\")\n",
    "model.fit(max_iter = 1100, display = 200, eval_interval = 500, snapshot = 1000000, snapshot_prefix = \"wdl\")\n",
    "# Get the updated embedding features in model.fit()\n",
    "# updated_model = model.get_incremental_model()\n",
    "# User defined operations to the updated_model\n",
    "# ...\n",
    "model.save_params_to_files(\"wdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "436c20e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r wdl_0_sparse_model\n",
    "!rm -r wdl_1_sparse_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69481a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HUGECTR][03:09:39][INFO][RANK0]: Empty embedding, trained table will be stored in ./wdl_0_sparse_model\n",
      "[HUGECTR][03:09:39][INFO][RANK0]: Empty embedding, trained table will be stored in ./wdl_1_sparse_model\n",
      "HugeCTR Version: 3.2\n",
      "====================================================Model Init=====================================================\n",
      "[HUGECTR][03:09:39][INFO][RANK0]: Initialize model: wdl\n",
      "[HUGECTR][03:09:39][INFO][RANK0]: Global seed is 407180828\n",
      "[HUGECTR][03:09:40][INFO][RANK0]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 0\n",
      "\n",
      "[HUGECTR][03:09:41][WARNING][RANK0]: Peer-to-peer access cannot be fully enabled.\n",
      "[HUGECTR][03:09:41][INFO][RANK0]: Start all2all warmup\n",
      "[HUGECTR][03:09:41][INFO][RANK0]: End all2all warmup\n",
      "[HUGECTR][03:09:41][INFO][RANK0]: Using All-reduce algorithm: NCCL\n",
      "[HUGECTR][03:09:41][INFO][RANK0]: Device 0: Tesla V100-SXM2-32GB\n",
      "[HUGECTR][03:09:41][DEBUG][RANK0]: Creating Kafka lifetime service.\n",
      "[HUGECTR][03:09:41][INFO][RANK0]: num of DataReader workers: 1\n",
      "[HUGECTR][03:09:41][INFO][RANK0]: Vocabulary size: 0\n",
      "[HUGECTR][03:09:41][INFO][RANK0]: max_vocabulary_size_per_gpu_=6029312\n",
      "[HUGECTR][03:09:41][INFO][RANK0]: max_vocabulary_size_per_gpu_=5865472\n",
      "[HUGECTR][03:09:41][INFO][RANK0]: Graph analysis to resolve tensor dependency\n",
      "===================================================Model Compile===================================================\n",
      "%3|1637809781.628|FAIL|rdkafka#producer-1| [thrd:10.23.137.25:9093/bootstrap]: 10.23.137.25:9093/bootstrap: Connect to ipv4#10.23.137.25:9093 failed: Connection refused (after 217ms in state CONNECT)\n",
      "%3|1637809781.628|ERROR|rdkafka#producer-1| [thrd:10.23.137.25:9093/bootstrap]: 1/1 brokers are down\n",
      "%3|1637809781.629|ERROR|rdkafka#producer-1| [thrd:app]: rdkafka#producer-1: 10.23.137.25:9093/bootstrap: Connect to ipv4#10.23.137.25:9093 failed: Connection refused (after 217ms in state CONNECT)\n",
      "%3|1637809782.118|FAIL|rdkafka#producer-1| [thrd:10.23.137.25:9093/bootstrap]: 10.23.137.25:9093/bootstrap: Connect to ipv4#10.23.137.25:9093 failed: Connection refused (after 207ms in state CONNECT, 1 identical error(s) suppressed)\n",
      "%3|1637809782.118|ERROR|rdkafka#producer-1| [thrd:app]: rdkafka#producer-1: 10.23.137.25:9093/bootstrap: Connect to ipv4#10.23.137.25:9093 failed: Connection refused (after 207ms in state CONNECT, 1 identical error(s) suppressed)\n",
      "[HUGECTR][03:09:44][INFO][RANK0]: gpu0 start to init embedding\n",
      "[HUGECTR][03:09:44][INFO][RANK0]: gpu0 init embedding done\n",
      "[HUGECTR][03:09:44][INFO][RANK0]: gpu0 start to init embedding\n",
      "[HUGECTR][03:09:44][INFO][RANK0]: gpu0 init embedding done\n",
      "[HUGECTR][03:09:44][INFO][RANK0]: Enable HMEM-Based Parameter Server\n",
      "[HUGECTR][03:09:44][INFO][RANK0]: ./wdl_0_sparse_model not exist, create and train from scratch\n",
      "[HUGECTR][03:09:44][INFO][RANK0]: Enable HMemCache-Based Parameter Server\n",
      "[HUGECTR][03:09:44][INFO][RANK0]: ./wdl_1_sparse_model/key doesn't exist, created\n",
      "[HUGECTR][03:09:44][INFO][RANK0]: ./wdl_1_sparse_model/emb_vector doesn't exist, created\n",
      "[HUGECTR][03:09:44][INFO][RANK0]: ./wdl_1_sparse_model/Adam.m doesn't exist, created\n",
      "[HUGECTR][03:09:44][INFO][RANK0]: ./wdl_1_sparse_model/Adam.v doesn't exist, created\n",
      "[HUGECTR][03:09:46][INFO][RANK0]: Starting AUC NCCL warm-up\n",
      "[HUGECTR][03:09:46][INFO][RANK0]: Warm-up done\n",
      "===================================================Model Summary===================================================\n",
      "label                                   Dense                         Sparse                        \n",
      "label                                   dense                          wide_data,deep_data           \n",
      "(None, 1)                               (None, 13)                              \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "DistributedSlotSparseEmbeddingHash      wide_data                     sparse_embedding2             (None, 1, 1)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "DistributedSlotSparseEmbeddingHash      deep_data                     sparse_embedding1             (None, 25, 16)                \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Reshape                                 sparse_embedding1             reshape1                      (None, 400)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Reshape                                 sparse_embedding2             reshape2                      (None, 1)                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Concat                                  reshape1                      concat1                       (None, 413)                   \n",
      "                                        dense                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            concat1                       fc1                           (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc1                           relu1                         (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Dropout                                 relu1                         dropout1                      (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            dropout1                      fc2                           (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc2                           relu2                         (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Dropout                                 relu2                         dropout2                      (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            dropout2                      fc3                           (None, 1)                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Add                                     fc3                           add1                          (None, 1)                     \n",
      "                                        reshape2                                                                                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "BinaryCrossEntropyLoss                  add1                          loss                                                        \n",
      "                                        label                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "[HUGECTR][03:09:46][INFO][RANK0]: Save the model graph to wdl.json successfully\n",
      "=====================================================Model Fit=====================================================\n",
      "[HUGECTR][03:09:46][INFO][RANK0]: Use embedding training cache mode with number of training sources: 2, number of epochs: 1\n",
      "[HUGECTR][03:09:46][INFO][RANK0]: Training batchsize: 1024, evaluation batchsize: 1024\n",
      "[HUGECTR][03:09:46][INFO][RANK0]: Evaluation interval: 500, snapshot interval: 1000000\n",
      "[HUGECTR][03:09:46][INFO][RANK0]: Sparse embedding trainable: True, dense network trainable: True\n",
      "[HUGECTR][03:09:46][INFO][RANK0]: Use mixed precision: False, scaler: 1.000000, use cuda graph: True\n",
      "[HUGECTR][03:09:46][INFO][RANK0]: lr: 0.001000, warmup_steps: 1, decay_start: 0, decay_steps: 1, decay_power: 2.000000, end_lr: 0.000000\n",
      "[HUGECTR][03:09:46][INFO][RANK0]: Evaluation source file: criteo_data/file2/_file_list.txt\n",
      "[HUGECTR][03:09:46][INFO][RANK0]: --------------------Epoch 0, source file: criteo_data/file0/_file_list.txt--------------------\n",
      "[HUGECTR][03:09:46][INFO][RANK0]: Preparing embedding table for next pass\n",
      "[HUGECTR][03:09:46][INFO][RANK0]: HMEM-Cache PS: Hit rate [load]: 0 %\n",
      "[HUGECTR][03:09:48][INFO][RANK0]: Iter: 200 Time(200 iters): 2.056589s Loss: 0.085241 lr:0.001000\n",
      "[HUGECTR][03:09:49][INFO][RANK0]: Iter: 400 Time(200 iters): 1.134001s Loss: 0.160827 lr:0.001000\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python wdl_demo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd89a18a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
