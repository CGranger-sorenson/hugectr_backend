{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94d5d02d",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# HugeCTR Continuous Training Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a0ee57",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In HugeCTR version 3.3, we finished the whole pipeline of parameter server, including \n",
    "1. The parameter dumping interface from training to kafka.\n",
    "2. Redis as a level2 cache.\n",
    "3. RocksDB as a persistence storage.\n",
    "4. Embedding cache update mechanism.\n",
    "\n",
    "\n",
    "The purpose of this notebook is to give you a brief idea of how parameter server works in terms of the flow of parameters. \n",
    "\n",
    "## Table of Contents\n",
    "-  [Data Preparation](#1)\n",
    "-  [Data Preprocessing using NVTabular](#2)\n",
    "-  [Kafka broker start](#3)\n",
    "-  [Wide&Deep Continuous Training Demo](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e90f01d",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609f8eb4",
   "metadata": {},
   "source": [
    "1. Firstly, we made a folder to store our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63ff0b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘criteo_script’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir criteo_data\n",
    "!mkdir criteo_script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7a5c9e",
   "metadata": {},
   "source": [
    "2. Download Criteo Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134282e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://azuremlsampleexperiments.blob.core.windows.net/criteo/day_1.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a1d948",
   "metadata": {},
   "source": [
    "**NOTE**: Replace `1` with a value from [0, 23] to use a different day.\n",
    "\n",
    "During preprocessing, the amount of data, which is used to speed up the preprocessing, fill missing values, and remove the feature values that are considered rare, is further reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d8cf29",
   "metadata": {},
   "source": [
    "3. Thirdly, preprocess the data and split the data into 6 parts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccba5c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocess.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocess.sh\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "if [[ $# -lt 3 ]]; then\n",
    "  echo \"Usage: preprocess.sh [DATASET_NO.] [DST_DATA_DIR] [SCRIPT_TYPE] [SCRIPT_TYPE_SPECIFIC_ARGS...]\"\n",
    "  exit 2\n",
    "fi\n",
    "\n",
    "DST_DATA_DIR=$2\n",
    "\n",
    "echo \"Warning: existing $DST_DATA_DIR is erased\"\n",
    "rm -rf $DST_DATA_DIR\n",
    "\n",
    "if [[ $3 == \"nvt\" ]]; then\n",
    "  if [[ $# -ne 6 ]]; then\n",
    "\t\techo \"Usage: preprocess.sh [DATASET_NO.] [DST_DATA_DIR] nvt [IS_PARQUET_FORMAT] [IS_CRITEO_MODE] [IS_FEATURE_CROSSED]\"\n",
    "    exit 2\n",
    "\tfi\n",
    "\techo \"Preprocessing script: NVTabular\"\n",
    "elif [[ $3 == \"perl\" ]]; then\n",
    "  if [[ $# -ne 4 ]]; then\n",
    "\t\techo \"Usage: preprocess.sh [DATASET_NO.] [DST_DATA_DIR] perl [NUM_SLOTS]\"\n",
    "    exit 2\n",
    "\tfi\n",
    "\techo \"Preprocessing script: Perl\"\n",
    "elif [[ $3 == \"pandas\" ]]; then\n",
    "  if [[ $# -lt 5 ]]; then\n",
    "\t\techo \"Usage: preprocess.sh [DATASET_NO.] [DST_DATA_DIR] pandas [IS_DENSE_NORMALIZED] [IS_FEATURE_CROSSED] (FILE_LIST_LENGTH)\"\n",
    "    exit 2\n",
    "\tfi\n",
    "\techo \"Preprocessing script: Pandas\"\n",
    "else\n",
    "\techo \"Error: $3 is an invalid script type. Pick one from {nvt, perl, pandas}.\"\n",
    "\texit 2\n",
    "fi\n",
    "\n",
    "SCRIPT_TYPE=$3\n",
    "\n",
    "echo \"Getting the first few examples from the uncompressed dataset...\"\n",
    "mkdir -p $DST_DATA_DIR/train                         && \\\n",
    "mkdir -p $DST_DATA_DIR/val                           && \\\n",
    "head -n 500000 day_$1 > $DST_DATA_DIR/day_$1_small\n",
    "if [ $? -ne 0 ]; then\n",
    "\techo \"Warning: fallback to find original compressed data day_$1.gz...\"\n",
    "\techo \"Decompressing day_$1.gz...\"\n",
    "\tgzip -d -c day_$1.gz > day_$1\n",
    "\tif [ $? -ne 0 ]; then\n",
    "\t\techo \"Error: failed to decompress the file.\"\n",
    "\t\texit 2\n",
    "\tfi\n",
    "\thead -n 500000 day_$1 > $DST_DATA_DIR/day_$1_small\n",
    "\tif [ $? -ne 0 ]; then\n",
    "\t\techo \"Error: day_$1 file\"\n",
    "\t\texit 2\n",
    "\tfi\n",
    "fi\n",
    "\n",
    "echo \"Counting the number of samples in day_$1 dataset...\"\n",
    "total_count=$(wc -l $DST_DATA_DIR/day_$1_small)\n",
    "total_count=(${total_count})\n",
    "echo \"The first $total_count examples will be used in day_$1 dataset.\"\n",
    "\n",
    "echo \"Shuffling dataset...\"\n",
    "shuf $DST_DATA_DIR/day_$1_small > $DST_DATA_DIR/day_$1_shuf\n",
    "\n",
    "train_count=$(( total_count * 8 / 10))\n",
    "valtest_count=$(( total_count - train_count ))\n",
    "val_count=$(( valtest_count * 5 / 10 ))\n",
    "test_count=$(( valtest_count - val_count  ))\n",
    "\n",
    "split_dataset()\n",
    "{\n",
    "\techo \"Splitting into $train_count-sample training, $val_count-sample val, and $test_count-sample test datasets...\"\n",
    "\thead -n $train_count $DST_DATA_DIR/$1 > $DST_DATA_DIR/train/train.txt          && \\\n",
    "\ttail -n $valtest_count $DST_DATA_DIR/$1 > $DST_DATA_DIR/val/valtest.txt        && \\\n",
    "\thead -n $val_count $DST_DATA_DIR/val/valtest.txt > $DST_DATA_DIR/val/val.txt   && \\\n",
    "\ttail -n $test_count $DST_DATA_DIR/val/valtest.txt > $DST_DATA_DIR/val/test.txt\n",
    "\n",
    "\tif [ $? -ne 0 ]; then\n",
    "\t\texit 2\n",
    "\tfi\n",
    "}\n",
    "\n",
    "echo \"Preprocessing...\"\n",
    "if [[ $SCRIPT_TYPE == \"nvt\" ]]; then\n",
    "\tIS_PARQUET_FORMAT=$4\n",
    "\tIS_CRITEO_MODE=$5\n",
    "\tFEATURE_CROSS_LIST_OPTION=\"\"\n",
    "\tif [[ ( $IS_CRITEO_MODE -eq 0 ) && ( $6 -eq 1 ) ]]; then\n",
    "\t\tFEATURE_CROSS_LIST_OPTION=\"--feature_cross_list C1_C2,C3_C4\"\n",
    "\t\techo $FEATURE_CROSS_LIST_OPTION\n",
    "\tfi\n",
    "  split_dataset day_$1_shuf\n",
    "  python3 criteo_script/preprocess_nvt.py \\\n",
    "\t\t--data_path $DST_DATA_DIR             \\\n",
    "\t\t--out_path $DST_DATA_DIR              \\\n",
    "\t\t--freq_limit 6                        \\\n",
    "\t\t--device_limit_frac 0.5               \\\n",
    "\t\t--device_pool_frac 0.5                \\\n",
    "\t\t--out_files_per_proc 8                \\\n",
    "\t\t--devices \"0\"                         \\\n",
    "\t\t--num_io_threads 2                    \\\n",
    "        --parquet_format=$IS_PARQUET_FORMAT   \\\n",
    "\t\t--criteo_mode=$IS_CRITEO_MODE         \\\n",
    "\t\t$FEATURE_CROSS_LIST_OPTION\n",
    "\n",
    "elif [[ $SCRIPT_TYPE == \"perl\" ]]; then\n",
    "\tNUM_SLOT=$4\n",
    "  split_dataset day_$1_shuf\n",
    "\tperl criteo_script_legacy/preprocess.pl $DST_DATA_DIR/train/train.txt $DST_DATA_DIR/val/val.txt $DST_DATA_DIR/val/test.txt                      && \\\n",
    "\tcriteo2hugectr_legacy $NUM_SLOT $DST_DATA_DIR/train/train.txt.out $DST_DATA_DIR/train/sparse_embedding $DST_DATA_DIR/file_list.txt && \\\n",
    "\tcriteo2hugectr_legacy $NUM_SLOT $DST_DATA_DIR/val/test.txt.out $DST_DATA_DIR/val/sparse_embedding $DST_DATA_DIR/file_list_test.txt\n",
    "\n",
    "elif [[ $SCRIPT_TYPE == \"pandas\" ]]; then\n",
    "\tpython3 criteo_script/preprocess.py                 \\\n",
    "\t\t--src_csv_path=$DST_DATA_DIR/day_$1_shuf          \\\n",
    "\t\t--dst_csv_path=$DST_DATA_DIR/day_$1_shuf.out      \\\n",
    "\t\t--normalize_dense=$4 --feature_cross=$5      &&   \\\n",
    "  split_dataset day_$1_shuf.out\n",
    "\tNUM_WIDE_KEYS=\"\"\n",
    "\tif [[ $5 -ne 0 ]]; then\n",
    "\t\tNUM_WIDE_KEYS=2\n",
    "\tfi\n",
    "\n",
    "  FILE_LIST_LENGTH=\"\"\n",
    "  if [[ $# -gt 5 ]]; then\n",
    "    FILE_LIST_LENGTH=$6\n",
    "\tfi\n",
    "\n",
    "\tcriteo2hugectr $DST_DATA_DIR/train/train.txt $DST_DATA_DIR/train/sparse_embedding $DST_DATA_DIR/file_list.txt $NUM_WIDE_KEYS $FILE_LIST_LENGTH && \\\n",
    "\tcriteo2hugectr $DST_DATA_DIR/val/test.txt $DST_DATA_DIR/val/sparse_embedding $DST_DATA_DIR/file_list_test.txt $NUM_WIDE_KEYS $FILE_LIST_LENGTH\n",
    "fi\n",
    "\n",
    "if [ $? -ne 0 ]; then\n",
    "\texit 2\n",
    "fi\n",
    "\n",
    "echo \"All done!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "857eccd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting criteo_script/preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile criteo_script/preprocess.py\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import tempfile\n",
    "\n",
    "from six.moves import urllib\n",
    "import urllib.request \n",
    "\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import logging\n",
    "import concurrent.futures as cf\n",
    "from traceback import print_exc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing as skp\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(message)s')\n",
    "logging.root.setLevel(logging.NOTSET)\n",
    "\n",
    "NUM_INTEGER_COLUMNS = 13\n",
    "NUM_CATEGORICAL_COLUMNS = 26\n",
    "NUM_TOTAL_COLUMNS = 1 + NUM_INTEGER_COLUMNS + NUM_CATEGORICAL_COLUMNS\n",
    "\n",
    "MAX_NUM_WORKERS = NUM_TOTAL_COLUMNS\n",
    "\n",
    "INT_NAN_VALUE = np.iinfo(np.int32).min\n",
    "CAT_NAN_VALUE = '80000000'\n",
    "\n",
    "def idx2key(idx):\n",
    "    if idx == 0:\n",
    "        return 'label'\n",
    "    return 'I' + str(idx) if idx <= NUM_INTEGER_COLUMNS else 'C' + str(idx - NUM_INTEGER_COLUMNS)\n",
    "\n",
    "def _fill_missing_features_and_split(chunk, series_list_dict):\n",
    "    for cid, col in enumerate(chunk.columns):\n",
    "        NAN_VALUE = INT_NAN_VALUE if cid <= NUM_INTEGER_COLUMNS else CAT_NAN_VALUE\n",
    "        result_series = chunk[col].fillna(NAN_VALUE)\n",
    "        series_list_dict[col].append(result_series)\n",
    "\n",
    "def _merge_and_transform_series(src_series_list, col, dense_cols,\n",
    "                                normalize_dense):\n",
    "    result_series = pd.concat(src_series_list)\n",
    "\n",
    "    if col != 'label':\n",
    "        unique_value_counts = result_series.value_counts()\n",
    "        unique_value_counts = unique_value_counts.loc[unique_value_counts >= 6]\n",
    "        unique_value_counts = set(unique_value_counts.index.values)\n",
    "        NAN_VALUE = INT_NAN_VALUE if col.startswith('I') else CAT_NAN_VALUE\n",
    "        result_series = result_series.apply(\n",
    "                lambda x: x if x in unique_value_counts else NAN_VALUE)\n",
    "\n",
    "    if col == 'label' or col in dense_cols:\n",
    "        result_series = result_series.astype(np.int64)\n",
    "        le = skp.LabelEncoder()\n",
    "        result_series = pd.DataFrame(le.fit_transform(result_series))\n",
    "        if col != 'label':\n",
    "            result_series = result_series + 1\n",
    "    else:\n",
    "        oe = skp.OrdinalEncoder(dtype=np.int64)\n",
    "        result_series = pd.DataFrame(oe.fit_transform(pd.DataFrame(result_series)))\n",
    "        result_series = result_series + 1\n",
    "\n",
    "\n",
    "    if normalize_dense != 0:\n",
    "        if col in dense_cols:\n",
    "            mms = skp.MinMaxScaler(feature_range=(0,1))\n",
    "            result_series = pd.DataFrame(mms.fit_transform(result_series))\n",
    "\n",
    "    result_series.columns = [col]\n",
    "\n",
    "    min_max = (np.int64(result_series[col].min()), np.int64(result_series[col].max()))\n",
    "    if col != 'label':\n",
    "        logging.info('column {} [{}, {}]'.format(col, str(min_max[0]),str(min_max[1])))\n",
    "\n",
    "    return [result_series, min_max]\n",
    "\n",
    "def _convert_to_string(series):\n",
    "    return series.astype(str)\n",
    "\n",
    "def _merge_columns_and_feature_cross(series_list, min_max, feature_pairs,\n",
    "                                     feature_cross):\n",
    "    name_to_series = dict()\n",
    "    for series in series_list:\n",
    "        name_to_series[series.columns[0]] = series.iloc[:,0]\n",
    "    df = pd.DataFrame(name_to_series)\n",
    "    cols = [idx2key(idx) for idx in range(0, NUM_TOTAL_COLUMNS)]\n",
    "    df = df.reindex(columns=cols)\n",
    "\n",
    "    offset = np.int64(0)\n",
    "    for col in cols:\n",
    "        if col != 'label' and col.startswith('I') == False:\n",
    "            df[col] += offset\n",
    "            logging.info('column {} offset {}'.format(col, str(offset)))\n",
    "            offset += min_max[col][1]\n",
    "\n",
    "    if feature_cross != 0:\n",
    "        for idx, pair in enumerate(feature_pairs):\n",
    "            col0 = pair[0]\n",
    "            col1 = pair[1]\n",
    "\n",
    "            col1_width = int(min_max[col1][1] - min_max[col1][0] + 1)\n",
    "\n",
    "            crossed_column_series = df[col0] * col1_width + df[col1]\n",
    "            oe = skp.OrdinalEncoder(dtype=np.int64)\n",
    "            crossed_column_series = pd.DataFrame(oe.fit_transform(pd.DataFrame(crossed_column_series)))\n",
    "            crossed_column_series = crossed_column_series + 1\n",
    "\n",
    "            crossed_column = col0 + '_' + col1\n",
    "            df.insert(NUM_INTEGER_COLUMNS + 1 + idx, crossed_column, crossed_column_series)\n",
    "            crossed_column_max_val = np.int64(df[crossed_column].max())\n",
    "            logging.info('column {} [{}, {}]'.format(\n",
    "                crossed_column,\n",
    "                str(df[crossed_column].min()),\n",
    "                str(crossed_column_max_val)))\n",
    "            df[crossed_column] += offset\n",
    "            logging.info('column {} offset {}'.format(crossed_column, str(offset)))\n",
    "            offset += crossed_column_max_val\n",
    "\n",
    "    return df\n",
    "\n",
    "def _wait_futures_and_reset(futures):\n",
    "    for future in futures:\n",
    "        result = future.result()\n",
    "        if result:\n",
    "            print(result)\n",
    "    futures = list()\n",
    "\n",
    "def _process_chunks(executor, chunks_to_process, op, *argv):\n",
    "    futures = list()\n",
    "    for chunk in chunks_to_process:\n",
    "        argv_list = list(argv)\n",
    "        argv_list.insert(0, chunk)\n",
    "        new_argv = tuple(argv_list)\n",
    "        future = executor.submit(op, *new_argv)\n",
    "        futures.append(future)\n",
    "    _wait_futures_and_reset(futures)\n",
    "\n",
    "def preprocess(src_txt_name, dst_txt_name, normalize_dense, feature_cross):\n",
    "    cols = [idx2key(idx) for idx in range(0, NUM_TOTAL_COLUMNS)]\n",
    "    series_list_dict = dict()\n",
    "\n",
    "    with cf.ThreadPoolExecutor(max_workers=MAX_NUM_WORKERS) as executor:\n",
    "        logging.info('read a CSV file')\n",
    "        reader = pd.read_csv(src_txt_name, sep='\\t',\n",
    "                             names=cols,\n",
    "                             chunksize=131072)\n",
    "\n",
    "        logging.info('_fill_missing_features_and_split')\n",
    "        for col in cols:\n",
    "            series_list_dict[col] = list()\n",
    "        _process_chunks(executor, reader, _fill_missing_features_and_split,\n",
    "                        series_list_dict)\n",
    "\n",
    "    with cf.ProcessPoolExecutor(max_workers=MAX_NUM_WORKERS) as executor:\n",
    "        logging.info('_merge_and_transform_series')\n",
    "        futures = list()\n",
    "        dense_cols = [idx2key(idx+1) for idx in range(NUM_INTEGER_COLUMNS)]\n",
    "        dst_series_list = list()\n",
    "        min_max = dict()\n",
    "        for col, src_series_list in series_list_dict.items():\n",
    "            future = executor.submit(_merge_and_transform_series,\n",
    "                                     src_series_list, col, dense_cols,\n",
    "                                     normalize_dense)\n",
    "            futures.append(future)\n",
    "\n",
    "        for future in futures:\n",
    "            col = None\n",
    "            for idx, ret in enumerate(future.result()):\n",
    "                try:\n",
    "                    if idx == 0:\n",
    "                        col = ret.columns[0]\n",
    "                        dst_series_list.append(ret)\n",
    "                    else:\n",
    "                        min_max[col] = ret\n",
    "                except:\n",
    "                    print_exc()\n",
    "        futures = list()\n",
    "\n",
    "        logging.info('_merge_columns_and_feature_cross')\n",
    "        feature_pairs = [('C1', 'C2'), ('C3', 'C4')]\n",
    "        df = _merge_columns_and_feature_cross(dst_series_list, min_max, feature_pairs,\n",
    "                                              feature_cross)\n",
    "\n",
    "        \n",
    "        logging.info('_convert_to_string')\n",
    "        futures = dict()\n",
    "        for col in cols:\n",
    "            future = executor.submit(_convert_to_string, df[col])\n",
    "            futures[col] = future\n",
    "        if feature_cross != 0:\n",
    "            for pair in feature_pairs:\n",
    "                col = pair[0] + '_' + pair[1]\n",
    "                future = executor.submit(_convert_to_string, df[col])\n",
    "                futures[col] = future\n",
    "\n",
    "        logging.info('_store_to_df')\n",
    "        for col, future in futures.items():\n",
    "            ret = future.result()\n",
    "            try:\n",
    "                df[col] = ret\n",
    "            except:\n",
    "                print_exc()\n",
    "        futures = dict()\n",
    "\n",
    "        logging.info('write to a CSV file')\n",
    "        df.to_csv(dst_txt_name, sep=' ', header=False, index=False)\n",
    "\n",
    "        logging.info('done!')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    arg_parser = argparse.ArgumentParser(description='Preprocssing Criteo Dataset')\n",
    "\n",
    "    arg_parser.add_argument('--src_csv_path', type=str, required=True)\n",
    "    arg_parser.add_argument('--dst_csv_path', type=str, required=True)\n",
    "    arg_parser.add_argument('--normalize_dense', type=int, default=1)\n",
    "    arg_parser.add_argument('--feature_cross', type=int, default=1)\n",
    "\n",
    "    args = arg_parser.parse_args()\n",
    "\n",
    "    src_csv_path = args.src_csv_path\n",
    "    dst_csv_path = args.dst_csv_path\n",
    "\n",
    "    normalize_dense = args.normalize_dense\n",
    "    feature_cross = args.feature_cross\n",
    "\n",
    "    if os.path.exists(src_csv_path) == False:\n",
    "        sys.exit('ERROR: the file \\'{}\\' doesn\\'t exist'.format(src_csv_path))\n",
    "\n",
    "    if os.path.exists(dst_csv_path) == True:\n",
    "        sys.exit('ERROR: the file \\'{}\\' exists'.format(dst_csv_path))\n",
    "\n",
    "    preprocess(src_csv_path, dst_csv_path, normalize_dense, feature_cross)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c025b3",
   "metadata": {},
   "source": [
    "4. Run the preprocess script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3229d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: existing criteo_data is erased\n",
      "Preprocessing script: Pandas\n",
      "Getting the first few examples from the uncompressed dataset...\n",
      "Counting the number of samples in day_0 dataset...\n",
      "The first 500000 examples will be used in day_0 dataset.\n",
      "Shuffling dataset...\n",
      "Preprocessing...\n",
      "2021-11-26 07:00:29,359 read a CSV file\n",
      "2021-11-26 07:00:29,361 _fill_missing_features_and_split\n",
      "2021-11-26 07:00:31,876 _merge_and_transform_series\n",
      "2021-11-26 07:00:32,300 column I1 [0, 1]\n",
      "2021-11-26 07:00:32,342 column I3 [0, 1]\n",
      "2021-11-26 07:00:32,369 column I4 [0, 1]\n",
      "2021-11-26 07:00:32,379 column I5 [0, 1]\n",
      "2021-11-26 07:00:32,381 column I2 [0, 1]\n",
      "2021-11-26 07:00:32,426 column I6 [0, 1]\n",
      "2021-11-26 07:00:32,435 column I7 [0, 0]\n",
      "2021-11-26 07:00:32,463 column I10 [0, 1]\n",
      "2021-11-26 07:00:32,488 column I11 [0, 1]\n",
      "2021-11-26 07:00:32,533 column I13 [0, 1]\n",
      "2021-11-26 07:00:32,551 column I9 [0, 0]\n",
      "2021-11-26 07:00:32,559 column I8 [0, 1]\n",
      "2021-11-26 07:00:32,598 column I12 [0, 1]\n",
      "2021-11-26 07:00:32,751 column C1 [1, 3559]\n",
      "2021-11-26 07:00:32,821 column C2 [1, 3830]\n",
      "2021-11-26 07:00:32,842 column C4 [1, 1843]\n",
      "2021-11-26 07:00:32,880 column C3 [1, 5986]\n",
      "2021-11-26 07:00:32,881 column C6 [1, 3]\n",
      "2021-11-26 07:00:32,929 column C5 [1, 4105]\n",
      "2021-11-26 07:00:32,956 column C8 [1, 810]\n",
      "2021-11-26 07:00:32,971 column C9 [1, 25]\n",
      "2021-11-26 07:00:33,058 column C7 [1, 3770]\n",
      "2021-11-26 07:00:33,146 column C10 [1, 4203]\n",
      "2021-11-26 07:00:33,163 column C11 [1, 3575]\n",
      "2021-11-26 07:00:33,207 column C13 [1, 10]\n",
      "2021-11-26 07:00:33,255 column C14 [1, 954]\n",
      "2021-11-26 07:00:33,320 column C16 [1, 42]\n",
      "2021-11-26 07:00:33,325 column C15 [1, 2210]\n",
      "2021-11-26 07:00:33,348 column C17 [1, 4]\n",
      "2021-11-26 07:00:33,350 column C12 [1, 6806]\n",
      "2021-11-26 07:00:33,380 column C18 [1, 338]\n",
      "2021-11-26 07:00:33,406 column C19 [1, 14]\n",
      "2021-11-26 07:00:33,627 column C20 [1, 3473]\n",
      "2021-11-26 07:00:33,655 column C21 [1, 4463]\n",
      "2021-11-26 07:00:33,726 column C23 [1, 3414]\n",
      "2021-11-26 07:00:33,741 column C25 [1, 38]\n",
      "2021-11-26 07:00:33,756 column C22 [1, 3745]\n",
      "2021-11-26 07:00:33,767 column C26 [1, 29]\n",
      "2021-11-26 07:00:33,796 column C24 [1, 4277]\n",
      "2021-11-26 07:00:33,803 _merge_columns_and_feature_cross\n",
      "2021-11-26 07:00:34,003 column C1 offset 0\n",
      "2021-11-26 07:00:34,005 column C2 offset 3559\n",
      "2021-11-26 07:00:34,006 column C3 offset 7389\n",
      "2021-11-26 07:00:34,008 column C4 offset 13375\n",
      "2021-11-26 07:00:34,010 column C5 offset 15218\n",
      "2021-11-26 07:00:34,012 column C6 offset 19323\n",
      "2021-11-26 07:00:34,014 column C7 offset 19326\n",
      "2021-11-26 07:00:34,015 column C8 offset 23096\n",
      "2021-11-26 07:00:34,017 column C9 offset 23906\n",
      "2021-11-26 07:00:34,019 column C10 offset 23931\n",
      "2021-11-26 07:00:34,021 column C11 offset 28134\n",
      "2021-11-26 07:00:34,022 column C12 offset 31709\n",
      "2021-11-26 07:00:34,024 column C13 offset 38515\n",
      "2021-11-26 07:00:34,026 column C14 offset 38525\n",
      "2021-11-26 07:00:34,028 column C15 offset 39479\n",
      "2021-11-26 07:00:34,030 column C16 offset 41689\n",
      "2021-11-26 07:00:34,031 column C17 offset 41731\n",
      "2021-11-26 07:00:34,033 column C18 offset 41735\n",
      "2021-11-26 07:00:34,035 column C19 offset 42073\n",
      "2021-11-26 07:00:34,037 column C20 offset 42087\n",
      "2021-11-26 07:00:34,038 column C21 offset 45560\n",
      "2021-11-26 07:00:34,040 column C22 offset 50023\n",
      "2021-11-26 07:00:34,042 column C23 offset 53768\n",
      "2021-11-26 07:00:34,044 column C24 offset 57182\n",
      "2021-11-26 07:00:34,046 column C25 offset 61459\n",
      "2021-11-26 07:00:34,047 column C26 offset 61497\n",
      "2021-11-26 07:00:34,136 column C1_C2 [1, 16580]\n",
      "2021-11-26 07:00:34,138 column C1_C2 offset 61526\n",
      "2021-11-26 07:00:34,348 column C3_C4 [1, 129558]\n",
      "2021-11-26 07:00:34,349 column C3_C4 offset 78106\n",
      "2021-11-26 07:00:34,349 _convert_to_string\n",
      "2021-11-26 07:00:34,352 _store_to_df\n",
      "2021-11-26 07:00:38,904 write to a CSV file\n",
      "2021-11-26 07:00:44,393 done!\n",
      "Splitting into 400000-sample training, 50000-sample val, and 50000-sample test datasets...\n",
      "slot_num for w&D is:27\n",
      "1\n",
      "criteo_data/train exist\n",
      "criteo_data/train/sparse_embedding0.data\n",
      "Opening criteo_data/file_list.txt\n",
      "0 keyset size is: 71182\n",
      "reopen file_list_tmp\n",
      "criteo_data/train/sparse_embedding1.data\n",
      "Opening criteo_data/file_list.txt\n",
      "1 keyset size is: 71456\n",
      "reopen file_list_tmp\n",
      "criteo_data/train/sparse_embedding2.data\n",
      "Opening criteo_data/file_list.txt\n",
      "2 keyset size is: 71132\n",
      "reopen file_list_tmp\n",
      "criteo_data/train/sparse_embedding3.data\n",
      "Opening criteo_data/file_list.txt\n",
      "3 keyset size is: 71350\n",
      "reopen file_list_tmp\n",
      "criteo_data/train/sparse_embedding4.data\n",
      "Opening criteo_data/file_list.txt\n",
      "4 keyset size is: 71304\n",
      "reopen file_list_tmp\n",
      "criteo_data/train/sparse_embedding5.data\n",
      "Opening criteo_data/file_list.txt\n",
      "5 keyset size is: 71132\n",
      "reopen file_list_tmp\n",
      "criteo_data/train/sparse_embedding6.data\n",
      "Opening criteo_data/file_list.txt\n",
      "6 keyset size is: 71213\n",
      "reopen file_list_tmp\n",
      "criteo_data/train/sparse_embedding7.data\n",
      "Opening criteo_data/file_list.txt\n",
      "7 keyset size is: 71104\n",
      "reopen file_list_tmp\n",
      "criteo_data/train/sparse_embedding8.data\n",
      "Opening criteo_data/file_list.txt\n",
      "8 keyset size is: 71509\n",
      "reopen file_list_tmp\n",
      "criteo_data/train/sparse_embedding9.data\n",
      "last keyset size is:62063\n",
      "Opening criteo_data/file_list.txt\n",
      "slot_num for w&D is:27\n",
      "1\n",
      "criteo_data/val exist\n",
      "criteo_data/val/sparse_embedding0.data\n",
      "Opening criteo_data/file_list_test.txt\n",
      "0 keyset size is: 71058\n",
      "reopen file_list_tmp\n",
      "criteo_data/val/sparse_embedding1.data\n",
      "last keyset size is:29978\n",
      "Opening criteo_data/file_list_test.txt\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "!bash preprocess.sh 0 criteo_data pandas 1 1 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c701d5a1",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTES**: \n",
    "- The first argument represents the dataset postfix.  For instance, if `day_1` is used, the postfix is `1`.\n",
    "- The second argument, `criteo_data`, is where the preprocessed data is stored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4a4446",
   "metadata": {},
   "source": [
    "## Wide&Deep Model Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5f5bbf0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wdl_demo.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wdl_demo.py\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "solver = hugectr.CreateSolver(model_name = \"wdl\",\n",
    "                              max_eval_batches = 5000,\n",
    "                              batchsize_eval = 1024,\n",
    "                              batchsize = 1024,\n",
    "                              lr = 0.001,\n",
    "                              vvgpu = [[0]],\n",
    "                              i64_input_key = False,\n",
    "                              use_mixed_precision = False,\n",
    "                              repeat_dataset = False,\n",
    "                              use_cuda_graph = True,\n",
    "                              kafka_brockers = \"10.23.137.25:9093\")\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Norm,\n",
    "                          source = [\"criteo_data/file_list.\"+str(i)+\".txt\" for i in range(2)],\n",
    "                          keyset = [\"criteo_data/file_list.\"+str(i)+\".keyset\" for i in range(2)],\n",
    "                          eval_source = \"criteo_data/file_list.2.txt\",\n",
    "                          check_type = hugectr.Check_t.Sum)\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.Adam)\n",
    "hc_config = hugectr.CreateHMemCache(2, 0.5, 0)\n",
    "etc = hugectr.CreateETC(ps_types = [hugectr.TrainPSType_t.Staged, hugectr.TrainPSType_t.Cached],\\\n",
    "                        sparse_models = [\"./wdl_0_sparse_model\", \"./wdl_1_sparse_model\"],\\\n",
    "                        local_paths = [\"./\"], hmem_cache_configs = [hc_config])\n",
    "model = hugectr.Model(solver, reader, optimizer, etc)\n",
    "model.add(hugectr.Input(label_dim = 1, label_name = \"label\",\n",
    "                        dense_dim = 13, dense_name = \"dense\",\n",
    "                        data_reader_sparse_param_array = \n",
    "                        [hugectr.DataReaderSparseParam(\"wide_data\", 2, True, 1),\n",
    "                        hugectr.DataReaderSparseParam(\"deep_data\", 1, True, 26)]))\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash, \n",
    "                            workspace_size_per_gpu_in_mb = 23,\n",
    "                            embedding_vec_size = 1,\n",
    "                            combiner = \"sum\",\n",
    "                            sparse_embedding_name = \"sparse_embedding2\",\n",
    "                            bottom_name = \"wide_data\",\n",
    "                            optimizer = optimizer))\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash, \n",
    "                            workspace_size_per_gpu_in_mb = 358,\n",
    "                            embedding_vec_size = 16,\n",
    "                            combiner = \"sum\",\n",
    "                            sparse_embedding_name = \"sparse_embedding1\",\n",
    "                            bottom_name = \"deep_data\",\n",
    "                            optimizer = optimizer))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding1\"],\n",
    "                            top_names = [\"reshape1\"],\n",
    "                            leading_dim=416))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding2\"],\n",
    "                            top_names = [\"reshape2\"],\n",
    "                            leading_dim=1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Concat,\n",
    "                            bottom_names = [\"reshape1\", \"dense\"], top_names = [\"concat1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"concat1\"],\n",
    "                            top_names = [\"fc1\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc1\"],\n",
    "                            top_names = [\"relu1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu1\"],\n",
    "                            top_names = [\"dropout1\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dropout1\"],\n",
    "                            top_names = [\"fc2\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc2\"],\n",
    "                            top_names = [\"relu2\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu2\"],\n",
    "                            top_names = [\"dropout2\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dropout2\"],\n",
    "                            top_names = [\"fc3\"],\n",
    "                            num_output=1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Add,\n",
    "                            bottom_names = [\"fc3\", \"reshape2\"],\n",
    "                            top_names = [\"add1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "                            bottom_names = [\"add1\", \"label\"],\n",
    "                            top_names = [\"loss\"]))\n",
    "model.compile()\n",
    "model.summary()\n",
    "model.graph_to_json(graph_config_file = \"wdl.json\")\n",
    "#model.save_params_to_files(\"wdl\")\n",
    "model.fit(num_epochs = 1, display = 500, eval_interval = 1000)\n",
    "\n",
    "# Get the updated embedding features in model.fit()\n",
    "# updated_model = model.get_incremental_model()\n",
    "# User defined operations to the updated_model\n",
    "# ...+\n",
    "model.set_source(source = [\"criteo_data/file_list.3.txt\", \"criteo_data/file_list.4.txt\"], \\\n",
    "                 keyset = [\"criteo_data/file_list.3.keyset\", \"criteo_data/file_list.4.keyset\"], \\\n",
    "                 eval_source = \"criteo_data/file_list.5.txt\")\n",
    "\n",
    "model.fit(num_epochs = 1, display = 500, eval_interval = 1000)\n",
    "# Get the updated embedding features in model.fit()\n",
    "# updated_model = model.get_incremental_model()\n",
    "# User defined operations to the updated_model\n",
    "# ...\n",
    "#model.dump_incremental_model_2kafka()\n",
    "model.save_params_to_files(\"wdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "03efc909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '*model': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm -r *model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ac811e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HUGECTR][03:07:03][INFO][RANK0]: Empty embedding, trained table will be stored in ./wdl_0_sparse_model\n",
      "[HUGECTR][03:07:03][INFO][RANK0]: Empty embedding, trained table will be stored in ./wdl_1_sparse_model\n",
      "HugeCTR Version: 3.2\n",
      "====================================================Model Init=====================================================\n",
      "[HUGECTR][03:07:03][INFO][RANK0]: Initialize model: wdl\n",
      "[HUGECTR][03:07:03][INFO][RANK0]: Global seed is 2195796211\n",
      "[HUGECTR][03:07:03][INFO][RANK0]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 0\n",
      "\n",
      "[HUGECTR][03:07:04][WARNING][RANK0]: Peer-to-peer access cannot be fully enabled.\n",
      "[HUGECTR][03:07:04][INFO][RANK0]: Start all2all warmup\n",
      "[HUGECTR][03:07:04][INFO][RANK0]: End all2all warmup\n",
      "[HUGECTR][03:07:04][INFO][RANK0]: Using All-reduce algorithm: NCCL\n",
      "[HUGECTR][03:07:04][INFO][RANK0]: Device 0: Tesla V100-SXM2-32GB\n",
      "[HUGECTR][03:07:04][DEBUG][RANK0]: Creating Kafka lifetime service.\n",
      "[HUGECTR][03:07:04][INFO][RANK0]: num of DataReader workers: 12\n",
      "[HUGECTR][03:07:04][INFO][RANK0]: max_vocabulary_size_per_gpu_=6029312\n",
      "[HUGECTR][03:07:04][INFO][RANK0]: max_vocabulary_size_per_gpu_=5865472\n",
      "[HUGECTR][03:07:04][INFO][RANK0]: Graph analysis to resolve tensor dependency\n",
      "===================================================Model Compile===================================================\n",
      "[HUGECTR][03:07:08][INFO][RANK0]: gpu0 start to init embedding\n",
      "[HUGECTR][03:07:08][INFO][RANK0]: gpu0 init embedding done\n",
      "[HUGECTR][03:07:08][INFO][RANK0]: gpu0 start to init embedding\n",
      "[HUGECTR][03:07:08][INFO][RANK0]: gpu0 init embedding done\n",
      "[HUGECTR][03:07:08][INFO][RANK0]: Enable HMEM-Based Parameter Server\n",
      "[HUGECTR][03:07:08][INFO][RANK0]: ./wdl_0_sparse_model not exist, create and train from scratch\n",
      "[HUGECTR][03:07:08][INFO][RANK0]: Enable HMemCache-Based Parameter Server\n",
      "[HUGECTR][03:07:08][INFO][RANK0]: ./wdl_1_sparse_model/key doesn't exist, created\n",
      "[HUGECTR][03:07:08][INFO][RANK0]: ./wdl_1_sparse_model/emb_vector doesn't exist, created\n",
      "[HUGECTR][03:07:08][INFO][RANK0]: ./wdl_1_sparse_model/Adam.m doesn't exist, created\n",
      "[HUGECTR][03:07:08][INFO][RANK0]: ./wdl_1_sparse_model/Adam.v doesn't exist, created\n",
      "[HUGECTR][03:07:09][INFO][RANK0]: Starting AUC NCCL warm-up\n",
      "[HUGECTR][03:07:09][INFO][RANK0]: Warm-up done\n",
      "===================================================Model Summary===================================================\n",
      "label                                   Dense                         Sparse                        \n",
      "label                                   dense                          wide_data,deep_data           \n",
      "(None, 1)                               (None, 13)                              \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "DistributedSlotSparseEmbeddingHash      wide_data                     sparse_embedding2             (None, 1, 1)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "DistributedSlotSparseEmbeddingHash      deep_data                     sparse_embedding1             (None, 26, 16)                \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Reshape                                 sparse_embedding1             reshape1                      (None, 416)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Reshape                                 sparse_embedding2             reshape2                      (None, 1)                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Concat                                  reshape1                      concat1                       (None, 429)                   \n",
      "                                        dense                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            concat1                       fc1                           (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc1                           relu1                         (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Dropout                                 relu1                         dropout1                      (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            dropout1                      fc2                           (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc2                           relu2                         (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Dropout                                 relu2                         dropout2                      (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            dropout2                      fc3                           (None, 1)                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Add                                     fc3                           add1                          (None, 1)                     \n",
      "                                        reshape2                                                                                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "BinaryCrossEntropyLoss                  add1                          loss                                                        \n",
      "                                        label                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "[HUGECTR][03:07:09][INFO][RANK0]: Save the model graph to wdl.json successfully\n",
      "=====================================================Model Fit=====================================================\n",
      "[HUGECTR][03:07:09][INFO][RANK0]: Use embedding training cache mode with number of training sources: 2, number of epochs: 1\n",
      "[HUGECTR][03:07:09][INFO][RANK0]: Training batchsize: 1024, evaluation batchsize: 1024\n",
      "[HUGECTR][03:07:09][INFO][RANK0]: Evaluation interval: 1000, snapshot interval: 10000\n",
      "[HUGECTR][03:07:09][INFO][RANK0]: Sparse embedding trainable: True, dense network trainable: True\n",
      "[HUGECTR][03:07:09][INFO][RANK0]: Use mixed precision: False, scaler: 1.000000, use cuda graph: True\n",
      "[HUGECTR][03:07:09][INFO][RANK0]: lr: 0.001000, warmup_steps: 1, decay_start: 0, decay_steps: 1, decay_power: 2.000000, end_lr: 0.000000\n",
      "[HUGECTR][03:07:09][INFO][RANK0]: Evaluation source file: criteo_data/file_list.2.txt\n",
      "[HUGECTR][03:07:09][INFO][RANK0]: --------------------Epoch 0, source file: criteo_data/file_list.0.txt--------------------\n",
      "[HUGECTR][03:07:09][INFO][RANK0]: Preparing embedding table for next pass\n",
      "[HUGECTR][03:07:09][INFO][RANK0]: HMEM-Cache PS: Hit rate [load]: 0 %\n",
      "[HUGECTR][03:07:10][INFO][RANK0]: --------------------Epoch 0, source file: criteo_data/file_list.1.txt--------------------\n",
      "[HUGECTR][03:07:10][INFO][RANK0]: Preparing embedding table for next pass\n",
      "[HUGECTR][03:07:10][INFO][RANK0]: HMEM-Cache PS: Hit rate [dump]: 0 %\n",
      "[HUGECTR][03:07:10][INFO][RANK0]: HMEM-Cache PS: Hit rate [load]: 0 %\n",
      "=====================================================Model Fit=====================================================\n",
      "[HUGECTR][03:07:11][INFO][RANK0]: Use embedding training cache mode with number of training sources: 2, number of epochs: 1\n",
      "[HUGECTR][03:07:11][INFO][RANK0]: Training batchsize: 1024, evaluation batchsize: 1024\n",
      "[HUGECTR][03:07:11][INFO][RANK0]: Evaluation interval: 1000, snapshot interval: 10000\n",
      "[HUGECTR][03:07:11][INFO][RANK0]: Sparse embedding trainable: True, dense network trainable: True\n",
      "[HUGECTR][03:07:11][INFO][RANK0]: Use mixed precision: False, scaler: 1.000000, use cuda graph: True\n",
      "[HUGECTR][03:07:11][INFO][RANK0]: lr: 0.001000, warmup_steps: 1, decay_start: 0, decay_steps: 1, decay_power: 2.000000, end_lr: 0.000000\n",
      "[HUGECTR][03:07:11][INFO][RANK0]: Evaluation source file: criteo_data/file_list.5.txt\n",
      "[HUGECTR][03:07:11][INFO][RANK0]: --------------------Epoch 0, source file: criteo_data/file_list.3.txt--------------------\n",
      "[HUGECTR][03:07:11][INFO][RANK0]: Preparing embedding table for next pass\n",
      "[HUGECTR][03:07:11][INFO][RANK0]: HMEM-Cache PS: Hit rate [dump]: 76.75 %\n",
      "[HUGECTR][03:07:11][INFO][RANK0]: HMEM-Cache PS: Hit rate [load]: 69.83 %\n",
      "[HUGECTR][03:07:11][INFO][RANK0]: --------------------Epoch 0, source file: criteo_data/file_list.4.txt--------------------\n",
      "[HUGECTR][03:07:11][INFO][RANK0]: Preparing embedding table for next pass\n",
      "[HUGECTR][03:07:12][INFO][RANK0]: HMEM-Cache PS: Hit rate [dump]: 63.37 %\n",
      "[HUGECTR][03:07:12][INFO][RANK0]: HMEM-Cache PS: Hit rate [load]: 66.58 %\n",
      "[HUGECTR][03:07:13][INFO][RANK0]: HMEM-Cache PS: Hit rate [dump]: 63.77 %\n",
      "[HUGECTR][03:07:13][INFO][RANK0]: Updating sparse model in SSD [DONE]\n",
      "[HUGECTR][03:07:13][INFO][RANK0]: Sync blocks from HMEM-Cache to SSD\n",
      " \u001b[38;2;89;255;89m ████████████████████████████████████████▏ \u001b[1m\u001b[31m100.0% \u001b[34m[   2/   2 | 58.5 Hz | 0s<0s]  \u001b[0m\u001b[32m\u001b[0mm\n",
      "[HUGECTR][03:07:13][INFO][RANK0]: Dumping dense weights to file, successful\n",
      "[HUGECTR][03:07:13][INFO][RANK0]: Dumping dense optimizer states to file, successful\n",
      "[HUGECTR][03:07:13][INFO][RANK0]: Dumping untrainable weights to file, successful\n",
      "[HUGECTR][03:07:14][DEBUG][RANK0]: Destroying Kafka lifetime service.\n"
     ]
    }
   ],
   "source": [
    "!python wdl_demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92de4dc0",
   "metadata": {},
   "source": [
    "## WDL Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e34b1b",
   "metadata": {},
   "source": [
    "### Inference using HugeCTR python API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74bef569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wdl_predict.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 'wdl_predict.py'\n",
    "from hugectr.inference import InferenceParams, CreateInferenceSession\n",
    "import hugectr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from mpi4py import MPI\n",
    "def wdl_inference(model_name='wdl', network_file='wdl.json', dense_file='wdl_dense_0.model', \\\n",
    "                  embedding_file_list=['wdl_0_sparse_model', 'wdl_1_sparse_model'], data_file='infer_data.csv',\\\n",
    "                  enable_cache=True, rocksdb_path=\"\"):\n",
    "    CATEGORICAL_COLUMNS=[\"C1_C2\",\"C3_C4\"] + [\"C\" + str(x) for x in range(1, 27)]\n",
    "    CONTINUOUS_COLUMNS=[\"I\" + str(x) for x in range(1, 14)]\n",
    "    LABEL_COLUMNS = ['label']\n",
    "    test_df=pd.read_csv(data_file,sep=',')\n",
    "    print(test_df)\n",
    "    config_file = network_file\n",
    "    row_ptrs = list(range(0,11, 2)) + list(range(0,131))\n",
    "    print(row_ptrs)\n",
    "    dense_features =  list(test_df[CONTINUOUS_COLUMNS].values.flatten())\n",
    "    print(test_df[CONTINUOUS_COLUMNS])\n",
    "    test_df[CATEGORICAL_COLUMNS].astype(np.int64)\n",
    "    embedding_columns = list((test_df[CATEGORICAL_COLUMNS]).values.flatten())\n",
    "\n",
    "    # create parameter server, embedding cache and inference session\n",
    "    inference_params = InferenceParams(model_name = model_name,\n",
    "                                max_batchsize = 64,\n",
    "                                hit_rate_threshold = 0.5,\n",
    "                                dense_model_file = dense_file,\n",
    "                                sparse_model_files = embedding_file_list,\n",
    "                                device_id = 0,\n",
    "                                use_gpu_embedding_cache = enable_cache,\n",
    "                                cache_size_percentage = 0.9,\n",
    "                                i64_input_key = True,\n",
    "                                use_mixed_precision = False)\n",
    "    inference_session = CreateInferenceSession(config_file, inference_params)\n",
    "    output = inference_session.predict(dense_features, embedding_columns, row_ptrs)\n",
    "    print(\"WDL multi-embedding table inference result is {}\".format(output))\n",
    "\n",
    "wdl_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e246d0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label        I1        I2        I3  ...    C23    C24    C25    C26\n",
      "0    0.0  0.005929  0.029422  0.010309  ...  55468  57567  61479  61516\n",
      "1    0.0  0.000000  0.051489  0.000000  ...  54391  57477  61461  61516\n",
      "2    0.0  0.003953  0.207005  0.000000  ...  54396  58410  61462  61512\n",
      "3    0.0  0.021739  0.017163  0.164948  ...  56631  59988  61497  61521\n",
      "4    0.0  0.021739  0.024168  0.092784  ...  56578  60317  61462  61520\n",
      "\n",
      "[5 rows x 42 columns]\n",
      "[0, 2, 4, 6, 8, 10, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130]\n",
      "         I1        I2        I3  ...       I11       I12       I13\n",
      "0  0.005929  0.029422  0.010309  ...  0.000000  0.000073  0.005495\n",
      "1  0.000000  0.051489  0.000000  ...  0.031579  0.699773  0.000000\n",
      "2  0.003953  0.207005  0.000000  ...  0.021053  0.000000  0.000000\n",
      "3  0.021739  0.017163  0.164948  ...  0.010526  0.000658  0.082418\n",
      "4  0.021739  0.024168  0.092784  ...  0.000000  0.096687  0.054945\n",
      "\n",
      "[5 rows x 13 columns]\n",
      "[HUGECTR][12:17:52][INFO][RANK0]: default_emb_vec_value is not specified using default: 0.000000\n",
      "[HUGECTR][12:17:52][INFO][RANK0]: default_emb_vec_value is not specified using default: 0.000000\n",
      "[HUGECTR][12:17:52][INFO][RANK0]: Creating ParallelHashMap CPU database backend...\n",
      "[HUGECTR][12:17:52][INFO][RANK0]: Created parallel (16 partitions) blank database backend in local memory!\n",
      "[HUGECTR][12:17:52][INFO][RANK0]: ParallelHashMap backend. Table: hctr_et.wdl.sparse_embedding2. Inserted 71121 / 71121 pairs.\n",
      "[HUGECTR][12:17:52][INFO][RANK0]: Table: hctr_et.wdl.sparse_embedding2; cached 71121 / 71121 embeddings in CPU memory database!\n",
      "[HUGECTR][12:17:52][INFO][RANK0]: ParallelHashMap backend. Table: hctr_et.wdl.sparse_embedding1. Inserted 60117 / 60117 pairs.\n",
      "[HUGECTR][12:17:52][INFO][RANK0]: Table: hctr_et.wdl.sparse_embedding1; cached 60117 / 60117 embeddings in CPU memory database!\n",
      "[HUGECTR][12:17:52][DEBUG][RANK0]: Real-time subscribers created!\n",
      "[HUGECTR][12:17:52][INFO][RANK0]: Create embedding cache in device 0.\n",
      "[HUGECTR][12:17:52][INFO][RANK0]: Use GPU embedding cache: True, cache size percentage: 0.900000\n",
      "[HUGECTR][12:17:52][INFO][RANK0]: Configured cache hit rate threshold: 0.500000\n",
      "[HUGECTR][12:17:52][INFO][RANK0]: Global seed is 2585588841\n",
      "[HUGECTR][12:17:52][INFO][RANK0]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 0\n",
      "\n",
      "[HUGECTR][12:17:53][WARNING][RANK0]: Peer-to-peer access cannot be fully enabled.\n",
      "[HUGECTR][12:17:53][INFO][RANK0]: Start all2all warmup\n",
      "[HUGECTR][12:17:53][INFO][RANK0]: End all2all warmup\n",
      "[HUGECTR][12:17:53][INFO][RANK0]: Model name: wdl\n",
      "[HUGECTR][12:17:53][INFO][RANK0]: Use mixed precision: False\n",
      "[HUGECTR][12:17:53][INFO][RANK0]: Use cuda graph: True\n",
      "[HUGECTR][12:17:53][INFO][RANK0]: Max batchsize: 64\n",
      "[HUGECTR][12:17:53][INFO][RANK0]: Use I64 input key: True\n",
      "[HUGECTR][12:17:53][INFO][RANK0]: start create embedding for inference\n",
      "[HUGECTR][12:17:53][INFO][RANK0]: sparse_input name wide_data\n",
      "[HUGECTR][12:17:53][INFO][RANK0]: sparse_input name deep_data\n",
      "[HUGECTR][12:17:53][INFO][RANK0]: create embedding for inference success\n",
      "[HUGECTR][12:17:53][INFO][RANK0]: Inference stage skip BinaryCrossEntropyLoss layer, replaced by Sigmoid layer\n",
      "[HUGECTR][12:17:54][INFO][RANK0]: *****Insert embedding cache of model wdl on device 0*****\n",
      "[HUGECTR][12:17:54][INFO][RANK0]: Looking up 10 embeddings (each with 1 values)...\n",
      "[HUGECTR][12:17:54][INFO][RANK0]: ParallelHashMap backend. Table: hctr_et.wdl.sparse_embedding2. Fetched 9 / 10 values.\n",
      "[HUGECTR][12:17:54][INFO][RANK0]: ParallelHashMap: 9 hits, 1 missing!\n",
      "[HUGECTR][12:17:54][INFO][RANK0]: Parameter server lookup of 9 / 10 embeddings took 158 us.\n",
      "[HUGECTR][12:17:54][INFO][RANK0]: Looking up 103 embeddings (each with 16 values)...\n",
      "[HUGECTR][12:17:54][INFO][RANK0]: ParallelHashMap backend. Table: hctr_et.wdl.sparse_embedding1. Fetched 103 / 103 values.\n",
      "[HUGECTR][12:17:54][INFO][RANK0]: ParallelHashMap: 103 hits, 0 missing!\n",
      "[HUGECTR][12:17:54][INFO][RANK0]: Parameter server lookup of 103 / 103 embeddings took 914 us.\n",
      "WDL multi-embedding table inference result is [0.03038935735821724, 0.026247011497616768, 0.04919133335351944, 0.05634024366736412, 0.02710285410284996]\n"
     ]
    }
   ],
   "source": [
    "!python wdl_predict.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aadf174",
   "metadata": {},
   "source": [
    "### Inference using Triton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6776cb",
   "metadata": {},
   "source": [
    "Please make sure you have started triton server according to the HugeCTR_Hierachy_Deployment notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c52acc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tritonclient[all]\n",
      "  Downloading tritonclient-2.16.0-py3-none-manylinux1_x86_64.whl (7.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.8 MB 5.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.19.1 in /usr/local/lib/python3.8/dist-packages (from tritonclient[all]) (1.19.4)\n",
      "Collecting python-rapidjson>=0.9.1\n",
      "  Downloading python_rapidjson-1.5-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 6.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting geventhttpclient>=1.4.4\n",
      "  Downloading geventhttpclient-1.5.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 12.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: grpcio>=1.31.0 in /usr/local/lib/python3.8/dist-packages (from tritonclient[all]) (1.39.0)\n",
      "Requirement already satisfied: protobuf>=3.5.0 in /usr/local/lib/python3.8/dist-packages (from tritonclient[all]) (3.19.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from geventhttpclient>=1.4.4->tritonclient[all]) (2021.5.30)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from geventhttpclient>=1.4.4->tritonclient[all]) (1.15.0)\n",
      "Collecting brotli\n",
      "  Downloading Brotli-1.0.9-cp38-cp38-manylinux1_x86_64.whl (357 kB)\n",
      "\u001b[K     |████████████████████████████████| 357 kB 5.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: gevent>=0.13 in /usr/local/lib/python3.8/dist-packages (from geventhttpclient>=1.4.4->tritonclient[all]) (21.8.0)\n",
      "Requirement already satisfied: greenlet<2.0,>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from gevent>=0.13->geventhttpclient>=1.4.4->tritonclient[all]) (1.1.2)\n",
      "Requirement already satisfied: zope.interface in /usr/local/lib/python3.8/dist-packages (from gevent>=0.13->geventhttpclient>=1.4.4->tritonclient[all]) (5.4.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from gevent>=0.13->geventhttpclient>=1.4.4->tritonclient[all]) (58.1.0)\n",
      "Requirement already satisfied: zope.event in /usr/local/lib/python3.8/dist-packages (from gevent>=0.13->geventhttpclient>=1.4.4->tritonclient[all]) (4.5.0)\n",
      "Installing collected packages: python-rapidjson, brotli, tritonclient, geventhttpclient\n",
      "Successfully installed brotli-1.0.9 geventhttpclient-1.5.3 python-rapidjson-1.5 tritonclient-2.16.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tritonclient[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4ec536",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wdlpredict_triton.py\n",
    "from tritonclient.utils import *\n",
    "import tritonclient.http  as httpclient\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "model_name = 'wdl'\n",
    "CATEGORICAL_COLUMNS=[\"C\" + str(x) for x in range(1, 27)]+[\"C1_C2\",\"C3_C4\"]\n",
    "CONTINUOUS_COLUMNS=[\"I\" + str(x) for x in range(1, 14)]\n",
    "LABEL_COLUMNS = ['label']\n",
    "test_df=pd.read_csv(\"infer_data.csv\",sep=',')\n",
    "\n",
    "with httpclient.InferenceServerClient(\"localhost:8000\") as client:\n",
    "    dense_features = np.array([list(test_df[CONTINUOUS_COLUMNS].values.flatten())],dtype='float32')\n",
    "    embedding_columns = np.array([list((test_df[CATEGORICAL_COLUMNS]+shift).values.flatten())],dtype='int64')\n",
    "    row_ptrs = list(range(0,11, 2)) + list(range(0,131))\n",
    "    \n",
    "    inputs = [\n",
    "        httpclient.InferInput(\"DES\", dense_features.shape,\n",
    "                              np_to_triton_dtype(dense_features.dtype)),\n",
    "        httpclient.InferInput(\"CATCOLUMN\", embedding_columns.shape,\n",
    "                              np_to_triton_dtype(embedding_columns.dtype)),\n",
    "        httpclient.InferInput(\"ROWINDEX\", row_ptrs.shape,\n",
    "                              np_to_triton_dtype(row_ptrs.dtype)),\n",
    "\n",
    "    ]\n",
    "\n",
    "    inputs[0].set_data_from_numpy(dense_features)\n",
    "    inputs[1].set_data_from_numpy(embedding_columns)\n",
    "    inputs[2].set_data_from_numpy(row_ptrs)\n",
    "    outputs = [\n",
    "        httpclient.InferRequestedOutput(\"OUTPUT0\")\n",
    "    ]\n",
    "\n",
    "    response = client.infer(model_name,\n",
    "                            inputs,\n",
    "                            request_id=str(1),\n",
    "                            outputs=outputs)\n",
    "\n",
    "    result = response.get_response()\n",
    "    print(result)\n",
    "    print(\"Prediction Result:\")\n",
    "    print(response.as_numpy(\"OUTPUT0\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
