{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a338d8af",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# HugeCTR Continuous Training Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d363db9",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In HugeCTR version 3.3, we finished the whole pipeline of parameter server, including \n",
    "1. The parameter dumping interface from training to kafka.\n",
    "2. Redis as a level2 cache.\n",
    "3. RocksDB as a persistence storage.\n",
    "4. Embedding cache update mechanism.\n",
    "\n",
    "\n",
    "The purpose of this notebook is to give you a brief idea of how parameter server works in terms of the flow of parameters. \n",
    "\n",
    "## Table of Contents\n",
    "-  [Data Preparation](#1)\n",
    "-  [Data Preprocessing using NVTabular](#2)\n",
    "-  [Kafka broker start](#3)\n",
    "-  [Wide&Deep Continuous Training Demo](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8263efa",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfc3273",
   "metadata": {},
   "source": [
    "1. Firstly, we made a folder to store our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccca9c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘criteo_script’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir criteo_data\n",
    "!mkdir criteo_script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b953cf",
   "metadata": {},
   "source": [
    "2. Download Criteo Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315ccdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://azuremlsampleexperiments.blob.core.windows.net/criteo/day_1.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda17b84",
   "metadata": {},
   "source": [
    "**NOTE**: Replace `1` with a value from [0, 23] to use a different day.\n",
    "\n",
    "During preprocessing, the amount of data, which is used to speed up the preprocessing, fill missing values, and remove the feature values that are considered rare, is further reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65c3860",
   "metadata": {},
   "source": [
    "3. Thirdly, preprocess the data and split the data into 6 parts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1e4a34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocess.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocess.sh\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "if [[ $# -lt 3 ]]; then\n",
    "  echo \"Usage: preprocess.sh [DATASET_NO.] [DST_DATA_DIR] [SCRIPT_TYPE] [SCRIPT_TYPE_SPECIFIC_ARGS...]\"\n",
    "  exit 2\n",
    "fi\n",
    "\n",
    "DST_DATA_DIR=$2\n",
    "\n",
    "echo \"Warning: existing $DST_DATA_DIR is erased\"\n",
    "rm -rf $DST_DATA_DIR\n",
    "\n",
    "if [[ $3 == \"nvt\" ]]; then\n",
    "  if [[ $# -ne 6 ]]; then\n",
    "\t\techo \"Usage: preprocess.sh [DATASET_NO.] [DST_DATA_DIR] nvt [IS_PARQUET_FORMAT] [IS_CRITEO_MODE] [IS_FEATURE_CROSSED]\"\n",
    "    exit 2\n",
    "\tfi\n",
    "\techo \"Preprocessing script: NVTabular\"\n",
    "elif [[ $3 == \"perl\" ]]; then\n",
    "  if [[ $# -ne 4 ]]; then\n",
    "\t\techo \"Usage: preprocess.sh [DATASET_NO.] [DST_DATA_DIR] perl [NUM_SLOTS]\"\n",
    "    exit 2\n",
    "\tfi\n",
    "\techo \"Preprocessing script: Perl\"\n",
    "elif [[ $3 == \"pandas\" ]]; then\n",
    "  if [[ $# -lt 5 ]]; then\n",
    "\t\techo \"Usage: preprocess.sh [DATASET_NO.] [DST_DATA_DIR] pandas [IS_DENSE_NORMALIZED] [IS_FEATURE_CROSSED] (FILE_LIST_LENGTH)\"\n",
    "    exit 2\n",
    "\tfi\n",
    "\techo \"Preprocessing script: Pandas\"\n",
    "else\n",
    "\techo \"Error: $3 is an invalid script type. Pick one from {nvt, perl, pandas}.\"\n",
    "\texit 2\n",
    "fi\n",
    "\n",
    "SCRIPT_TYPE=$3\n",
    "\n",
    "echo \"Getting the first few examples from the uncompressed dataset...\"\n",
    "mkdir -p $DST_DATA_DIR/train                         && \\\n",
    "mkdir -p $DST_DATA_DIR/val                           && \\\n",
    "head -n 500000 day_$1 > $DST_DATA_DIR/day_$1_small\n",
    "if [ $? -ne 0 ]; then\n",
    "\techo \"Warning: fallback to find original compressed data day_$1.gz...\"\n",
    "\techo \"Decompressing day_$1.gz...\"\n",
    "\tgzip -d -c day_$1.gz > day_$1\n",
    "\tif [ $? -ne 0 ]; then\n",
    "\t\techo \"Error: failed to decompress the file.\"\n",
    "\t\texit 2\n",
    "\tfi\n",
    "\thead -n 500000 day_$1 > $DST_DATA_DIR/day_$1_small\n",
    "\tif [ $? -ne 0 ]; then\n",
    "\t\techo \"Error: day_$1 file\"\n",
    "\t\texit 2\n",
    "\tfi\n",
    "fi\n",
    "\n",
    "echo \"Counting the number of samples in day_$1 dataset...\"\n",
    "total_count=$(wc -l $DST_DATA_DIR/day_$1_small)\n",
    "total_count=(${total_count})\n",
    "echo \"The first $total_count examples will be used in day_$1 dataset.\"\n",
    "\n",
    "echo \"Shuffling dataset...\"\n",
    "shuf $DST_DATA_DIR/day_$1_small > $DST_DATA_DIR/day_$1_shuf\n",
    "\n",
    "train_count=$(( total_count * 8 / 10))\n",
    "valtest_count=$(( total_count - train_count ))\n",
    "val_count=$(( valtest_count * 5 / 10 ))\n",
    "test_count=$(( valtest_count - val_count  ))\n",
    "\n",
    "split_dataset()\n",
    "{\n",
    "\techo \"Splitting into $train_count-sample training, $val_count-sample val, and $test_count-sample test datasets...\"\n",
    "\thead -n $train_count $DST_DATA_DIR/$1 > $DST_DATA_DIR/train/train.txt          && \\\n",
    "\ttail -n $valtest_count $DST_DATA_DIR/$1 > $DST_DATA_DIR/val/valtest.txt        && \\\n",
    "\thead -n $val_count $DST_DATA_DIR/val/valtest.txt > $DST_DATA_DIR/val/val.txt   && \\\n",
    "\ttail -n $test_count $DST_DATA_DIR/val/valtest.txt > $DST_DATA_DIR/val/test.txt\n",
    "\n",
    "\tif [ $? -ne 0 ]; then\n",
    "\t\texit 2\n",
    "\tfi\n",
    "}\n",
    "\n",
    "echo \"Preprocessing...\"\n",
    "if [[ $SCRIPT_TYPE == \"nvt\" ]]; then\n",
    "\tIS_PARQUET_FORMAT=$4\n",
    "\tIS_CRITEO_MODE=$5\n",
    "\tFEATURE_CROSS_LIST_OPTION=\"\"\n",
    "\tif [[ ( $IS_CRITEO_MODE -eq 0 ) && ( $6 -eq 1 ) ]]; then\n",
    "\t\tFEATURE_CROSS_LIST_OPTION=\"--feature_cross_list C1_C2,C3_C4\"\n",
    "\t\techo $FEATURE_CROSS_LIST_OPTION\n",
    "\tfi\n",
    "  split_dataset day_$1_shuf\n",
    "  python3 criteo_script/preprocess_nvt.py \\\n",
    "\t\t--data_path $DST_DATA_DIR             \\\n",
    "\t\t--out_path $DST_DATA_DIR              \\\n",
    "\t\t--freq_limit 6                        \\\n",
    "\t\t--device_limit_frac 0.5               \\\n",
    "\t\t--device_pool_frac 0.5                \\\n",
    "\t\t--out_files_per_proc 8                \\\n",
    "\t\t--devices \"0\"                         \\\n",
    "\t\t--num_io_threads 2                    \\\n",
    "        --parquet_format=$IS_PARQUET_FORMAT   \\\n",
    "\t\t--criteo_mode=$IS_CRITEO_MODE         \\\n",
    "\t\t$FEATURE_CROSS_LIST_OPTION\n",
    "\n",
    "elif [[ $SCRIPT_TYPE == \"perl\" ]]; then\n",
    "\tNUM_SLOT=$4\n",
    "  split_dataset day_$1_shuf\n",
    "\tperl criteo_script_legacy/preprocess.pl $DST_DATA_DIR/train/train.txt $DST_DATA_DIR/val/val.txt $DST_DATA_DIR/val/test.txt                      && \\\n",
    "\tcriteo2hugectr_legacy $NUM_SLOT $DST_DATA_DIR/train/train.txt.out $DST_DATA_DIR/train/sparse_embedding $DST_DATA_DIR/file_list.txt && \\\n",
    "\tcriteo2hugectr_legacy $NUM_SLOT $DST_DATA_DIR/val/test.txt.out $DST_DATA_DIR/val/sparse_embedding $DST_DATA_DIR/file_list_test.txt\n",
    "\n",
    "elif [[ $SCRIPT_TYPE == \"pandas\" ]]; then\n",
    "\tpython3 criteo_script/preprocess.py                 \\\n",
    "\t\t--src_csv_path=$DST_DATA_DIR/day_$1_shuf          \\\n",
    "\t\t--dst_csv_path=$DST_DATA_DIR/day_$1_shuf.out      \\\n",
    "\t\t--normalize_dense=$4 --feature_cross=$5      &&   \\\n",
    "  split_dataset day_$1_shuf.out\n",
    "\tNUM_WIDE_KEYS=\"\"\n",
    "\tif [[ $5 -ne 0 ]]; then\n",
    "\t\tNUM_WIDE_KEYS=2\n",
    "\tfi\n",
    "\n",
    "  FILE_LIST_LENGTH=\"\"\n",
    "  if [[ $# -gt 5 ]]; then\n",
    "    FILE_LIST_LENGTH=$6\n",
    "\tfi\n",
    "\n",
    "\tcriteo2hugectr $DST_DATA_DIR/train/train.txt $DST_DATA_DIR/train/sparse_embedding $DST_DATA_DIR/file_list.txt $NUM_WIDE_KEYS $FILE_LIST_LENGTH && \\\n",
    "\tcriteo2hugectr $DST_DATA_DIR/val/test.txt $DST_DATA_DIR/val/sparse_embedding $DST_DATA_DIR/file_list_test.txt $NUM_WIDE_KEYS $FILE_LIST_LENGTH\n",
    "fi\n",
    "\n",
    "if [ $? -ne 0 ]; then\n",
    "\texit 2\n",
    "fi\n",
    "\n",
    "echo \"All done!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8cdb12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting criteo_script/preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile criteo_script/preprocess.py\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import tempfile\n",
    "\n",
    "from six.moves import urllib\n",
    "import urllib.request \n",
    "\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import logging\n",
    "import concurrent.futures as cf\n",
    "from traceback import print_exc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing as skp\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(message)s')\n",
    "logging.root.setLevel(logging.NOTSET)\n",
    "\n",
    "NUM_INTEGER_COLUMNS = 13\n",
    "NUM_CATEGORICAL_COLUMNS = 26\n",
    "NUM_TOTAL_COLUMNS = 1 + NUM_INTEGER_COLUMNS + NUM_CATEGORICAL_COLUMNS\n",
    "\n",
    "MAX_NUM_WORKERS = NUM_TOTAL_COLUMNS\n",
    "\n",
    "INT_NAN_VALUE = np.iinfo(np.int32).min\n",
    "CAT_NAN_VALUE = '80000000'\n",
    "\n",
    "def idx2key(idx):\n",
    "    if idx == 0:\n",
    "        return 'label'\n",
    "    return 'I' + str(idx) if idx <= NUM_INTEGER_COLUMNS else 'C' + str(idx - NUM_INTEGER_COLUMNS)\n",
    "\n",
    "def _fill_missing_features_and_split(chunk, series_list_dict):\n",
    "    for cid, col in enumerate(chunk.columns):\n",
    "        NAN_VALUE = INT_NAN_VALUE if cid <= NUM_INTEGER_COLUMNS else CAT_NAN_VALUE\n",
    "        result_series = chunk[col].fillna(NAN_VALUE)\n",
    "        series_list_dict[col].append(result_series)\n",
    "\n",
    "def _merge_and_transform_series(src_series_list, col, dense_cols,\n",
    "                                normalize_dense):\n",
    "    result_series = pd.concat(src_series_list)\n",
    "\n",
    "    if col != 'label':\n",
    "        unique_value_counts = result_series.value_counts()\n",
    "        unique_value_counts = unique_value_counts.loc[unique_value_counts >= 6]\n",
    "        unique_value_counts = set(unique_value_counts.index.values)\n",
    "        NAN_VALUE = INT_NAN_VALUE if col.startswith('I') else CAT_NAN_VALUE\n",
    "        result_series = result_series.apply(\n",
    "                lambda x: x if x in unique_value_counts else NAN_VALUE)\n",
    "\n",
    "    if col == 'label' or col in dense_cols:\n",
    "        result_series = result_series.astype(np.int64)\n",
    "        le = skp.LabelEncoder()\n",
    "        result_series = pd.DataFrame(le.fit_transform(result_series))\n",
    "        if col != 'label':\n",
    "            result_series = result_series + 1\n",
    "    else:\n",
    "        oe = skp.OrdinalEncoder(dtype=np.int64)\n",
    "        result_series = pd.DataFrame(oe.fit_transform(pd.DataFrame(result_series)))\n",
    "        result_series = result_series + 1\n",
    "\n",
    "\n",
    "    if normalize_dense != 0:\n",
    "        if col in dense_cols:\n",
    "            mms = skp.MinMaxScaler(feature_range=(0,1))\n",
    "            result_series = pd.DataFrame(mms.fit_transform(result_series))\n",
    "\n",
    "    result_series.columns = [col]\n",
    "\n",
    "    min_max = (np.int64(result_series[col].min()), np.int64(result_series[col].max()))\n",
    "    if col != 'label':\n",
    "        logging.info('column {} [{}, {}]'.format(col, str(min_max[0]),str(min_max[1])))\n",
    "\n",
    "    return [result_series, min_max]\n",
    "\n",
    "def _convert_to_string(series):\n",
    "    return series.astype(str)\n",
    "\n",
    "def _merge_columns_and_feature_cross(series_list, min_max, feature_pairs,\n",
    "                                     feature_cross):\n",
    "    name_to_series = dict()\n",
    "    for series in series_list:\n",
    "        name_to_series[series.columns[0]] = series.iloc[:,0]\n",
    "    df = pd.DataFrame(name_to_series)\n",
    "    cols = [idx2key(idx) for idx in range(0, NUM_TOTAL_COLUMNS)]\n",
    "    df = df.reindex(columns=cols)\n",
    "\n",
    "    offset = np.int64(0)\n",
    "    for col in cols:\n",
    "        if col != 'label' and col.startswith('I') == False:\n",
    "            df[col] += offset\n",
    "            logging.info('column {} offset {}'.format(col, str(offset)))\n",
    "            offset += min_max[col][1]\n",
    "\n",
    "    if feature_cross != 0:\n",
    "        for idx, pair in enumerate(feature_pairs):\n",
    "            col0 = pair[0]\n",
    "            col1 = pair[1]\n",
    "\n",
    "            col1_width = int(min_max[col1][1] - min_max[col1][0] + 1)\n",
    "\n",
    "            crossed_column_series = df[col0] * col1_width + df[col1]\n",
    "            oe = skp.OrdinalEncoder(dtype=np.int64)\n",
    "            crossed_column_series = pd.DataFrame(oe.fit_transform(pd.DataFrame(crossed_column_series)))\n",
    "            crossed_column_series = crossed_column_series + 1\n",
    "\n",
    "            crossed_column = col0 + '_' + col1\n",
    "            df.insert(NUM_INTEGER_COLUMNS + 1 + idx, crossed_column, crossed_column_series)\n",
    "            crossed_column_max_val = np.int64(df[crossed_column].max())\n",
    "            logging.info('column {} [{}, {}]'.format(\n",
    "                crossed_column,\n",
    "                str(df[crossed_column].min()),\n",
    "                str(crossed_column_max_val)))\n",
    "            df[crossed_column] += offset\n",
    "            logging.info('column {} offset {}'.format(crossed_column, str(offset)))\n",
    "            offset += crossed_column_max_val\n",
    "\n",
    "    return df\n",
    "\n",
    "def _wait_futures_and_reset(futures):\n",
    "    for future in futures:\n",
    "        result = future.result()\n",
    "        if result:\n",
    "            print(result)\n",
    "    futures = list()\n",
    "\n",
    "def _process_chunks(executor, chunks_to_process, op, *argv):\n",
    "    futures = list()\n",
    "    for chunk in chunks_to_process:\n",
    "        argv_list = list(argv)\n",
    "        argv_list.insert(0, chunk)\n",
    "        new_argv = tuple(argv_list)\n",
    "        future = executor.submit(op, *new_argv)\n",
    "        futures.append(future)\n",
    "    _wait_futures_and_reset(futures)\n",
    "\n",
    "def preprocess(src_txt_name, dst_txt_name, normalize_dense, feature_cross):\n",
    "    cols = [idx2key(idx) for idx in range(0, NUM_TOTAL_COLUMNS)]\n",
    "    series_list_dict = dict()\n",
    "\n",
    "    with cf.ThreadPoolExecutor(max_workers=MAX_NUM_WORKERS) as executor:\n",
    "        logging.info('read a CSV file')\n",
    "        reader = pd.read_csv(src_txt_name, sep='\\t',\n",
    "                             names=cols,\n",
    "                             chunksize=131072)\n",
    "\n",
    "        logging.info('_fill_missing_features_and_split')\n",
    "        for col in cols:\n",
    "            series_list_dict[col] = list()\n",
    "        _process_chunks(executor, reader, _fill_missing_features_and_split,\n",
    "                        series_list_dict)\n",
    "\n",
    "    with cf.ProcessPoolExecutor(max_workers=MAX_NUM_WORKERS) as executor:\n",
    "        logging.info('_merge_and_transform_series')\n",
    "        futures = list()\n",
    "        dense_cols = [idx2key(idx+1) for idx in range(NUM_INTEGER_COLUMNS)]\n",
    "        dst_series_list = list()\n",
    "        min_max = dict()\n",
    "        for col, src_series_list in series_list_dict.items():\n",
    "            future = executor.submit(_merge_and_transform_series,\n",
    "                                     src_series_list, col, dense_cols,\n",
    "                                     normalize_dense)\n",
    "            futures.append(future)\n",
    "\n",
    "        for future in futures:\n",
    "            col = None\n",
    "            for idx, ret in enumerate(future.result()):\n",
    "                try:\n",
    "                    if idx == 0:\n",
    "                        col = ret.columns[0]\n",
    "                        dst_series_list.append(ret)\n",
    "                    else:\n",
    "                        min_max[col] = ret\n",
    "                except:\n",
    "                    print_exc()\n",
    "        futures = list()\n",
    "\n",
    "        logging.info('_merge_columns_and_feature_cross')\n",
    "        feature_pairs = [('C1', 'C2'), ('C3', 'C4')]\n",
    "        df = _merge_columns_and_feature_cross(dst_series_list, min_max, feature_pairs,\n",
    "                                              feature_cross)\n",
    "\n",
    "        \n",
    "        logging.info('_convert_to_string')\n",
    "        futures = dict()\n",
    "        for col in cols:\n",
    "            future = executor.submit(_convert_to_string, df[col])\n",
    "            futures[col] = future\n",
    "        if feature_cross != 0:\n",
    "            for pair in feature_pairs:\n",
    "                col = pair[0] + '_' + pair[1]\n",
    "                future = executor.submit(_convert_to_string, df[col])\n",
    "                futures[col] = future\n",
    "\n",
    "        logging.info('_store_to_df')\n",
    "        for col, future in futures.items():\n",
    "            ret = future.result()\n",
    "            try:\n",
    "                df[col] = ret\n",
    "            except:\n",
    "                print_exc()\n",
    "        futures = dict()\n",
    "\n",
    "        logging.info('write to a CSV file')\n",
    "        df.to_csv(dst_txt_name, sep=' ', header=False, index=False)\n",
    "\n",
    "        logging.info('done!')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    arg_parser = argparse.ArgumentParser(description='Preprocssing Criteo Dataset')\n",
    "\n",
    "    arg_parser.add_argument('--src_csv_path', type=str, required=True)\n",
    "    arg_parser.add_argument('--dst_csv_path', type=str, required=True)\n",
    "    arg_parser.add_argument('--normalize_dense', type=int, default=1)\n",
    "    arg_parser.add_argument('--feature_cross', type=int, default=1)\n",
    "\n",
    "    args = arg_parser.parse_args()\n",
    "\n",
    "    src_csv_path = args.src_csv_path\n",
    "    dst_csv_path = args.dst_csv_path\n",
    "\n",
    "    normalize_dense = args.normalize_dense\n",
    "    feature_cross = args.feature_cross\n",
    "\n",
    "    if os.path.exists(src_csv_path) == False:\n",
    "        sys.exit('ERROR: the file \\'{}\\' doesn\\'t exist'.format(src_csv_path))\n",
    "\n",
    "    if os.path.exists(dst_csv_path) == True:\n",
    "        sys.exit('ERROR: the file \\'{}\\' exists'.format(dst_csv_path))\n",
    "\n",
    "    preprocess(src_csv_path, dst_csv_path, normalize_dense, feature_cross)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685e3d8e",
   "metadata": {},
   "source": [
    "4. Run the preprocess script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c829a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: existing criteo_data is erased\n",
      "Preprocessing script: Pandas\n",
      "Getting the first few examples from the uncompressed dataset...\n",
      "Counting the number of samples in day_0 dataset...\n",
      "The first 500000 examples will be used in day_0 dataset.\n",
      "Shuffling dataset...\n",
      "Preprocessing...\n",
      "2021-11-26 07:00:29,359 read a CSV file\n",
      "2021-11-26 07:00:29,361 _fill_missing_features_and_split\n",
      "2021-11-26 07:00:31,876 _merge_and_transform_series\n",
      "2021-11-26 07:00:32,300 column I1 [0, 1]\n",
      "2021-11-26 07:00:32,342 column I3 [0, 1]\n",
      "2021-11-26 07:00:32,369 column I4 [0, 1]\n",
      "2021-11-26 07:00:32,379 column I5 [0, 1]\n",
      "2021-11-26 07:00:32,381 column I2 [0, 1]\n",
      "2021-11-26 07:00:32,426 column I6 [0, 1]\n",
      "2021-11-26 07:00:32,435 column I7 [0, 0]\n",
      "2021-11-26 07:00:32,463 column I10 [0, 1]\n",
      "2021-11-26 07:00:32,488 column I11 [0, 1]\n",
      "2021-11-26 07:00:32,533 column I13 [0, 1]\n",
      "2021-11-26 07:00:32,551 column I9 [0, 0]\n",
      "2021-11-26 07:00:32,559 column I8 [0, 1]\n",
      "2021-11-26 07:00:32,598 column I12 [0, 1]\n",
      "2021-11-26 07:00:32,751 column C1 [1, 3559]\n",
      "2021-11-26 07:00:32,821 column C2 [1, 3830]\n",
      "2021-11-26 07:00:32,842 column C4 [1, 1843]\n",
      "2021-11-26 07:00:32,880 column C3 [1, 5986]\n",
      "2021-11-26 07:00:32,881 column C6 [1, 3]\n",
      "2021-11-26 07:00:32,929 column C5 [1, 4105]\n",
      "2021-11-26 07:00:32,956 column C8 [1, 810]\n",
      "2021-11-26 07:00:32,971 column C9 [1, 25]\n",
      "2021-11-26 07:00:33,058 column C7 [1, 3770]\n",
      "2021-11-26 07:00:33,146 column C10 [1, 4203]\n",
      "2021-11-26 07:00:33,163 column C11 [1, 3575]\n",
      "2021-11-26 07:00:33,207 column C13 [1, 10]\n",
      "2021-11-26 07:00:33,255 column C14 [1, 954]\n",
      "2021-11-26 07:00:33,320 column C16 [1, 42]\n",
      "2021-11-26 07:00:33,325 column C15 [1, 2210]\n",
      "2021-11-26 07:00:33,348 column C17 [1, 4]\n",
      "2021-11-26 07:00:33,350 column C12 [1, 6806]\n",
      "2021-11-26 07:00:33,380 column C18 [1, 338]\n",
      "2021-11-26 07:00:33,406 column C19 [1, 14]\n",
      "2021-11-26 07:00:33,627 column C20 [1, 3473]\n",
      "2021-11-26 07:00:33,655 column C21 [1, 4463]\n",
      "2021-11-26 07:00:33,726 column C23 [1, 3414]\n",
      "2021-11-26 07:00:33,741 column C25 [1, 38]\n",
      "2021-11-26 07:00:33,756 column C22 [1, 3745]\n",
      "2021-11-26 07:00:33,767 column C26 [1, 29]\n",
      "2021-11-26 07:00:33,796 column C24 [1, 4277]\n",
      "2021-11-26 07:00:33,803 _merge_columns_and_feature_cross\n",
      "2021-11-26 07:00:34,003 column C1 offset 0\n",
      "2021-11-26 07:00:34,005 column C2 offset 3559\n",
      "2021-11-26 07:00:34,006 column C3 offset 7389\n",
      "2021-11-26 07:00:34,008 column C4 offset 13375\n",
      "2021-11-26 07:00:34,010 column C5 offset 15218\n",
      "2021-11-26 07:00:34,012 column C6 offset 19323\n",
      "2021-11-26 07:00:34,014 column C7 offset 19326\n",
      "2021-11-26 07:00:34,015 column C8 offset 23096\n",
      "2021-11-26 07:00:34,017 column C9 offset 23906\n",
      "2021-11-26 07:00:34,019 column C10 offset 23931\n",
      "2021-11-26 07:00:34,021 column C11 offset 28134\n",
      "2021-11-26 07:00:34,022 column C12 offset 31709\n",
      "2021-11-26 07:00:34,024 column C13 offset 38515\n",
      "2021-11-26 07:00:34,026 column C14 offset 38525\n",
      "2021-11-26 07:00:34,028 column C15 offset 39479\n",
      "2021-11-26 07:00:34,030 column C16 offset 41689\n",
      "2021-11-26 07:00:34,031 column C17 offset 41731\n",
      "2021-11-26 07:00:34,033 column C18 offset 41735\n",
      "2021-11-26 07:00:34,035 column C19 offset 42073\n",
      "2021-11-26 07:00:34,037 column C20 offset 42087\n",
      "2021-11-26 07:00:34,038 column C21 offset 45560\n",
      "2021-11-26 07:00:34,040 column C22 offset 50023\n",
      "2021-11-26 07:00:34,042 column C23 offset 53768\n",
      "2021-11-26 07:00:34,044 column C24 offset 57182\n",
      "2021-11-26 07:00:34,046 column C25 offset 61459\n",
      "2021-11-26 07:00:34,047 column C26 offset 61497\n",
      "2021-11-26 07:00:34,136 column C1_C2 [1, 16580]\n",
      "2021-11-26 07:00:34,138 column C1_C2 offset 61526\n",
      "2021-11-26 07:00:34,348 column C3_C4 [1, 129558]\n",
      "2021-11-26 07:00:34,349 column C3_C4 offset 78106\n",
      "2021-11-26 07:00:34,349 _convert_to_string\n",
      "2021-11-26 07:00:34,352 _store_to_df\n",
      "2021-11-26 07:00:38,904 write to a CSV file\n",
      "2021-11-26 07:00:44,393 done!\n",
      "Splitting into 400000-sample training, 50000-sample val, and 50000-sample test datasets...\n",
      "slot_num for w&D is:27\n",
      "1\n",
      "criteo_data/train exist\n",
      "criteo_data/train/sparse_embedding0.data\n",
      "Opening criteo_data/file_list.txt\n",
      "0 keyset size is: 71182\n",
      "reopen file_list_tmp\n",
      "criteo_data/train/sparse_embedding1.data\n",
      "Opening criteo_data/file_list.txt\n",
      "1 keyset size is: 71456\n",
      "reopen file_list_tmp\n",
      "criteo_data/train/sparse_embedding2.data\n",
      "Opening criteo_data/file_list.txt\n",
      "2 keyset size is: 71132\n",
      "reopen file_list_tmp\n",
      "criteo_data/train/sparse_embedding3.data\n",
      "Opening criteo_data/file_list.txt\n",
      "3 keyset size is: 71350\n",
      "reopen file_list_tmp\n",
      "criteo_data/train/sparse_embedding4.data\n",
      "Opening criteo_data/file_list.txt\n",
      "4 keyset size is: 71304\n",
      "reopen file_list_tmp\n",
      "criteo_data/train/sparse_embedding5.data\n",
      "Opening criteo_data/file_list.txt\n",
      "5 keyset size is: 71132\n",
      "reopen file_list_tmp\n",
      "criteo_data/train/sparse_embedding6.data\n",
      "Opening criteo_data/file_list.txt\n",
      "6 keyset size is: 71213\n",
      "reopen file_list_tmp\n",
      "criteo_data/train/sparse_embedding7.data\n",
      "Opening criteo_data/file_list.txt\n",
      "7 keyset size is: 71104\n",
      "reopen file_list_tmp\n",
      "criteo_data/train/sparse_embedding8.data\n",
      "Opening criteo_data/file_list.txt\n",
      "8 keyset size is: 71509\n",
      "reopen file_list_tmp\n",
      "criteo_data/train/sparse_embedding9.data\n",
      "last keyset size is:62063\n",
      "Opening criteo_data/file_list.txt\n",
      "slot_num for w&D is:27\n",
      "1\n",
      "criteo_data/val exist\n",
      "criteo_data/val/sparse_embedding0.data\n",
      "Opening criteo_data/file_list_test.txt\n",
      "0 keyset size is: 71058\n",
      "reopen file_list_tmp\n",
      "criteo_data/val/sparse_embedding1.data\n",
      "last keyset size is:29978\n",
      "Opening criteo_data/file_list_test.txt\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "!bash preprocess.sh 0 criteo_data pandas 1 1 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff0159e",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTES**: \n",
    "- The first argument represents the dataset postfix.  For instance, if `day_1` is used, the postfix is `1`.\n",
    "- The second argument, `criteo_data`, is where the preprocessed data is stored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283a46e1",
   "metadata": {},
   "source": [
    "## Start the Kafka Broker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b1d865",
   "metadata": {},
   "source": [
    "**Please refer to the README to start the Kafka Broker properly.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36727836",
   "metadata": {},
   "source": [
    "## Wide&Deep Model Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "37cc95e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wdl_demo.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wdl_demo.py\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "solver = hugectr.CreateSolver(model_name = \"wdl\",\n",
    "                              max_eval_batches = 5000,\n",
    "                              batchsize_eval = 1024,\n",
    "                              batchsize = 1024,\n",
    "                              lr = 0.001,\n",
    "                              vvgpu = [[0]],\n",
    "                              i64_input_key = False,\n",
    "                              use_mixed_precision = False,\n",
    "                              repeat_dataset = False,\n",
    "                              use_cuda_graph = True,\n",
    "                              kafka_brockers = \"10.23.137.25:9093\")\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Norm,\n",
    "                          source = [\"criteo_data/file_list.\"+str(i)+\".txt\" for i in range(2)],\n",
    "                          keyset = [\"criteo_data/file_list.\"+str(i)+\".keyset\" for i in range(2)],\n",
    "                          eval_source = \"criteo_data/file_list.2.txt\",\n",
    "                          check_type = hugectr.Check_t.Sum)\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.Adam)\n",
    "hc_config = hugectr.CreateHMemCache(2, 0.5, 0)\n",
    "etc = hugectr.CreateETC(ps_types = [hugectr.TrainPSType_t.Staged, hugectr.TrainPSType_t.Cached],\\\n",
    "                        sparse_models = [\"./wdl_0_sparse_model\", \"./wdl_1_sparse_model\"],\\\n",
    "                        local_paths = [\"./\"], hmem_cache_configs = [hc_config])\n",
    "model = hugectr.Model(solver, reader, optimizer, etc)\n",
    "model.add(hugectr.Input(label_dim = 1, label_name = \"label\",\n",
    "                        dense_dim = 13, dense_name = \"dense\",\n",
    "                        data_reader_sparse_param_array = \n",
    "                        [hugectr.DataReaderSparseParam(\"wide_data\", 2, True, 1),\n",
    "                        hugectr.DataReaderSparseParam(\"deep_data\", 1, True, 26)]))\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash, \n",
    "                            workspace_size_per_gpu_in_mb = 23,\n",
    "                            embedding_vec_size = 1,\n",
    "                            combiner = \"sum\",\n",
    "                            sparse_embedding_name = \"sparse_embedding2\",\n",
    "                            bottom_name = \"wide_data\",\n",
    "                            optimizer = optimizer))\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash, \n",
    "                            workspace_size_per_gpu_in_mb = 358,\n",
    "                            embedding_vec_size = 16,\n",
    "                            combiner = \"sum\",\n",
    "                            sparse_embedding_name = \"sparse_embedding1\",\n",
    "                            bottom_name = \"deep_data\",\n",
    "                            optimizer = optimizer))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding1\"],\n",
    "                            top_names = [\"reshape1\"],\n",
    "                            leading_dim=416))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding2\"],\n",
    "                            top_names = [\"reshape2\"],\n",
    "                            leading_dim=1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Concat,\n",
    "                            bottom_names = [\"reshape1\", \"dense\"], top_names = [\"concat1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"concat1\"],\n",
    "                            top_names = [\"fc1\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc1\"],\n",
    "                            top_names = [\"relu1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu1\"],\n",
    "                            top_names = [\"dropout1\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dropout1\"],\n",
    "                            top_names = [\"fc2\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc2\"],\n",
    "                            top_names = [\"relu2\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu2\"],\n",
    "                            top_names = [\"dropout2\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dropout2\"],\n",
    "                            top_names = [\"fc3\"],\n",
    "                            num_output=1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Add,\n",
    "                            bottom_names = [\"fc3\", \"reshape2\"],\n",
    "                            top_names = [\"add1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "                            bottom_names = [\"add1\", \"label\"],\n",
    "                            top_names = [\"loss\"]))\n",
    "model.compile()\n",
    "model.summary()\n",
    "model.graph_to_json(graph_config_file = \"wdl.json\")\n",
    "#model.save_params_to_files(\"wdl\")\n",
    "model.fit(num_epochs = 1, display = 500, eval_interval = 1000)\n",
    "\n",
    "# Get the updated embedding features in model.fit()\n",
    "# updated_model = model.get_incremental_model()\n",
    "# User defined operations to the updated_model\n",
    "# ...+\n",
    "model.set_source(source = [\"criteo_data/file_list.3.txt\", \"criteo_data/file_list.4.txt\"], \\\n",
    "                 keyset = [\"criteo_data/file_list.3.keyset\", \"criteo_data/file_list.4.keyset\"], \\\n",
    "                 eval_source = \"criteo_data/file_list.5.txt\")\n",
    "\n",
    "model.fit(num_epochs = 1, display = 500, eval_interval = 1000)\n",
    "# Get the updated embedding features in model.fit()\n",
    "# updated_model = model.get_incremental_model()\n",
    "# User defined operations to the updated_model\n",
    "# ...\n",
    "#model.dump_incremental_model_2kafka()\n",
    "model.save_params_to_files(\"wdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "290f30da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '*model': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm -r *model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c29732c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HUGECTR][03:07:03][INFO][RANK0]: Empty embedding, trained table will be stored in ./wdl_0_sparse_model\n",
      "[HUGECTR][03:07:03][INFO][RANK0]: Empty embedding, trained table will be stored in ./wdl_1_sparse_model\n",
      "HugeCTR Version: 3.2\n",
      "====================================================Model Init=====================================================\n",
      "[HUGECTR][03:07:03][INFO][RANK0]: Initialize model: wdl\n",
      "[HUGECTR][03:07:03][INFO][RANK0]: Global seed is 2195796211\n",
      "[HUGECTR][03:07:03][INFO][RANK0]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 0\n",
      "\n",
      "[HUGECTR][03:07:04][WARNING][RANK0]: Peer-to-peer access cannot be fully enabled.\n",
      "[HUGECTR][03:07:04][INFO][RANK0]: Start all2all warmup\n",
      "[HUGECTR][03:07:04][INFO][RANK0]: End all2all warmup\n",
      "[HUGECTR][03:07:04][INFO][RANK0]: Using All-reduce algorithm: NCCL\n",
      "[HUGECTR][03:07:04][INFO][RANK0]: Device 0: Tesla V100-SXM2-32GB\n",
      "[HUGECTR][03:07:04][DEBUG][RANK0]: Creating Kafka lifetime service.\n",
      "[HUGECTR][03:07:04][INFO][RANK0]: num of DataReader workers: 12\n",
      "[HUGECTR][03:07:04][INFO][RANK0]: max_vocabulary_size_per_gpu_=6029312\n",
      "[HUGECTR][03:07:04][INFO][RANK0]: max_vocabulary_size_per_gpu_=5865472\n",
      "[HUGECTR][03:07:04][INFO][RANK0]: Graph analysis to resolve tensor dependency\n",
      "===================================================Model Compile===================================================\n",
      "[HUGECTR][03:07:08][INFO][RANK0]: gpu0 start to init embedding\n",
      "[HUGECTR][03:07:08][INFO][RANK0]: gpu0 init embedding done\n",
      "[HUGECTR][03:07:08][INFO][RANK0]: gpu0 start to init embedding\n",
      "[HUGECTR][03:07:08][INFO][RANK0]: gpu0 init embedding done\n",
      "[HUGECTR][03:07:08][INFO][RANK0]: Enable HMEM-Based Parameter Server\n",
      "[HUGECTR][03:07:08][INFO][RANK0]: ./wdl_0_sparse_model not exist, create and train from scratch\n",
      "[HUGECTR][03:07:08][INFO][RANK0]: Enable HMemCache-Based Parameter Server\n",
      "[HUGECTR][03:07:08][INFO][RANK0]: ./wdl_1_sparse_model/key doesn't exist, created\n",
      "[HUGECTR][03:07:08][INFO][RANK0]: ./wdl_1_sparse_model/emb_vector doesn't exist, created\n",
      "[HUGECTR][03:07:08][INFO][RANK0]: ./wdl_1_sparse_model/Adam.m doesn't exist, created\n",
      "[HUGECTR][03:07:08][INFO][RANK0]: ./wdl_1_sparse_model/Adam.v doesn't exist, created\n",
      "[HUGECTR][03:07:09][INFO][RANK0]: Starting AUC NCCL warm-up\n",
      "[HUGECTR][03:07:09][INFO][RANK0]: Warm-up done\n",
      "===================================================Model Summary===================================================\n",
      "label                                   Dense                         Sparse                        \n",
      "label                                   dense                          wide_data,deep_data           \n",
      "(None, 1)                               (None, 13)                              \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "DistributedSlotSparseEmbeddingHash      wide_data                     sparse_embedding2             (None, 1, 1)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "DistributedSlotSparseEmbeddingHash      deep_data                     sparse_embedding1             (None, 26, 16)                \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Reshape                                 sparse_embedding1             reshape1                      (None, 416)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Reshape                                 sparse_embedding2             reshape2                      (None, 1)                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Concat                                  reshape1                      concat1                       (None, 429)                   \n",
      "                                        dense                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            concat1                       fc1                           (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc1                           relu1                         (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Dropout                                 relu1                         dropout1                      (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            dropout1                      fc2                           (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc2                           relu2                         (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Dropout                                 relu2                         dropout2                      (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            dropout2                      fc3                           (None, 1)                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Add                                     fc3                           add1                          (None, 1)                     \n",
      "                                        reshape2                                                                                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "BinaryCrossEntropyLoss                  add1                          loss                                                        \n",
      "                                        label                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "[HUGECTR][03:07:09][INFO][RANK0]: Save the model graph to wdl.json successfully\n",
      "=====================================================Model Fit=====================================================\n",
      "[HUGECTR][03:07:09][INFO][RANK0]: Use embedding training cache mode with number of training sources: 2, number of epochs: 1\n",
      "[HUGECTR][03:07:09][INFO][RANK0]: Training batchsize: 1024, evaluation batchsize: 1024\n",
      "[HUGECTR][03:07:09][INFO][RANK0]: Evaluation interval: 1000, snapshot interval: 10000\n",
      "[HUGECTR][03:07:09][INFO][RANK0]: Sparse embedding trainable: True, dense network trainable: True\n",
      "[HUGECTR][03:07:09][INFO][RANK0]: Use mixed precision: False, scaler: 1.000000, use cuda graph: True\n",
      "[HUGECTR][03:07:09][INFO][RANK0]: lr: 0.001000, warmup_steps: 1, decay_start: 0, decay_steps: 1, decay_power: 2.000000, end_lr: 0.000000\n",
      "[HUGECTR][03:07:09][INFO][RANK0]: Evaluation source file: criteo_data/file_list.2.txt\n",
      "[HUGECTR][03:07:09][INFO][RANK0]: --------------------Epoch 0, source file: criteo_data/file_list.0.txt--------------------\n",
      "[HUGECTR][03:07:09][INFO][RANK0]: Preparing embedding table for next pass\n",
      "[HUGECTR][03:07:09][INFO][RANK0]: HMEM-Cache PS: Hit rate [load]: 0 %\n",
      "[HUGECTR][03:07:10][INFO][RANK0]: --------------------Epoch 0, source file: criteo_data/file_list.1.txt--------------------\n",
      "[HUGECTR][03:07:10][INFO][RANK0]: Preparing embedding table for next pass\n",
      "[HUGECTR][03:07:10][INFO][RANK0]: HMEM-Cache PS: Hit rate [dump]: 0 %\n",
      "[HUGECTR][03:07:10][INFO][RANK0]: HMEM-Cache PS: Hit rate [load]: 0 %\n",
      "=====================================================Model Fit=====================================================\n",
      "[HUGECTR][03:07:11][INFO][RANK0]: Use embedding training cache mode with number of training sources: 2, number of epochs: 1\n",
      "[HUGECTR][03:07:11][INFO][RANK0]: Training batchsize: 1024, evaluation batchsize: 1024\n",
      "[HUGECTR][03:07:11][INFO][RANK0]: Evaluation interval: 1000, snapshot interval: 10000\n",
      "[HUGECTR][03:07:11][INFO][RANK0]: Sparse embedding trainable: True, dense network trainable: True\n",
      "[HUGECTR][03:07:11][INFO][RANK0]: Use mixed precision: False, scaler: 1.000000, use cuda graph: True\n",
      "[HUGECTR][03:07:11][INFO][RANK0]: lr: 0.001000, warmup_steps: 1, decay_start: 0, decay_steps: 1, decay_power: 2.000000, end_lr: 0.000000\n",
      "[HUGECTR][03:07:11][INFO][RANK0]: Evaluation source file: criteo_data/file_list.5.txt\n",
      "[HUGECTR][03:07:11][INFO][RANK0]: --------------------Epoch 0, source file: criteo_data/file_list.3.txt--------------------\n",
      "[HUGECTR][03:07:11][INFO][RANK0]: Preparing embedding table for next pass\n",
      "[HUGECTR][03:07:11][INFO][RANK0]: HMEM-Cache PS: Hit rate [dump]: 76.75 %\n",
      "[HUGECTR][03:07:11][INFO][RANK0]: HMEM-Cache PS: Hit rate [load]: 69.83 %\n",
      "[HUGECTR][03:07:11][INFO][RANK0]: --------------------Epoch 0, source file: criteo_data/file_list.4.txt--------------------\n",
      "[HUGECTR][03:07:11][INFO][RANK0]: Preparing embedding table for next pass\n",
      "[HUGECTR][03:07:12][INFO][RANK0]: HMEM-Cache PS: Hit rate [dump]: 63.37 %\n",
      "[HUGECTR][03:07:12][INFO][RANK0]: HMEM-Cache PS: Hit rate [load]: 66.58 %\n",
      "[HUGECTR][03:07:13][INFO][RANK0]: HMEM-Cache PS: Hit rate [dump]: 63.77 %\n",
      "[HUGECTR][03:07:13][INFO][RANK0]: Updating sparse model in SSD [DONE]\n",
      "[HUGECTR][03:07:13][INFO][RANK0]: Sync blocks from HMEM-Cache to SSD\n",
      " \u001b[38;2;89;255;89m ████████████████████████████████████████▏ \u001b[1m\u001b[31m100.0% \u001b[34m[   2/   2 | 58.5 Hz | 0s<0s]  \u001b[0m\u001b[32m\u001b[0mm\n",
      "[HUGECTR][03:07:13][INFO][RANK0]: Dumping dense weights to file, successful\n",
      "[HUGECTR][03:07:13][INFO][RANK0]: Dumping dense optimizer states to file, successful\n",
      "[HUGECTR][03:07:13][INFO][RANK0]: Dumping untrainable weights to file, successful\n",
      "[HUGECTR][03:07:14][DEBUG][RANK0]: Destroying Kafka lifetime service.\n"
     ]
    }
   ],
   "source": [
    "!python wdl_demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc1a0e1",
   "metadata": {},
   "source": [
    "## WDL Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2135144",
   "metadata": {},
   "source": [
    "### Inference using HugeCTR python API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b77269a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wdl_predict.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 'wdl_predict.py'\n",
    "from hugectr.inference import InferenceParams, CreateInferenceSession\n",
    "import hugectr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from mpi4py import MPI\n",
    "def wdl_inference(model_name='wdl', network_file='wdl.json', dense_file='wdl_dense_0.model', \\\n",
    "                  embedding_file_list=['wdl_0_sparse_model', 'wdl_1_sparse_model'], data_file='infer_data.csv',\\\n",
    "                  enable_cache=False, rocksdb_path=\"\"):\n",
    "    CATEGORICAL_COLUMNS=[\"C1_C2\",\"C3_C4\"] + [\"C\" + str(x) for x in range(1, 27)]\n",
    "    CONTINUOUS_COLUMNS=[\"I\" + str(x) for x in range(1, 14)]\n",
    "    LABEL_COLUMNS = ['label']\n",
    "    test_df=pd.read_csv(data_file,sep=',')[:1]\n",
    "    config_file = network_file\n",
    "    row_ptrs = list(range(0, 3, 2)) + list(range(0, 27))\n",
    "    #row_ptrs = [0, 2] + list(range(0,27))\n",
    "    print(row_ptrs)\n",
    "    #dense_features =  list(test_df[CONTINUOUS_COLUMNS].values.flatten())\n",
    "    print(dense_features)\n",
    "    dense_features = list(np.zeros(13, dtype=\"float32\"))\n",
    "    test_df[CATEGORICAL_COLUMNS].astype(np.int64)\n",
    "    #embedding_columns = list((test_df[CATEGORICAL_COLUMNS]).values.flatten())\n",
    "    \n",
    "    embedding_columns = list(np.array([74153]*2 + [41979]*26, dtype='int64'))\n",
    "    print(embedding_columns)\n",
    "\n",
    "    # create parameter server, embedding cache and inference session\n",
    "    inference_params = InferenceParams(model_name = model_name,\n",
    "                                max_batchsize = 64,\n",
    "                                hit_rate_threshold = 0.5,\n",
    "                                dense_model_file = dense_file,\n",
    "                                sparse_model_files = embedding_file_list,\n",
    "                                device_id = 0,\n",
    "                                use_gpu_embedding_cache = enable_cache,\n",
    "                                cache_size_percentage = 0.9,\n",
    "                                i64_input_key = True,\n",
    "                                use_mixed_precision = False)\n",
    "    inference_session = CreateInferenceSession(config_file, inference_params)\n",
    "    output = inference_session.predict(dense_features, embedding_columns, row_ptrs)\n",
    "    print(\"WDL multi-embedding table inference result is {}\".format(output))\n",
    "\n",
    "wdl_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7efb3716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "[0.00592885, 0.0294221, 0.0103093, 0.0, 0.0, 0.00970874, 0.0263158, 0.3085, 0.0196078, 0.142857, 0.0, 7.31368e-05, 0.00549451]\n",
      "[74153, 79734, 2528, 5065, 7423, 15128, 15520, 19324, 21804, 23488, 23909, 26212, 31645, 32203, 38524, 39022, 41086, 41711, 41734, 41979, 42083, 42275, 45672, 51072, 55468, 57567, 61479, 61516]\n",
      "[HUGECTR][11:34:34][INFO][RANK0]: default_emb_vec_value is not specified using default: 0.000000\n",
      "[HUGECTR][11:34:34][INFO][RANK0]: default_emb_vec_value is not specified using default: 0.000000\n",
      "[HUGECTR][11:34:34][INFO][RANK0]: Creating ParallelHashMap CPU database backend...\n",
      "[HUGECTR][11:34:34][INFO][RANK0]: Created parallel (16 partitions) blank database backend in local memory!\n",
      "[HUGECTR][11:34:34][INFO][RANK0]: ParallelHashMap backend. Table: hctr_et.wdl.sparse_embedding2. Inserted 71121 / 71121 pairs.\n",
      "[HUGECTR][11:34:34][INFO][RANK0]: Table: hctr_et.wdl.sparse_embedding2; cached 71121 / 71121 embeddings in CPU memory database!\n",
      "[HUGECTR][11:34:34][INFO][RANK0]: ParallelHashMap backend. Table: hctr_et.wdl.sparse_embedding1. Inserted 60117 / 60117 pairs.\n",
      "[HUGECTR][11:34:34][INFO][RANK0]: Table: hctr_et.wdl.sparse_embedding1; cached 60117 / 60117 embeddings in CPU memory database!\n",
      "[HUGECTR][11:34:34][DEBUG][RANK0]: Real-time subscribers created!\n",
      "[HUGECTR][11:34:34][INFO][RANK0]: Create embedding cache in device 0.\n",
      "[HUGECTR][11:34:34][INFO][RANK0]: Use GPU embedding cache: False, cache size percentage: 0.900000\n",
      "[HUGECTR][11:34:34][INFO][RANK0]: Configured cache hit rate threshold: 0.500000\n",
      "[HUGECTR][11:34:35][INFO][RANK0]: Global seed is 558169123\n",
      "[HUGECTR][11:34:35][INFO][RANK0]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 0\n",
      "\n",
      "[HUGECTR][11:34:36][WARNING][RANK0]: Peer-to-peer access cannot be fully enabled.\n",
      "[HUGECTR][11:34:36][INFO][RANK0]: Start all2all warmup\n",
      "[HUGECTR][11:34:36][INFO][RANK0]: End all2all warmup\n",
      "[HUGECTR][11:34:36][INFO][RANK0]: Model name: wdl\n",
      "[HUGECTR][11:34:36][INFO][RANK0]: Use mixed precision: False\n",
      "[HUGECTR][11:34:36][INFO][RANK0]: Use cuda graph: True\n",
      "[HUGECTR][11:34:36][INFO][RANK0]: Max batchsize: 64\n",
      "[HUGECTR][11:34:36][INFO][RANK0]: Use I64 input key: True\n",
      "[HUGECTR][11:34:36][INFO][RANK0]: start create embedding for inference\n",
      "[HUGECTR][11:34:36][INFO][RANK0]: sparse_input name wide_data\n",
      "[HUGECTR][11:34:36][INFO][RANK0]: sparse_input name deep_data\n",
      "[HUGECTR][11:34:36][INFO][RANK0]: create embedding for inference success\n",
      "[HUGECTR][11:34:36][INFO][RANK0]: Inference stage skip BinaryCrossEntropyLoss layer, replaced by Sigmoid layer\n",
      "[HUGECTR][11:34:37][INFO][RANK0]: Looking up 2 embeddings (each with 1 values)...\n",
      "74153\n",
      "\n",
      "79734\n",
      "\n",
      "[HUGECTR][11:34:37][INFO][RANK0]: ParallelHashMap backend. Table: hctr_et.wdl.sparse_embedding2. Fetched 2 / 2 values.\n",
      "[HUGECTR][11:34:37][INFO][RANK0]: ParallelHashMap: 2 hits, 0 missing!\n",
      "[HUGECTR][11:34:37][INFO][RANK0]: Parameter server lookup of 2 / 2 embeddings took 90 us.\n",
      "[HUGECTR][11:34:37][INFO][RANK0]: Looking up 26 embeddings (each with 16 values)...\n",
      "2528\n",
      "\n",
      "15520\n",
      "\n",
      "23488\n",
      "\n",
      "51072\n",
      "\n",
      "42083\n",
      "\n",
      "42275\n",
      "\n",
      "26212\n",
      "\n",
      "23909\n",
      "\n",
      "41734\n",
      "\n",
      "61479\n",
      "\n",
      "15128\n",
      "\n",
      "45672\n",
      "\n",
      "5065\n",
      "\n",
      "32203\n",
      "\n",
      "41979\n",
      "\n",
      "19324\n",
      "\n",
      "21804\n",
      "\n",
      "38524\n",
      "\n",
      "55468\n",
      "\n",
      "61516\n",
      "\n",
      "31645\n",
      "\n",
      "39022\n",
      "\n",
      "41086\n",
      "\n",
      "7423\n",
      "\n",
      "41711\n",
      "\n",
      "57567\n",
      "\n",
      "[HUGECTR][11:34:37][INFO][RANK0]: ParallelHashMap backend. Table: hctr_et.wdl.sparse_embedding1. Fetched 26 / 26 values.\n",
      "[HUGECTR][11:34:37][INFO][RANK0]: ParallelHashMap: 26 hits, 0 missing!\n",
      "[HUGECTR][11:34:37][INFO][RANK0]: Parameter server lookup of 26 / 26 embeddings took 257 us.\n",
      "WDL multi-embedding table inference result is [0.030389342457056046]\n"
     ]
    }
   ],
   "source": [
    "!python wdl_predict.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547ff7a7",
   "metadata": {},
   "source": [
    "### Inference using Triton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6f8ab3",
   "metadata": {},
   "source": [
    "**Please make sure you have started Redis Cluster according to the README.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64a1a559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: tritonclient[all] in /usr/local/lib/python3.8/dist-packages (2.16.0)\n",
      "Requirement already satisfied: python-rapidjson>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from tritonclient[all]) (1.5)\n",
      "Requirement already satisfied: numpy>=1.19.1 in /usr/local/lib/python3.8/dist-packages (from tritonclient[all]) (1.21.2)\n",
      "Requirement already satisfied: protobuf>=3.5.0 in /usr/local/lib/python3.8/dist-packages (from tritonclient[all]) (3.19.1)\n",
      "Requirement already satisfied: grpcio>=1.31.0 in /usr/local/lib/python3.8/dist-packages (from tritonclient[all]) (1.42.0)\n",
      "Requirement already satisfied: geventhttpclient>=1.4.4 in /usr/local/lib/python3.8/dist-packages (from tritonclient[all]) (1.5.3)\n",
      "Requirement already satisfied: brotli in /usr/local/lib/python3.8/dist-packages (from geventhttpclient>=1.4.4->tritonclient[all]) (1.0.9)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from geventhttpclient>=1.4.4->tritonclient[all]) (1.14.0)\n",
      "Requirement already satisfied: gevent>=0.13 in /usr/local/lib/python3.8/dist-packages (from geventhttpclient>=1.4.4->tritonclient[all]) (21.8.0)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from geventhttpclient>=1.4.4->tritonclient[all]) (2019.11.28)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from gevent>=0.13->geventhttpclient>=1.4.4->tritonclient[all]) (58.1.0)\n",
      "Requirement already satisfied: zope.interface in /usr/local/lib/python3.8/dist-packages (from gevent>=0.13->geventhttpclient>=1.4.4->tritonclient[all]) (5.4.0)\n",
      "Requirement already satisfied: greenlet<2.0,>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from gevent>=0.13->geventhttpclient>=1.4.4->tritonclient[all]) (1.1.2)\n",
      "Requirement already satisfied: zope.event in /usr/local/lib/python3.8/dist-packages (from gevent>=0.13->geventhttpclient>=1.4.4->tritonclient[all]) (4.5.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tritonclient[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13d035ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "import re\n",
    "import shutil\n",
    "import glob\n",
    "import warnings\n",
    "\n",
    "BASE_DIR = \"/wdl_infer\"\n",
    "model_folder  = os.path.join(BASE_DIR, \"model\")\n",
    "wdl_model_repo= os.path.join(model_folder, \"wdl\")\n",
    "wdl_version =os.path.join(wdl_model_repo, \"1\")\n",
    "\n",
    "if os.path.isdir(model_folder):\n",
    "    shutil.rmtree(model_folder)\n",
    "os.makedirs(model_folder)\n",
    "\n",
    "if os.path.isdir(wdl_model_repo):\n",
    "    shutil.rmtree(wdl_model_repo)\n",
    "os.makedirs(wdl_model_repo)\n",
    "\n",
    "if os.path.isdir(wdl_version):\n",
    "    shutil.rmtree(wdl_version)\n",
    "os.makedirs(wdl_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a9ba494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 5832\n",
      "-rw-r--r-- 1 root root    3628 Nov 29 07:35 wdl.json\n",
      "drwxr-xr-x 2 root root      35 Nov 29 07:35 wdl_0_sparse_model\n",
      "drwxr-xr-x 2 root root      63 Nov 29 07:35 wdl_1_sparse_model\n",
      "-rw-r--r-- 1 root root 5963780 Nov 29 07:35 wdl_dense_0.model\n"
     ]
    }
   ],
   "source": [
    "!cp -r wdl_0_sparse_model $wdl_version/\n",
    "!cp -r wdl_1_sparse_model $wdl_version/\n",
    "!cp  wdl_dense_0.model $wdl_version/\n",
    "!cp wdl.json $wdl_version/\n",
    "!ls -l $wdl_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9863609b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /wdl_infer/model/wdl/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%writefile $wdl_model_repo/config.pbtxt\n",
    "name: \"wdl\"\n",
    "backend: \"hugectr\"\n",
    "max_batch_size:64,\n",
    "input [\n",
    "   {\n",
    "    name: \"DES\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ -1 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"CATCOLUMN\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [ -1 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"ROWINDEX\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ -1 ]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"OUTPUT0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ -1 ]\n",
    "  }\n",
    "]\n",
    "instance_group [\n",
    "  {\n",
    "    count: 1\n",
    "    kind : KIND_GPU\n",
    "    gpus:[0]\n",
    "  }\n",
    "]\n",
    "\n",
    "parameters [\n",
    "  {\n",
    "  key: \"config\"\n",
    "  value: { string_value: \"/wdl_infer/model/wdl/1/wdl.json\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"gpucache\"\n",
    "  value: { string_value: \"true\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"hit_rate_threshold\"\n",
    "  value: { string_value: \"0.8\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"gpucacheper\"\n",
    "  value: { string_value: \"0.5\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"label_dim\"\n",
    "  value: { string_value: \"1\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"slots\"\n",
    "  value: { string_value: \"27\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"cat_feature_num\"\n",
    "  value: { string_value: \"28\" }\n",
    "  },\n",
    " {\n",
    "  key: \"des_feature_num\"\n",
    "  value: { string_value: \"13\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"max_nnz\"\n",
    "  value: { string_value: \"2\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"embedding_vector_size\"\n",
    "  value: { string_value: \"16\" }\n",
    "  },\n",
    "  {\n",
    "  key: \"embeddingkey_long_type\"\n",
    "  value: { string_value: \"true\" }\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "321b7426",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /wdl_infer/rocksdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85986b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /wdl_infer/model/ps.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile /wdl_infer/model/ps.json\n",
    "{\n",
    "    \"supportlonglong\":\"true\",\n",
    "    \"db_type\":\"hierarchy\",\n",
    "    \"redis_ip\":\"127.0.0.1:7000,127.0.0.1:7001,127.0.0.1:7002\",\n",
    "    \"rocksdb_path\":\"/wdl_infer/rocksdb\",\n",
    "    \"cache_size_percentage_redis\":\"0.1\",\n",
    "    \"distributed_db_update_source\":\"kafka\",\n",
    "    \"persistent_db_update_source\":\"kafka\",\n",
    "    \"kafka_brokers\":\"10.23.137.25:9093\",\n",
    "    \"models\":[\n",
    "        {\n",
    "            \"model\":\"wdl\",\n",
    "            \"sparse_files\":[\"/wdl_infer/model/wdl/1/wdl_0_sparse_model\", \"/wdl_infer/model/wdl/1/wdl_1_sparse_model\"],\n",
    "            \"dense_file\":\"/wdl_infer/model/wdl/1/wdl_dense_0.model\",\n",
    "            \"network_file\":\"/wdl_infer/model/wdl/1/wdl.json\",\n",
    "            \"num_of_worker_buffer_in_pool\": \"1\",\n",
    "            \"num_of_refresher_buffer_in_pool\": \"1\",\n",
    "            \"cache_refresh_percentage_per_iteration\": \"0.2\",\n",
    "            \"deployed_device_list\":[\"0\"],\n",
    "            \"max_batch_size\":\"1024\",\n",
    "            \"default_value_for_each_table\":[\"0.0\",\"0.0\"],\n",
    "            \"hit_rate_threshold\":\"0.9\",\n",
    "            \"gpucacheper\":\"0.5\",\n",
    "            \"gpucache\":\"true\"\n",
    "        }\n",
    "    ]  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6337e440",
   "metadata": {},
   "source": [
    "**Start the Triton server in a new terminal using the following command:**\n",
    "```\n",
    "tritonserver --model-repository=/wdl_infer/model/ --load-model=wdl --model-control-mode=explicit --backend-directory=/usr/local/hugectr/backends --backend-config=hugectr,ps=/wdl_infer/model/ps.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8680443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 74153  79734   2528   5065   7423  15128  15520  19324  21804  23488\n",
      "   23909  26212  31645  32203  38524  39022  41086  41711  41734  41979\n",
      "   42083  42275  45672  51072  55468  57567  61479  61516  69270 166915\n",
      "    1766   4856  11569  15128  15520  19326  22546  23570  23908  26098\n",
      "   29951  35136  38523  39085  41533  41697  41732  41979  42083  43821\n",
      "   47798  51918  54391  57477  61461  61516  70519 203418   1766   6741\n",
      "   13180  13567  18371  19326  19642  23431  23909  27825  30714  31813\n",
      "   38521  38835  40106  41702  41732  41979  42083  43821  47786  51918\n",
      "   54396  58410  61462  61512  66882 149028   1358   3932  10742  13676\n",
      "   16676  19324  21949  23859  23909  24749  28295  35142  38521  39242\n",
      "   40685  41704  41735  41979  42083  44759  45948  50596  56631  59988\n",
      "   61497  61521  73718 111611   2444   6660   8910  15096  15520  19324\n",
      "   21143  23730  23909  25617  28403  31839  38521  39242  41666  41704\n",
      "   41735  41979  42085  42380  48203  50340  56578  60317  61462  61520]]\n",
      "{'id': '1', 'model_name': 'wdl', 'model_version': '1', 'parameters': {'NumSample': 5, 'DeviceID': 0}, 'outputs': [{'name': 'OUTPUT0', 'datatype': 'FP32', 'shape': [5], 'parameters': {'binary_data_size': 20}}]}\n",
      "Prediction Result:\n",
      "[0.11633033 0.0779141  0.10812995 0.08358601 0.06648827]\n"
     ]
    }
   ],
   "source": [
    "from tritonclient.utils import *\n",
    "import tritonclient.http  as httpclient\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "model_name = 'wdl'\n",
    "CATEGORICAL_COLUMNS=[\"C1_C2\",\"C3_C4\"] + [\"C\" + str(x) for x in range(1, 27)]\n",
    "CONTINUOUS_COLUMNS=[\"I\" + str(x) for x in range(1, 14)]\n",
    "LABEL_COLUMNS = ['label']\n",
    "test_df=pd.read_csv(\"infer_data.csv\",sep=',')\n",
    "\n",
    "with httpclient.InferenceServerClient(\"localhost:8000\") as client:\n",
    "    dense_features = np.array([list(test_df[CONTINUOUS_COLUMNS].values.flatten())],dtype='float32')\n",
    "    embedding_columns = np.array([list((test_df[CATEGORICAL_COLUMNS]).values.flatten())],dtype='int64')\n",
    "    print(embedding_columns)\n",
    "    row_ptrs = np.array([list(range(0,11,2)) + list(range(0,131))], dtype='int32')\n",
    "    \n",
    "    inputs = [\n",
    "        httpclient.InferInput(\"DES\", dense_features.shape,\n",
    "                              np_to_triton_dtype(dense_features.dtype)),\n",
    "        httpclient.InferInput(\"CATCOLUMN\", embedding_columns.shape,\n",
    "                              np_to_triton_dtype(embedding_columns.dtype)),\n",
    "        httpclient.InferInput(\"ROWINDEX\", row_ptrs.shape,\n",
    "                              np_to_triton_dtype(row_ptrs.dtype)),\n",
    "\n",
    "    ]\n",
    "\n",
    "    inputs[0].set_data_from_numpy(dense_features)\n",
    "    inputs[1].set_data_from_numpy(embedding_columns)\n",
    "    inputs[2].set_data_from_numpy(row_ptrs)\n",
    "    outputs = [\n",
    "        httpclient.InferRequestedOutput(\"OUTPUT0\")\n",
    "    ]\n",
    "\n",
    "    response = client.infer(model_name,\n",
    "                            inputs,\n",
    "                            request_id=str(1),\n",
    "                            outputs=outputs)\n",
    "\n",
    "    result = response.get_response()\n",
    "    print(result)\n",
    "    print(\"Prediction Result:\")\n",
    "    print(response.as_numpy(\"OUTPUT0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf6c28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.030389349907636642, 0.026247011497616768, 0.04919133335351944, 0.05634024366736412, 0.027102859690785408]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
