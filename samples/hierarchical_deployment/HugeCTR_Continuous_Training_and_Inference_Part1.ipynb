{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d232388",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# HugeCTR Continuous Training and Inference Demo (Part I)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da36993a",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In HugeCTR version 3.3, we finished the whole pipeline of parameter server, including \n",
    "1. The parameter dumping interface from training to kafka.\n",
    "2. Redis as a level2 cache.\n",
    "3. RocksDB as a persistence storage.\n",
    "4. Embedding cache update mechanism.\n",
    "\n",
    "\n",
    "The purpose of this notebook is to give you a brief idea of how parameter server works in terms of the flow of parameters. \n",
    "\n",
    "## Table of Contents\n",
    "-  [Data Preparation](#1)\n",
    "-  [Data Preprocessing using NVTabular](#2)\n",
    "-  [Kafka broker start](#3)\n",
    "-  [Wide&Deep Continuous Training Demo](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6ceab6",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335e1299",
   "metadata": {},
   "source": [
    "1. Firstly, we made a folder to store our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09548b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘criteo_script’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir criteo_data\n",
    "!mkdir criteo_script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae64efb",
   "metadata": {},
   "source": [
    "2. Download Criteo Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7ce3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://azuremlsampleexperiments.blob.core.windows.net/criteo/day_1.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741411a3",
   "metadata": {},
   "source": [
    "**NOTE**: Replace `1` with a value from [0, 23] to use a different day.\n",
    "\n",
    "During preprocessing, the amount of data, which is used to speed up the preprocessing, fill missing values, and remove the feature values that are considered rare, is further reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a72fd83",
   "metadata": {},
   "source": [
    "3. Thirdly, preprocess the data and split the data into 6 parts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "297f6bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocess.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocess.sh\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "if [[ $# -lt 3 ]]; then\n",
    "  echo \"Usage: preprocess.sh [DATASET_NO.] [DST_DATA_DIR] [SCRIPT_TYPE] [SCRIPT_TYPE_SPECIFIC_ARGS...]\"\n",
    "  exit 2\n",
    "fi\n",
    "\n",
    "DST_DATA_DIR=$2\n",
    "\n",
    "echo \"Warning: existing $DST_DATA_DIR is erased\"\n",
    "rm -rf $DST_DATA_DIR\n",
    "\n",
    "if [[ $3 == \"nvt\" ]]; then\n",
    "  if [[ $# -ne 6 ]]; then\n",
    "\t\techo \"Usage: preprocess.sh [DATASET_NO.] [DST_DATA_DIR] nvt [IS_PARQUET_FORMAT] [IS_CRITEO_MODE] [IS_FEATURE_CROSSED]\"\n",
    "    exit 2\n",
    "\tfi\n",
    "\techo \"Preprocessing script: NVTabular\"\n",
    "elif [[ $3 == \"perl\" ]]; then\n",
    "  if [[ $# -ne 4 ]]; then\n",
    "\t\techo \"Usage: preprocess.sh [DATASET_NO.] [DST_DATA_DIR] perl [NUM_SLOTS]\"\n",
    "    exit 2\n",
    "\tfi\n",
    "\techo \"Preprocessing script: Perl\"\n",
    "elif [[ $3 == \"pandas\" ]]; then\n",
    "  if [[ $# -lt 5 ]]; then\n",
    "\t\techo \"Usage: preprocess.sh [DATASET_NO.] [DST_DATA_DIR] pandas [IS_DENSE_NORMALIZED] [IS_FEATURE_CROSSED] (FILE_LIST_LENGTH)\"\n",
    "    exit 2\n",
    "\tfi\n",
    "\techo \"Preprocessing script: Pandas\"\n",
    "else\n",
    "\techo \"Error: $3 is an invalid script type. Pick one from {nvt, perl, pandas}.\"\n",
    "\texit 2\n",
    "fi\n",
    "\n",
    "SCRIPT_TYPE=$3\n",
    "\n",
    "echo \"Getting the first few examples from the uncompressed dataset...\"\n",
    "mkdir -p $DST_DATA_DIR/train                         && \\\n",
    "mkdir -p $DST_DATA_DIR/val                           && \\\n",
    "head -n 500000 day_$1 > $DST_DATA_DIR/day_$1_small\n",
    "if [ $? -ne 0 ]; then\n",
    "\techo \"Warning: fallback to find original compressed data day_$1.gz...\"\n",
    "\techo \"Decompressing day_$1.gz...\"\n",
    "\tgzip -d -c day_$1.gz > day_$1\n",
    "\tif [ $? -ne 0 ]; then\n",
    "\t\techo \"Error: failed to decompress the file.\"\n",
    "\t\texit 2\n",
    "\tfi\n",
    "\thead -n 500000 day_$1 > $DST_DATA_DIR/day_$1_small\n",
    "\tif [ $? -ne 0 ]; then\n",
    "\t\techo \"Error: day_$1 file\"\n",
    "\t\texit 2\n",
    "\tfi\n",
    "fi\n",
    "\n",
    "echo \"Counting the number of samples in day_$1 dataset...\"\n",
    "total_count=$(wc -l $DST_DATA_DIR/day_$1_small)\n",
    "total_count=(${total_count})\n",
    "echo \"The first $total_count examples will be used in day_$1 dataset.\"\n",
    "\n",
    "echo \"Shuffling dataset...\"\n",
    "shuf $DST_DATA_DIR/day_$1_small > $DST_DATA_DIR/day_$1_shuf\n",
    "\n",
    "train_count=$(( total_count * 8 / 10))\n",
    "valtest_count=$(( total_count - train_count ))\n",
    "val_count=$(( valtest_count * 5 / 10 ))\n",
    "test_count=$(( valtest_count - val_count  ))\n",
    "\n",
    "split_dataset()\n",
    "{\n",
    "\techo \"Splitting into $train_count-sample training, $val_count-sample val, and $test_count-sample test datasets...\"\n",
    "\thead -n $train_count $DST_DATA_DIR/$1 > $DST_DATA_DIR/train/train.txt          && \\\n",
    "\ttail -n $valtest_count $DST_DATA_DIR/$1 > $DST_DATA_DIR/val/valtest.txt        && \\\n",
    "\thead -n $val_count $DST_DATA_DIR/val/valtest.txt > $DST_DATA_DIR/val/val.txt   && \\\n",
    "\ttail -n $test_count $DST_DATA_DIR/val/valtest.txt > $DST_DATA_DIR/val/test.txt\n",
    "\n",
    "\tif [ $? -ne 0 ]; then\n",
    "\t\texit 2\n",
    "\tfi\n",
    "}\n",
    "\n",
    "echo \"Preprocessing...\"\n",
    "if [[ $SCRIPT_TYPE == \"nvt\" ]]; then\n",
    "\tIS_PARQUET_FORMAT=$4\n",
    "\tIS_CRITEO_MODE=$5\n",
    "\tFEATURE_CROSS_LIST_OPTION=\"\"\n",
    "\tif [[ ( $IS_CRITEO_MODE -eq 0 ) && ( $6 -eq 1 ) ]]; then\n",
    "\t\tFEATURE_CROSS_LIST_OPTION=\"--feature_cross_list C1_C2,C3_C4\"\n",
    "\t\techo $FEATURE_CROSS_LIST_OPTION\n",
    "\tfi\n",
    "  split_dataset day_$1_shuf\n",
    "  python3 criteo_script/preprocess_nvt.py \\\n",
    "\t\t--data_path $DST_DATA_DIR             \\\n",
    "\t\t--out_path $DST_DATA_DIR              \\\n",
    "\t\t--freq_limit 6                        \\\n",
    "\t\t--device_limit_frac 0.5               \\\n",
    "\t\t--device_pool_frac 0.5                \\\n",
    "\t\t--out_files_per_proc 8                \\\n",
    "\t\t--devices \"0\"                         \\\n",
    "\t\t--num_io_threads 2                    \\\n",
    "        --parquet_format=$IS_PARQUET_FORMAT   \\\n",
    "\t\t--criteo_mode=$IS_CRITEO_MODE         \\\n",
    "\t\t$FEATURE_CROSS_LIST_OPTION\n",
    "\n",
    "elif [[ $SCRIPT_TYPE == \"perl\" ]]; then\n",
    "\tNUM_SLOT=$4\n",
    "  split_dataset day_$1_shuf\n",
    "\tperl criteo_script_legacy/preprocess.pl $DST_DATA_DIR/train/train.txt $DST_DATA_DIR/val/val.txt $DST_DATA_DIR/val/test.txt                      && \\\n",
    "\tcriteo2hugectr_legacy $NUM_SLOT $DST_DATA_DIR/train/train.txt.out $DST_DATA_DIR/train/sparse_embedding $DST_DATA_DIR/file_list.txt && \\\n",
    "\tcriteo2hugectr_legacy $NUM_SLOT $DST_DATA_DIR/val/test.txt.out $DST_DATA_DIR/val/sparse_embedding $DST_DATA_DIR/file_list_test.txt\n",
    "\n",
    "elif [[ $SCRIPT_TYPE == \"pandas\" ]]; then\n",
    "\tpython3 criteo_script/preprocess.py                 \\\n",
    "\t\t--src_csv_path=$DST_DATA_DIR/day_$1_shuf          \\\n",
    "\t\t--dst_csv_path=$DST_DATA_DIR/day_$1_shuf.out      \\\n",
    "\t\t--normalize_dense=$4 --feature_cross=$5      &&   \\\n",
    "  split_dataset day_$1_shuf.out\n",
    "\tNUM_WIDE_KEYS=\"\"\n",
    "\tif [[ $5 -ne 0 ]]; then\n",
    "\t\tNUM_WIDE_KEYS=2\n",
    "\tfi\n",
    "\n",
    "  FILE_LIST_LENGTH=\"\"\n",
    "  if [[ $# -gt 5 ]]; then\n",
    "    FILE_LIST_LENGTH=$6\n",
    "\tfi\n",
    "\n",
    "\tcriteo2hugectr $DST_DATA_DIR/train/train.txt $DST_DATA_DIR/train/sparse_embedding $DST_DATA_DIR/file_list.txt $NUM_WIDE_KEYS $FILE_LIST_LENGTH && \\\n",
    "\tcriteo2hugectr $DST_DATA_DIR/val/test.txt $DST_DATA_DIR/val/sparse_embedding $DST_DATA_DIR/file_list_test.txt $NUM_WIDE_KEYS $FILE_LIST_LENGTH\n",
    "fi\n",
    "\n",
    "if [ $? -ne 0 ]; then\n",
    "\texit 2\n",
    "fi\n",
    "\n",
    "echo \"All done!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91412cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting criteo_script/preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile criteo_script/preprocess.py\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import tempfile\n",
    "\n",
    "from six.moves import urllib\n",
    "import urllib.request \n",
    "\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import logging\n",
    "import concurrent.futures as cf\n",
    "from traceback import print_exc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing as skp\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(message)s')\n",
    "logging.root.setLevel(logging.NOTSET)\n",
    "\n",
    "NUM_INTEGER_COLUMNS = 13\n",
    "NUM_CATEGORICAL_COLUMNS = 26\n",
    "NUM_TOTAL_COLUMNS = 1 + NUM_INTEGER_COLUMNS + NUM_CATEGORICAL_COLUMNS\n",
    "\n",
    "MAX_NUM_WORKERS = NUM_TOTAL_COLUMNS\n",
    "\n",
    "INT_NAN_VALUE = np.iinfo(np.int32).min\n",
    "CAT_NAN_VALUE = '80000000'\n",
    "\n",
    "def idx2key(idx):\n",
    "    if idx == 0:\n",
    "        return 'label'\n",
    "    return 'I' + str(idx) if idx <= NUM_INTEGER_COLUMNS else 'C' + str(idx - NUM_INTEGER_COLUMNS)\n",
    "\n",
    "def _fill_missing_features_and_split(chunk, series_list_dict):\n",
    "    for cid, col in enumerate(chunk.columns):\n",
    "        NAN_VALUE = INT_NAN_VALUE if cid <= NUM_INTEGER_COLUMNS else CAT_NAN_VALUE\n",
    "        result_series = chunk[col].fillna(NAN_VALUE)\n",
    "        series_list_dict[col].append(result_series)\n",
    "\n",
    "def _merge_and_transform_series(src_series_list, col, dense_cols,\n",
    "                                normalize_dense):\n",
    "    result_series = pd.concat(src_series_list)\n",
    "\n",
    "    if col != 'label':\n",
    "        unique_value_counts = result_series.value_counts()\n",
    "        unique_value_counts = unique_value_counts.loc[unique_value_counts >= 6]\n",
    "        unique_value_counts = set(unique_value_counts.index.values)\n",
    "        NAN_VALUE = INT_NAN_VALUE if col.startswith('I') else CAT_NAN_VALUE\n",
    "        result_series = result_series.apply(\n",
    "                lambda x: x if x in unique_value_counts else NAN_VALUE)\n",
    "\n",
    "    if col == 'label' or col in dense_cols:\n",
    "        result_series = result_series.astype(np.int64)\n",
    "        le = skp.LabelEncoder()\n",
    "        result_series = pd.DataFrame(le.fit_transform(result_series))\n",
    "        if col != 'label':\n",
    "            result_series = result_series + 1\n",
    "    else:\n",
    "        oe = skp.OrdinalEncoder(dtype=np.int64)\n",
    "        result_series = pd.DataFrame(oe.fit_transform(pd.DataFrame(result_series)))\n",
    "        result_series = result_series + 1\n",
    "\n",
    "\n",
    "    if normalize_dense != 0:\n",
    "        if col in dense_cols:\n",
    "            mms = skp.MinMaxScaler(feature_range=(0,1))\n",
    "            result_series = pd.DataFrame(mms.fit_transform(result_series))\n",
    "\n",
    "    result_series.columns = [col]\n",
    "\n",
    "    min_max = (np.int64(result_series[col].min()), np.int64(result_series[col].max()))\n",
    "    if col != 'label':\n",
    "        logging.info('column {} [{}, {}]'.format(col, str(min_max[0]),str(min_max[1])))\n",
    "\n",
    "    return [result_series, min_max]\n",
    "\n",
    "def _convert_to_string(series):\n",
    "    return series.astype(str)\n",
    "\n",
    "def _merge_columns_and_feature_cross(series_list, min_max, feature_pairs,\n",
    "                                     feature_cross):\n",
    "    name_to_series = dict()\n",
    "    for series in series_list:\n",
    "        name_to_series[series.columns[0]] = series.iloc[:,0]\n",
    "    df = pd.DataFrame(name_to_series)\n",
    "    cols = [idx2key(idx) for idx in range(0, NUM_TOTAL_COLUMNS)]\n",
    "    df = df.reindex(columns=cols)\n",
    "\n",
    "    offset = np.int64(0)\n",
    "    for col in cols:\n",
    "        if col != 'label' and col.startswith('I') == False:\n",
    "            df[col] += offset\n",
    "            logging.info('column {} offset {}'.format(col, str(offset)))\n",
    "            offset += min_max[col][1]\n",
    "\n",
    "    if feature_cross != 0:\n",
    "        for idx, pair in enumerate(feature_pairs):\n",
    "            col0 = pair[0]\n",
    "            col1 = pair[1]\n",
    "\n",
    "            col1_width = int(min_max[col1][1] - min_max[col1][0] + 1)\n",
    "\n",
    "            crossed_column_series = df[col0] * col1_width + df[col1]\n",
    "            oe = skp.OrdinalEncoder(dtype=np.int64)\n",
    "            crossed_column_series = pd.DataFrame(oe.fit_transform(pd.DataFrame(crossed_column_series)))\n",
    "            crossed_column_series = crossed_column_series + 1\n",
    "\n",
    "            crossed_column = col0 + '_' + col1\n",
    "            df.insert(NUM_INTEGER_COLUMNS + 1 + idx, crossed_column, crossed_column_series)\n",
    "            crossed_column_max_val = np.int64(df[crossed_column].max())\n",
    "            logging.info('column {} [{}, {}]'.format(\n",
    "                crossed_column,\n",
    "                str(df[crossed_column].min()),\n",
    "                str(crossed_column_max_val)))\n",
    "            df[crossed_column] += offset\n",
    "            logging.info('column {} offset {}'.format(crossed_column, str(offset)))\n",
    "            offset += crossed_column_max_val\n",
    "\n",
    "    return df\n",
    "\n",
    "def _wait_futures_and_reset(futures):\n",
    "    for future in futures:\n",
    "        result = future.result()\n",
    "        if result:\n",
    "            print(result)\n",
    "    futures = list()\n",
    "\n",
    "def _process_chunks(executor, chunks_to_process, op, *argv):\n",
    "    futures = list()\n",
    "    for chunk in chunks_to_process:\n",
    "        argv_list = list(argv)\n",
    "        argv_list.insert(0, chunk)\n",
    "        new_argv = tuple(argv_list)\n",
    "        future = executor.submit(op, *new_argv)\n",
    "        futures.append(future)\n",
    "    _wait_futures_and_reset(futures)\n",
    "\n",
    "def preprocess(src_txt_name, dst_txt_name, normalize_dense, feature_cross):\n",
    "    cols = [idx2key(idx) for idx in range(0, NUM_TOTAL_COLUMNS)]\n",
    "    series_list_dict = dict()\n",
    "\n",
    "    with cf.ThreadPoolExecutor(max_workers=MAX_NUM_WORKERS) as executor:\n",
    "        logging.info('read a CSV file')\n",
    "        reader = pd.read_csv(src_txt_name, sep='\\t',\n",
    "                             names=cols,\n",
    "                             chunksize=131072)\n",
    "\n",
    "        logging.info('_fill_missing_features_and_split')\n",
    "        for col in cols:\n",
    "            series_list_dict[col] = list()\n",
    "        _process_chunks(executor, reader, _fill_missing_features_and_split,\n",
    "                        series_list_dict)\n",
    "\n",
    "    with cf.ProcessPoolExecutor(max_workers=MAX_NUM_WORKERS) as executor:\n",
    "        logging.info('_merge_and_transform_series')\n",
    "        futures = list()\n",
    "        dense_cols = [idx2key(idx+1) for idx in range(NUM_INTEGER_COLUMNS)]\n",
    "        dst_series_list = list()\n",
    "        min_max = dict()\n",
    "        for col, src_series_list in series_list_dict.items():\n",
    "            future = executor.submit(_merge_and_transform_series,\n",
    "                                     src_series_list, col, dense_cols,\n",
    "                                     normalize_dense)\n",
    "            futures.append(future)\n",
    "\n",
    "        for future in futures:\n",
    "            col = None\n",
    "            for idx, ret in enumerate(future.result()):\n",
    "                try:\n",
    "                    if idx == 0:\n",
    "                        col = ret.columns[0]\n",
    "                        dst_series_list.append(ret)\n",
    "                    else:\n",
    "                        min_max[col] = ret\n",
    "                except:\n",
    "                    print_exc()\n",
    "        futures = list()\n",
    "\n",
    "        logging.info('_merge_columns_and_feature_cross')\n",
    "        feature_pairs = [('C1', 'C2'), ('C3', 'C4')]\n",
    "        df = _merge_columns_and_feature_cross(dst_series_list, min_max, feature_pairs,\n",
    "                                              feature_cross)\n",
    "\n",
    "        \n",
    "        logging.info('_convert_to_string')\n",
    "        futures = dict()\n",
    "        for col in cols:\n",
    "            future = executor.submit(_convert_to_string, df[col])\n",
    "            futures[col] = future\n",
    "        if feature_cross != 0:\n",
    "            for pair in feature_pairs:\n",
    "                col = pair[0] + '_' + pair[1]\n",
    "                future = executor.submit(_convert_to_string, df[col])\n",
    "                futures[col] = future\n",
    "\n",
    "        logging.info('_store_to_df')\n",
    "        for col, future in futures.items():\n",
    "            ret = future.result()\n",
    "            try:\n",
    "                df[col] = ret\n",
    "            except:\n",
    "                print_exc()\n",
    "        futures = dict()\n",
    "\n",
    "        logging.info('write to a CSV file')\n",
    "        df.to_csv(dst_txt_name, sep=' ', header=False, index=False)\n",
    "\n",
    "        logging.info('done!')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    arg_parser = argparse.ArgumentParser(description='Preprocssing Criteo Dataset')\n",
    "\n",
    "    arg_parser.add_argument('--src_csv_path', type=str, required=True)\n",
    "    arg_parser.add_argument('--dst_csv_path', type=str, required=True)\n",
    "    arg_parser.add_argument('--normalize_dense', type=int, default=1)\n",
    "    arg_parser.add_argument('--feature_cross', type=int, default=1)\n",
    "\n",
    "    args = arg_parser.parse_args()\n",
    "\n",
    "    src_csv_path = args.src_csv_path\n",
    "    dst_csv_path = args.dst_csv_path\n",
    "\n",
    "    normalize_dense = args.normalize_dense\n",
    "    feature_cross = args.feature_cross\n",
    "\n",
    "    if os.path.exists(src_csv_path) == False:\n",
    "        sys.exit('ERROR: the file \\'{}\\' doesn\\'t exist'.format(src_csv_path))\n",
    "\n",
    "    if os.path.exists(dst_csv_path) == True:\n",
    "        sys.exit('ERROR: the file \\'{}\\' exists'.format(dst_csv_path))\n",
    "\n",
    "    preprocess(src_csv_path, dst_csv_path, normalize_dense, feature_cross)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03af2f7",
   "metadata": {},
   "source": [
    "4. Run the preprocess script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f1a87e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash preprocess.sh 0 criteo_data pandas 1 1 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d6ddc1",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTES**: \n",
    "- The first argument represents the dataset postfix.  For instance, if `day_1` is used, the postfix is `1`.\n",
    "- The second argument, `criteo_data`, is where the preprocessed data is stored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19a4a6c",
   "metadata": {},
   "source": [
    "## Start the Kafka Broker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d5c1ce",
   "metadata": {},
   "source": [
    "**Please refer to the README to start the Kafka Broker properly.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bdcfa0",
   "metadata": {},
   "source": [
    "## Wide&Deep Model Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3dc1cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wdl_demo.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wdl_demo.py\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "solver = hugectr.CreateSolver(model_name = \"wdl\",\n",
    "                              max_eval_batches = 5000,\n",
    "                              batchsize_eval = 1024,\n",
    "                              batchsize = 1024,\n",
    "                              lr = 0.001,\n",
    "                              vvgpu = [[0]],\n",
    "                              i64_input_key = False,\n",
    "                              use_mixed_precision = False,\n",
    "                              repeat_dataset = False,\n",
    "                              use_cuda_graph = True,\n",
    "                              kafka_brockers = \"10.23.137.25:9093\")\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Norm,\n",
    "                          source = [\"criteo_data/file_list.\"+str(i)+\".txt\" for i in range(2)],\n",
    "                          keyset = [\"criteo_data/file_list.\"+str(i)+\".keyset\" for i in range(2)],\n",
    "                          eval_source = \"criteo_data/file_list.2.txt\",\n",
    "                          check_type = hugectr.Check_t.Sum)\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.Adam)\n",
    "hc_config = hugectr.CreateHMemCache(2, 0.5, 0)\n",
    "etc = hugectr.CreateETC(ps_types = [hugectr.TrainPSType_t.Staged, hugectr.TrainPSType_t.Cached],\\\n",
    "                        sparse_models = [\"./wdl_0_sparse_model\", \"./wdl_1_sparse_model\"],\\\n",
    "                        local_paths = [\"./\"], hmem_cache_configs = [hc_config])\n",
    "model = hugectr.Model(solver, reader, optimizer, etc)\n",
    "model.add(hugectr.Input(label_dim = 1, label_name = \"label\",\n",
    "                        dense_dim = 13, dense_name = \"dense\",\n",
    "                        data_reader_sparse_param_array = \n",
    "                        [hugectr.DataReaderSparseParam(\"wide_data\", 2, True, 1),\n",
    "                        hugectr.DataReaderSparseParam(\"deep_data\", 1, True, 26)]))\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash, \n",
    "                            workspace_size_per_gpu_in_mb = 23,\n",
    "                            embedding_vec_size = 1,\n",
    "                            combiner = \"sum\",\n",
    "                            sparse_embedding_name = \"sparse_embedding2\",\n",
    "                            bottom_name = \"wide_data\",\n",
    "                            optimizer = optimizer))\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash, \n",
    "                            workspace_size_per_gpu_in_mb = 358,\n",
    "                            embedding_vec_size = 16,\n",
    "                            combiner = \"sum\",\n",
    "                            sparse_embedding_name = \"sparse_embedding1\",\n",
    "                            bottom_name = \"deep_data\",\n",
    "                            optimizer = optimizer))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding1\"],\n",
    "                            top_names = [\"reshape1\"],\n",
    "                            leading_dim=416))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding2\"],\n",
    "                            top_names = [\"reshape2\"],\n",
    "                            leading_dim=1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Concat,\n",
    "                            bottom_names = [\"reshape1\", \"dense\"], top_names = [\"concat1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"concat1\"],\n",
    "                            top_names = [\"fc1\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc1\"],\n",
    "                            top_names = [\"relu1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu1\"],\n",
    "                            top_names = [\"dropout1\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dropout1\"],\n",
    "                            top_names = [\"fc2\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc2\"],\n",
    "                            top_names = [\"relu2\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu2\"],\n",
    "                            top_names = [\"dropout2\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dropout2\"],\n",
    "                            top_names = [\"fc3\"],\n",
    "                            num_output=1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Add,\n",
    "                            bottom_names = [\"fc3\", \"reshape2\"],\n",
    "                            top_names = [\"add1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "                            bottom_names = [\"add1\", \"label\"],\n",
    "                            top_names = [\"loss\"]))\n",
    "model.compile()\n",
    "model.summary()\n",
    "model.graph_to_json(graph_config_file = \"wdl.json\")\n",
    "#model.save_params_to_files(\"wdl\")\n",
    "model.fit(num_epochs = 1, display = 500, eval_interval = 1000)\n",
    "\n",
    "# Get the updated embedding features in model.fit()\n",
    "# updated_model = model.get_incremental_model()\n",
    "# User defined operations to the updated_model\n",
    "# ...+\n",
    "model.set_source(source = [\"criteo_data/file_list.3.txt\", \"criteo_data/file_list.4.txt\"], \\\n",
    "                 keyset = [\"criteo_data/file_list.3.keyset\", \"criteo_data/file_list.4.keyset\"], \\\n",
    "                 eval_source = \"criteo_data/file_list.5.txt\")\n",
    "\n",
    "# model.fit(num_epochs = 1, display = 500, eval_interval = 1000)\n",
    "# Get the updated embedding features in model.fit()\n",
    "# updated_model = model.get_incremental_model()\n",
    "# User defined operations to the updated_model\n",
    "# ...\n",
    "#model.dump_incremental_model_2kafka()\n",
    "model.save_params_to_files(\"wdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3cccdd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r *model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7a2f3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HUGECTR][12:06:57][INFO][RANK0]: Empty embedding, trained table will be stored in ./wdl_0_sparse_model\n",
      "[HUGECTR][12:06:57][INFO][RANK0]: Empty embedding, trained table will be stored in ./wdl_1_sparse_model\n",
      "HugeCTR Version: 3.2\n",
      "====================================================Model Init=====================================================\n",
      "[HUGECTR][12:06:57][INFO][RANK0]: Initialize model: wdl\n",
      "[HUGECTR][12:06:57][INFO][RANK0]: Global seed is 715918850\n",
      "[HUGECTR][12:06:57][INFO][RANK0]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 0\n",
      "\n",
      "[HUGECTR][12:06:58][WARNING][RANK0]: Peer-to-peer access cannot be fully enabled.\n",
      "[HUGECTR][12:06:58][INFO][RANK0]: Start all2all warmup\n",
      "[HUGECTR][12:06:58][INFO][RANK0]: End all2all warmup\n",
      "[HUGECTR][12:06:58][INFO][RANK0]: Using All-reduce algorithm: NCCL\n",
      "[HUGECTR][12:06:58][INFO][RANK0]: Device 0: Tesla V100-SXM2-32GB\n",
      "[HUGECTR][12:06:58][DEBUG][RANK0]: Creating Kafka lifetime service.\n",
      "[HUGECTR][12:06:58][INFO][RANK0]: num of DataReader workers: 12\n",
      "[HUGECTR][12:06:58][INFO][RANK0]: max_vocabulary_size_per_gpu_=6029312\n",
      "[HUGECTR][12:06:58][INFO][RANK0]: max_vocabulary_size_per_gpu_=5865472\n",
      "[HUGECTR][12:06:58][INFO][RANK0]: Graph analysis to resolve tensor dependency\n",
      "===================================================Model Compile===================================================\n",
      "[HUGECTR][12:07:02][INFO][RANK0]: gpu0 start to init embedding\n",
      "[HUGECTR][12:07:02][INFO][RANK0]: gpu0 init embedding done\n",
      "[HUGECTR][12:07:02][INFO][RANK0]: gpu0 start to init embedding\n",
      "[HUGECTR][12:07:02][INFO][RANK0]: gpu0 init embedding done\n",
      "[HUGECTR][12:07:02][INFO][RANK0]: Enable HMEM-Based Parameter Server\n",
      "[HUGECTR][12:07:02][INFO][RANK0]: ./wdl_0_sparse_model not exist, create and train from scratch\n",
      "[HUGECTR][12:07:02][INFO][RANK0]: Enable HMemCache-Based Parameter Server\n",
      "[HUGECTR][12:07:02][INFO][RANK0]: ./wdl_1_sparse_model/key doesn't exist, created\n",
      "[HUGECTR][12:07:02][INFO][RANK0]: ./wdl_1_sparse_model/emb_vector doesn't exist, created\n",
      "[HUGECTR][12:07:02][INFO][RANK0]: ./wdl_1_sparse_model/Adam.m doesn't exist, created\n",
      "[HUGECTR][12:07:02][INFO][RANK0]: ./wdl_1_sparse_model/Adam.v doesn't exist, created\n",
      "[HUGECTR][12:07:03][INFO][RANK0]: Starting AUC NCCL warm-up\n",
      "[HUGECTR][12:07:03][INFO][RANK0]: Warm-up done\n",
      "===================================================Model Summary===================================================\n",
      "label                                   Dense                         Sparse                        \n",
      "label                                   dense                          wide_data,deep_data           \n",
      "(None, 1)                               (None, 13)                              \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "DistributedSlotSparseEmbeddingHash      wide_data                     sparse_embedding2             (None, 1, 1)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "DistributedSlotSparseEmbeddingHash      deep_data                     sparse_embedding1             (None, 26, 16)                \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Reshape                                 sparse_embedding1             reshape1                      (None, 416)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Reshape                                 sparse_embedding2             reshape2                      (None, 1)                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Concat                                  reshape1                      concat1                       (None, 429)                   \n",
      "                                        dense                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            concat1                       fc1                           (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc1                           relu1                         (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Dropout                                 relu1                         dropout1                      (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            dropout1                      fc2                           (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc2                           relu2                         (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Dropout                                 relu2                         dropout2                      (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            dropout2                      fc3                           (None, 1)                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Add                                     fc3                           add1                          (None, 1)                     \n",
      "                                        reshape2                                                                                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "BinaryCrossEntropyLoss                  add1                          loss                                                        \n",
      "                                        label                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "[HUGECTR][12:07:03][INFO][RANK0]: Save the model graph to wdl.json successfully\n",
      "=====================================================Model Fit=====================================================\n",
      "[HUGECTR][12:07:03][INFO][RANK0]: Use embedding training cache mode with number of training sources: 2, number of epochs: 1\n",
      "[HUGECTR][12:07:03][INFO][RANK0]: Training batchsize: 1024, evaluation batchsize: 1024\n",
      "[HUGECTR][12:07:03][INFO][RANK0]: Evaluation interval: 1000, snapshot interval: 10000\n",
      "[HUGECTR][12:07:03][INFO][RANK0]: Dense network trainable: True\n",
      "[HUGECTR][12:07:03][INFO][RANK0]: Sparse embedding sparse_embedding1 trainable: True\n",
      "[HUGECTR][12:07:03][INFO][RANK0]: Sparse embedding sparse_embedding2 trainable: True\n",
      "[HUGECTR][12:07:03][INFO][RANK0]: Use mixed precision: False, scaler: 1.000000, use cuda graph: True\n",
      "[HUGECTR][12:07:03][INFO][RANK0]: lr: 0.001000, warmup_steps: 1, end_lr: 0.000000\n",
      "[HUGECTR][12:07:03][INFO][RANK0]: decay_start: 0, decay_steps: 1, decay_power: 2.000000\n",
      "[HUGECTR][12:07:03][INFO][RANK0]: Evaluation source file: criteo_data/file_list.2.txt\n",
      "[HUGECTR][12:07:03][INFO][RANK0]: --------------------Epoch 0, source file: criteo_data/file_list.0.txt--------------------\n",
      "[HUGECTR][12:07:03][INFO][RANK0]: Preparing embedding table for next pass\n",
      "[HUGECTR][12:07:03][INFO][RANK0]: HMEM-Cache PS: Hit rate [load]: 0 %\n",
      "[HUGECTR][12:07:04][INFO][RANK0]: --------------------Epoch 0, source file: criteo_data/file_list.1.txt--------------------\n",
      "[HUGECTR][12:07:04][INFO][RANK0]: Preparing embedding table for next pass\n",
      "[HUGECTR][12:07:05][INFO][RANK0]: HMEM-Cache PS: Hit rate [dump]: 0 %\n",
      "[HUGECTR][12:07:05][INFO][RANK0]: HMEM-Cache PS: Hit rate [load]: 0 %\n",
      "[HUGECTR][12:07:05][INFO][RANK0]: HMEM-Cache PS: Hit rate [dump]: 76.75 %\n",
      "[HUGECTR][12:07:05][INFO][RANK0]: Updating sparse model in SSD [DONE]\n",
      "[HUGECTR][12:07:05][INFO][RANK0]: Sync blocks from HMEM-Cache to SSD\n",
      " \u001b[38;2;89;255;89m ████████████████████████████████████████▏ \u001b[1m\u001b[31m100.0% \u001b[34m[   2/   2 | 63.5 Hz | 0s<0s]  \u001b[0m\u001b[32m\u001b[0mm\n",
      "[HUGECTR][12:07:05][INFO][RANK0]: Dumping dense weights to file, successful\n",
      "[HUGECTR][12:07:05][INFO][RANK0]: Dumping dense optimizer states to file, successful\n",
      "[HUGECTR][12:07:05][INFO][RANK0]: Dumping untrainable weights to file, successful\n",
      "[HUGECTR][12:07:06][DEBUG][RANK0]: Destroying Kafka lifetime service.\n"
     ]
    }
   ],
   "source": [
    "!python wdl_demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e95ba8",
   "metadata": {},
   "source": [
    "## WDL Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332de683",
   "metadata": {},
   "source": [
    "### Inference using HugeCTR python API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f483fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wdl_predict.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 'wdl_predict.py'\n",
    "from hugectr.inference import InferenceParams, CreateInferenceSession\n",
    "import hugectr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from mpi4py import MPI\n",
    "def wdl_inference(model_name='wdl', network_file='wdl.json', dense_file='wdl_dense_0.model', \\\n",
    "                  embedding_file_list=['wdl_0_sparse_model', 'wdl_1_sparse_model'], data_file='infer_data.csv',\\\n",
    "                  enable_cache=False, rocksdb_path=\"\"):\n",
    "    CATEGORICAL_COLUMNS=[\"C1_C2\",\"C3_C4\"] + [\"C\" + str(x) for x in range(1, 27)]\n",
    "    CONTINUOUS_COLUMNS=[\"I\" + str(x) for x in range(1, 14)]\n",
    "    LABEL_COLUMNS = ['label']\n",
    "    test_df=pd.read_csv(data_file,sep=',')\n",
    "    config_file = network_file\n",
    "    row_ptrs = list(range(0, 11, 2)) + list(range(0, 131))\n",
    "    #row_ptrs = [0, 2] + list(range(0,27))\n",
    "    #print(row_ptrs)\n",
    "    dense_features =  list(test_df[CONTINUOUS_COLUMNS].values.flatten())\n",
    "    #dense_features = list(np.zeros(13, dtype=\"float32\"))\n",
    "    test_df[CATEGORICAL_COLUMNS].astype(np.int64)\n",
    "    embedding_columns = list((test_df[CATEGORICAL_COLUMNS]).values.flatten())\n",
    "    #embedding_columns = list(np.array([74153]*2 + [41979]*26, dtype='int64'))\n",
    "    #print(embedding_columns)\n",
    "\n",
    "    # create parameter server, embedding cache and inference session\n",
    "    inference_params = InferenceParams(model_name = model_name,\n",
    "                                max_batchsize = 64,\n",
    "                                hit_rate_threshold = 0.5,\n",
    "                                dense_model_file = dense_file,\n",
    "                                sparse_model_files = embedding_file_list,\n",
    "                                device_id = 0,\n",
    "                                use_gpu_embedding_cache = enable_cache,\n",
    "                                cache_size_percentage = 0.9,\n",
    "                                i64_input_key = True,\n",
    "                                use_mixed_precision = False)\n",
    "    inference_session = CreateInferenceSession(config_file, inference_params)\n",
    "    output = inference_session.predict(dense_features, embedding_columns, row_ptrs)\n",
    "    print(\"WDL multi-embedding table inference result is {}\".format(output))\n",
    "\n",
    "wdl_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9dde69f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HUGECTR][12:07:13][INFO][RANK0]: default_emb_vec_value is not specified using default: 0.000000\n",
      "[HUGECTR][12:07:13][INFO][RANK0]: default_emb_vec_value is not specified using default: 0.000000\n",
      "[HUGECTR][12:07:13][INFO][RANK0]: Creating ParallelHashMap CPU database backend...\n",
      "[HUGECTR][12:07:13][INFO][RANK0]: Created parallel (16 partitions) blank database backend in local memory!\n",
      "[HUGECTR][12:07:13][INFO][RANK0]: ParallelHashMap backend. Table: hctr_et.wdl.sparse_embedding2. Inserted 44547 / 44547 pairs.\n",
      "[HUGECTR][12:07:13][INFO][RANK0]: Table: hctr_et.wdl.sparse_embedding2; cached 44547 / 44547 embeddings in CPU memory database!\n",
      "[HUGECTR][12:07:13][INFO][RANK0]: ParallelHashMap backend. Table: hctr_et.wdl.sparse_embedding1. Inserted 54176 / 54176 pairs.\n",
      "[HUGECTR][12:07:13][INFO][RANK0]: Table: hctr_et.wdl.sparse_embedding1; cached 54176 / 54176 embeddings in CPU memory database!\n",
      "[HUGECTR][12:07:13][DEBUG][RANK0]: Real-time subscribers created!\n",
      "[HUGECTR][12:07:13][INFO][RANK0]: Create embedding cache in device 0.\n",
      "[HUGECTR][12:07:13][INFO][RANK0]: Use GPU embedding cache: False, cache size percentage: 0.900000\n",
      "[HUGECTR][12:07:13][INFO][RANK0]: Configured cache hit rate threshold: 0.500000\n",
      "[HUGECTR][12:07:13][INFO][RANK0]: Global seed is 1304938168\n",
      "[HUGECTR][12:07:13][INFO][RANK0]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 0\n",
      "\n",
      "[HUGECTR][12:07:14][WARNING][RANK0]: Peer-to-peer access cannot be fully enabled.\n",
      "[HUGECTR][12:07:14][INFO][RANK0]: Start all2all warmup\n",
      "[HUGECTR][12:07:14][INFO][RANK0]: End all2all warmup\n",
      "[HUGECTR][12:07:14][INFO][RANK0]: Model name: wdl\n",
      "[HUGECTR][12:07:14][INFO][RANK0]: Use mixed precision: False\n",
      "[HUGECTR][12:07:14][INFO][RANK0]: Use cuda graph: True\n",
      "[HUGECTR][12:07:14][INFO][RANK0]: Max batchsize: 64\n",
      "[HUGECTR][12:07:14][INFO][RANK0]: Use I64 input key: True\n",
      "[HUGECTR][12:07:14][INFO][RANK0]: start create embedding for inference\n",
      "[HUGECTR][12:07:14][INFO][RANK0]: sparse_input name wide_data\n",
      "[HUGECTR][12:07:14][INFO][RANK0]: sparse_input name deep_data\n",
      "[HUGECTR][12:07:14][INFO][RANK0]: create embedding for inference success\n",
      "[HUGECTR][12:07:14][INFO][RANK0]: Inference stage skip BinaryCrossEntropyLoss layer, replaced by Sigmoid layer\n",
      "[HUGECTR][12:07:15][INFO][RANK0]: Looking up 10 embeddings (each with 1 values)...\n",
      "[HUGECTR][12:07:15][INFO][RANK0]: ParallelHashMap backend. Table: hctr_et.wdl.sparse_embedding2. Fetched 9 / 10 values.\n",
      "[HUGECTR][12:07:15][INFO][RANK0]: ParallelHashMap: 9 hits, 1 missing!\n",
      "[HUGECTR][12:07:15][INFO][RANK0]: Parameter server lookup of 9 / 10 embeddings took 60 us.\n",
      "[HUGECTR][12:07:15][INFO][RANK0]: Looking up 130 embeddings (each with 16 values)...\n",
      "[HUGECTR][12:07:15][INFO][RANK0]: ParallelHashMap backend. Table: hctr_et.wdl.sparse_embedding1. Fetched 130 / 130 values.\n",
      "[HUGECTR][12:07:15][INFO][RANK0]: ParallelHashMap: 130 hits, 0 missing!\n",
      "[HUGECTR][12:07:15][INFO][RANK0]: Parameter server lookup of 130 / 130 embeddings took 292 us.\n",
      "WDL multi-embedding table inference result is [0.02313862182199955, 0.01758299395442009, 0.03633511811494827, 0.09024021774530411, 0.025988798588514328]\n"
     ]
    }
   ],
   "source": [
    "!python wdl_predict.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805f59ec",
   "metadata": {},
   "source": [
    "### Inference using Triton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7130fe",
   "metadata": {},
   "source": [
    "Please refer to the HugeCTR_Continuous_Training_and_Inference(Part2) notebook to start Triton and do the inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30fbd86",
   "metadata": {},
   "source": [
    "## Continue Training WDL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef778797",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r wdl_continue*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44f689bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing wdl_continue.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wdl_continue.py\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "solver = hugectr.CreateSolver(model_name = \"wdl\",\n",
    "                              max_eval_batches = 5000,\n",
    "                              batchsize_eval = 1024,\n",
    "                              batchsize = 1024,\n",
    "                              lr = 0.001,\n",
    "                              vvgpu = [[0]],\n",
    "                              i64_input_key = False,\n",
    "                              use_mixed_precision = False,\n",
    "                              repeat_dataset = False,\n",
    "                              use_cuda_graph = True,\n",
    "                              kafka_brockers = \"10.23.137.25:9093\")\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Norm,\n",
    "                          source = [\"criteo_data/file_list.\"+str(i)+\".txt\" for i in range(6, 9)],\n",
    "                          keyset = [\"criteo_data/file_list.\"+str(i)+\".keyset\" for i in range(6, 9)],\n",
    "                          eval_source = \"criteo_data/file_list.9.txt\",\n",
    "                          check_type = hugectr.Check_t.Sum)\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.Adam)\n",
    "hc_config = hugectr.CreateHMemCache(2, 0.5, 0)\n",
    "etc = hugectr.CreateETC(ps_types = [hugectr.TrainPSType_t.Staged, hugectr.TrainPSType_t.Cached],\\\n",
    "                        sparse_models = [\"./wdl_0_sparse_model\", \"./wdl_1_sparse_model\"],\\\n",
    "                        local_paths = [\"./\"], hmem_cache_configs = [hc_config])\n",
    "model = hugectr.Model(solver, reader, optimizer, etc)\n",
    "model.construct_from_json(graph_config_file = \"wdl.json\", include_dense_network = True)\n",
    "model.compile()\n",
    "model.load_dense_weights(\"wdl_dense_0_model\")\n",
    "model.load_dense_optimizer_states(\"dcn_opt_dense_1000.model\")\n",
    "\n",
    "model.summary()\n",
    "model.graph_to_json(graph_config_file = \"wdl.json\")\n",
    "model.fit(num_epochs = 1, display = 500, eval_interval = 1000)\n",
    "model.dump_incremental_model_2kafka()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7afd6c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HUGECTR][11:19:41][INFO][RANK0]: Use existing embedding: ./wdl_0_sparse_model\n",
      "[HUGECTR][11:19:41][INFO][RANK0]: Use existing embedding: ./wdl_1_sparse_model\n",
      "HugeCTR Version: 3.2\n",
      "====================================================Model Init=====================================================\n",
      "[HUGECTR][11:19:41][INFO][RANK0]: Initialize model: wdl\n",
      "[HUGECTR][11:19:41][INFO][RANK0]: Global seed is 3493746619\n",
      "[HUGECTR][11:19:41][INFO][RANK0]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 0\n",
      "\n",
      "[HUGECTR][11:19:43][WARNING][RANK0]: Peer-to-peer access cannot be fully enabled.\n",
      "[HUGECTR][11:19:43][INFO][RANK0]: Start all2all warmup\n",
      "[HUGECTR][11:19:43][INFO][RANK0]: End all2all warmup\n",
      "[HUGECTR][11:19:43][INFO][RANK0]: Using All-reduce algorithm: NCCL\n",
      "[HUGECTR][11:19:43][INFO][RANK0]: Device 0: Tesla V100-SXM2-32GB\n",
      "[HUGECTR][11:19:43][DEBUG][RANK0]: Creating Kafka lifetime service.\n",
      "[HUGECTR][11:19:43][INFO][RANK0]: num of DataReader workers: 12\n",
      "[HUGECTR][11:19:43][INFO][RANK0]: max_num_frequent_categories is not specified using default: 1\n",
      "[HUGECTR][11:19:43][INFO][RANK0]: max_num_infrequent_samples is not specified using default: -1\n",
      "[HUGECTR][11:19:43][INFO][RANK0]: p_dup_max is not specified using default: 0.010000\n",
      "[HUGECTR][11:19:43][INFO][RANK0]: max_all_reduce_bandwidth is not specified using default: 130000000000.000000\n",
      "[HUGECTR][11:19:43][INFO][RANK0]: max_all_to_all_bandwidth is not specified using default: 190000000000.000000\n",
      "[HUGECTR][11:19:43][INFO][RANK0]: efficiency_bandwidth_ratio is not specified using default: 1.000000\n",
      "[HUGECTR][11:19:43][INFO][RANK0]: use_train_precompute_indices is not specified using default: 0\n",
      "[HUGECTR][11:19:43][INFO][RANK0]: use_eval_precompute_indices is not specified using default: 0\n",
      "[HUGECTR][11:19:43][INFO][RANK0]: communication_type is not specified using default: IB_NVLink\n",
      "[HUGECTR][11:19:43][INFO][RANK0]: hybrid_embedding_type is not specified using default: Distributed\n",
      "[HUGECTR][11:19:43][INFO][RANK0]: max_vocabulary_size_per_gpu_=6029312\n",
      "[HUGECTR][11:19:43][INFO][RANK0]: max_num_frequent_categories is not specified using default: 1\n",
      "[HUGECTR][11:19:43][INFO][RANK0]: max_num_infrequent_samples is not specified using default: -1\n",
      "[HUGECTR][11:19:43][INFO][RANK0]: p_dup_max is not specified using default: 0.010000\n",
      "[HUGECTR][11:19:43][INFO][RANK0]: max_all_reduce_bandwidth is not specified using default: 130000000000.000000\n",
      "[HUGECTR][11:19:43][INFO][RANK0]: max_all_to_all_bandwidth is not specified using default: 190000000000.000000\n",
      "[HUGECTR][11:19:43][INFO][RANK0]: efficiency_bandwidth_ratio is not specified using default: 1.000000\n",
      "[HUGECTR][11:19:43][INFO][RANK0]: use_train_precompute_indices is not specified using default: 0\n",
      "[HUGECTR][11:19:43][INFO][RANK0]: use_eval_precompute_indices is not specified using default: 0\n",
      "[HUGECTR][11:19:43][INFO][RANK0]: communication_type is not specified using default: IB_NVLink\n",
      "[HUGECTR][11:19:43][INFO][RANK0]: hybrid_embedding_type is not specified using default: Distributed\n",
      "[HUGECTR][11:19:43][INFO][RANK0]: max_vocabulary_size_per_gpu_=5865472\n",
      "[HUGECTR][11:19:43][INFO][RANK0]: Load the model graph from wdl.json successfully\n",
      "[HUGECTR][11:19:43][INFO][RANK0]: Graph analysis to resolve tensor dependency\n",
      "===================================================Model Compile===================================================\n",
      "[HUGECTR][11:19:46][INFO][RANK0]: gpu0 start to init embedding\n",
      "[HUGECTR][11:19:46][INFO][RANK0]: gpu0 init embedding done\n",
      "[HUGECTR][11:19:46][INFO][RANK0]: gpu0 start to init embedding\n",
      "[HUGECTR][11:19:46][INFO][RANK0]: gpu0 init embedding done\n",
      "[HUGECTR][11:19:46][INFO][RANK0]: Enable HMEM-Based Parameter Server\n",
      "[HUGECTR][11:19:46][INFO][RANK0]: Enable HMemCache-Based Parameter Server\n",
      "[HUGECTR][11:19:48][INFO][RANK0]: Starting AUC NCCL warm-up\n",
      "[HUGECTR][11:19:48][INFO][RANK0]: Warm-up done\n",
      "0. Runtime error: Cannot open dense model file /jershi/HugeCTR_gitlab/hugectr/HugeCTR/pybind/model.cpp:1983 \n",
      "\n",
      "0. Runtime error: Cannot open dense opt states file /jershi/HugeCTR_gitlab/hugectr/HugeCTR/pybind/model.cpp:1934 \n",
      "\n",
      "===================================================Model Summary===================================================\n",
      "label                                   Dense                         Sparse                        \n",
      "label                                   dense                          wide_data,deep_data           \n",
      "(None, 1)                               (None, 13)                              \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "DistributedSlotSparseEmbeddingHash      wide_data                     sparse_embedding2             (None, 1, 1)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "DistributedSlotSparseEmbeddingHash      deep_data                     sparse_embedding1             (None, 26, 16)                \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Reshape                                 sparse_embedding1             reshape1                      (None, 416)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Reshape                                 sparse_embedding2             reshape2                      (None, 1)                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Concat                                  reshape1                      concat1                       (None, 429)                   \n",
      "                                        dense                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            concat1                       fc1                           (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc1                           relu1                         (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Dropout                                 relu1                         dropout1                      (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            dropout1                      fc2                           (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc2                           relu2                         (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Dropout                                 relu2                         dropout2                      (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            dropout2                      fc3                           (None, 1)                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Add                                     fc3                           add1                          (None, 1)                     \n",
      "                                        reshape2                                                                                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "BinaryCrossEntropyLoss                  add1                          loss                                                        \n",
      "                                        label                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "[HUGECTR][11:19:48][INFO][RANK0]: Save the model graph to wdl.json successfully\n",
      "=====================================================Model Fit=====================================================\n",
      "[HUGECTR][11:19:48][INFO][RANK0]: Use embedding training cache mode with number of training sources: 3, number of epochs: 1\n",
      "[HUGECTR][11:19:48][INFO][RANK0]: Training batchsize: 1024, evaluation batchsize: 1024\n",
      "[HUGECTR][11:19:48][INFO][RANK0]: Evaluation interval: 1000, snapshot interval: 10000\n",
      "[HUGECTR][11:19:48][INFO][RANK0]: Dense network trainable: True\n",
      "[HUGECTR][11:19:48][INFO][RANK0]: Sparse embedding sparse_embedding1 trainable: True\n",
      "[HUGECTR][11:19:48][INFO][RANK0]: Sparse embedding sparse_embedding2 trainable: True\n",
      "[HUGECTR][11:19:48][INFO][RANK0]: Use mixed precision: False, scaler: 1.000000, use cuda graph: True\n",
      "[HUGECTR][11:19:48][INFO][RANK0]: lr: 0.001000, warmup_steps: 1, end_lr: 0.000000\n",
      "[HUGECTR][11:19:48][INFO][RANK0]: decay_start: 0, decay_steps: 1, decay_power: 2.000000\n",
      "[HUGECTR][11:19:48][INFO][RANK0]: Evaluation source file: criteo_data/file_list.9.txt\n",
      "[HUGECTR][11:19:48][INFO][RANK0]: --------------------Epoch 0, source file: criteo_data/file_list.6.txt--------------------\n",
      "[HUGECTR][11:19:48][INFO][RANK0]: Preparing embedding table for next pass\n",
      "[HUGECTR][11:19:48][INFO][RANK0]: HMEM-Cache PS: Hit rate [load]: 0 %\n",
      "[HUGECTR][11:19:48][INFO][RANK0]: --------------------Epoch 0, source file: criteo_data/file_list.7.txt--------------------\n",
      "[HUGECTR][11:19:48][INFO][RANK0]: Preparing embedding table for next pass\n",
      "[HUGECTR][11:19:49][INFO][RANK0]: HMEM-Cache PS: Hit rate [dump]: 90.57 %\n",
      "[HUGECTR][11:19:49][INFO][RANK0]: HMEM-Cache PS: Hit rate [load]: 74.75 %\n",
      "[HUGECTR][11:19:49][INFO][RANK0]: --------------------Epoch 0, source file: criteo_data/file_list.8.txt--------------------\n",
      "[HUGECTR][11:19:49][INFO][RANK0]: Preparing embedding table for next pass\n",
      "[HUGECTR][11:19:50][INFO][RANK0]: HMEM-Cache PS: Hit rate [dump]: 95.93 %\n",
      "[HUGECTR][11:19:50][INFO][RANK0]: HMEM-Cache PS: Hit rate [load]: 87.36 %\n",
      "[HUGECTR][11:19:51][INFO][RANK0]: HMEM-Cache PS: Hit rate [dump]: 85.71 %\n",
      "[HUGECTR][11:19:51][INFO][RANK0]: HMEM-Cache PS: Hit rate [load]: 86.59 %\n",
      "[HUGECTR][11:19:51][INFO][RANK0]: Get updated portion of embedding table [DONE}\n",
      "[HUGECTR][11:19:51][INFO][RANK0]: Creating new Kafka topic \"hctr_et.wdl.sparse_embedding2\".\n",
      "[HUGECTR][11:19:51][INFO][RANK0]: Creating new Kafka topic \"hctr_et.wdl.sparse_embedding1\".\n",
      "[HUGECTR][11:20:03][DEBUG][RANK0]: Destroying Kafka lifetime service.\n"
     ]
    }
   ],
   "source": [
    "!python wdl_continue.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de692f3",
   "metadata": {},
   "source": [
    "## Inference with new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24226a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing wdl_continue_predict.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 'wdl_continue_predict.py'\n",
    "from hugectr.inference import InferenceParams, CreateInferenceSession\n",
    "import hugectr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from mpi4py import MPI\n",
    "def wdl_inference(model_name='wdl', network_file='wdl.json', dense_file='wdl_dense_0.model', \\\n",
    "                  embedding_file_list=['wdl_0_sparse_model', 'wdl_1_sparse_model'], data_file='infer_data.csv',\\\n",
    "                  enable_cache=False, rocksdb_path=\"\"):\n",
    "    CATEGORICAL_COLUMNS=[\"C1_C2\",\"C3_C4\"] + [\"C\" + str(x) for x in range(1, 27)]\n",
    "    CONTINUOUS_COLUMNS=[\"I\" + str(x) for x in range(1, 14)]\n",
    "    LABEL_COLUMNS = ['label']\n",
    "    test_df=pd.read_csv(data_file,sep=',')\n",
    "    config_file = network_file\n",
    "    row_ptrs = list(range(0, 11, 2)) + list(range(0, 131))\n",
    "    dense_features =  list(test_df[CONTINUOUS_COLUMNS].values.flatten())\n",
    "    test_df[CATEGORICAL_COLUMNS].astype(np.int64)\n",
    "    embedding_columns = list((test_df[CATEGORICAL_COLUMNS]).values.flatten())\n",
    "\n",
    "    # create parameter server, embedding cache and inference session\n",
    "    inference_params = InferenceParams(model_name = model_name,\n",
    "                                max_batchsize = 64,\n",
    "                                hit_rate_threshold = 0.5,\n",
    "                                dense_model_file = dense_file,\n",
    "                                sparse_model_files = embedding_file_list,\n",
    "                                device_id = 0,\n",
    "                                use_gpu_embedding_cache = enable_cache,\n",
    "                                cache_size_percentage = 0.9,\n",
    "                                i64_input_key = True,\n",
    "                                use_mixed_precision = False)\n",
    "    inference_session = CreateInferenceSession(config_file, inference_params)\n",
    "    output = inference_session.predict(dense_features, embedding_columns, row_ptrs)\n",
    "    print(\"WDL multi-embedding table inference result is {}\".format(output))\n",
    "\n",
    "wdl_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e648b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HUGECTR][09:39:05][INFO][RANK0]: default_emb_vec_value is not specified using default: 0.000000\n",
      "[HUGECTR][09:39:05][INFO][RANK0]: default_emb_vec_value is not specified using default: 0.000000\n",
      "[HUGECTR][09:39:05][INFO][RANK0]: Creating ParallelHashMap CPU database backend...\n",
      "[HUGECTR][09:39:05][INFO][RANK0]: Created parallel (16 partitions) blank database backend in local memory!\n",
      "Traceback (most recent call last):\n",
      "  File \"wdl_continue_predict.py\", line 35, in <module>\n",
      "    wdl_inference()\n",
      "  File \"wdl_continue_predict.py\", line 31, in wdl_inference\n",
      "    inference_session = CreateInferenceSession(config_file, inference_params)\n",
      "RuntimeError: Runtime error: Error: embeddings file not open for reading /jershi/HugeCTR_gitlab/hugectr/HugeCTR/src/inference/parameter_server.cpp:284 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python wdl_continue_predict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eea92c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
