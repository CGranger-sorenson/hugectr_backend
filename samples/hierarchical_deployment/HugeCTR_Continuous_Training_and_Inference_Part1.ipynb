{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92b6cc57",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# HugeCTR Continuous Training and Inference Demo (Part I)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca93f85",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In HugeCTR version 3.3, we finished the whole pipeline of parameter server, including \n",
    "1. The parameter dumping interface from training to kafka.\n",
    "2. Redis as a level2 cache.\n",
    "3. RocksDB as a persistence storage.\n",
    "4. Embedding cache update mechanism.\n",
    "\n",
    "\n",
    "The purpose of this notebook is to give you a brief idea of how parameter server works in terms of the flow of parameters. \n",
    "\n",
    "## Table of Contents\n",
    "-  [Data Preparation](#1)\n",
    "-  [Data Preprocessing using NVTabular](#2)\n",
    "-  [Kafka broker start](#3)\n",
    "-  [Wide&Deep Continuous Training Demo](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716238b1",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3661ae3c",
   "metadata": {},
   "source": [
    "1. Firstly, we made a folder to store our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddd368bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r criteo_data\n",
    "!rm -r criteo_script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb4b7a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir criteo_data\n",
    "!mkdir criteo_script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833df360",
   "metadata": {},
   "source": [
    "2. Download Criteo Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abb6de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://azuremlsampleexperiments.blob.core.windows.net/criteo/day_1.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd018cc0",
   "metadata": {},
   "source": [
    "**NOTE**: Replace `1` with a value from [0, 23] to use a different day.\n",
    "\n",
    "During preprocessing, the amount of data, which is used to speed up the preprocessing, fill missing values, and remove the feature values that are considered rare, is further reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3599b0c",
   "metadata": {},
   "source": [
    "3. Thirdly, preprocess the data and split the data into 6 parts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f1f1afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocess.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocess.sh\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "if [[ $# -lt 3 ]]; then\n",
    "  echo \"Usage: preprocess.sh [DATASET_NO.] [DST_DATA_DIR] [SCRIPT_TYPE] [SCRIPT_TYPE_SPECIFIC_ARGS...]\"\n",
    "  exit 2\n",
    "fi\n",
    "\n",
    "DST_DATA_DIR=$2\n",
    "\n",
    "echo \"Warning: existing $DST_DATA_DIR is erased\"\n",
    "rm -rf $DST_DATA_DIR\n",
    "\n",
    "if [[ $3 == \"nvt\" ]]; then\n",
    "  if [[ $# -ne 6 ]]; then\n",
    "\t\techo \"Usage: preprocess.sh [DATASET_NO.] [DST_DATA_DIR] nvt [IS_PARQUET_FORMAT] [IS_CRITEO_MODE] [IS_FEATURE_CROSSED]\"\n",
    "    exit 2\n",
    "\tfi\n",
    "\techo \"Preprocessing script: NVTabular\"\n",
    "elif [[ $3 == \"perl\" ]]; then\n",
    "  if [[ $# -ne 4 ]]; then\n",
    "\t\techo \"Usage: preprocess.sh [DATASET_NO.] [DST_DATA_DIR] perl [NUM_SLOTS]\"\n",
    "    exit 2\n",
    "\tfi\n",
    "\techo \"Preprocessing script: Perl\"\n",
    "elif [[ $3 == \"pandas\" ]]; then\n",
    "  if [[ $# -lt 5 ]]; then\n",
    "\t\techo \"Usage: preprocess.sh [DATASET_NO.] [DST_DATA_DIR] pandas [IS_DENSE_NORMALIZED] [IS_FEATURE_CROSSED] (FILE_LIST_LENGTH)\"\n",
    "    exit 2\n",
    "\tfi\n",
    "\techo \"Preprocessing script: Pandas\"\n",
    "else\n",
    "\techo \"Error: $3 is an invalid script type. Pick one from {nvt, perl, pandas}.\"\n",
    "\texit 2\n",
    "fi\n",
    "\n",
    "SCRIPT_TYPE=$3\n",
    "\n",
    "echo \"Getting the first few examples from the uncompressed dataset...\"\n",
    "mkdir -p $DST_DATA_DIR/train                         && \\\n",
    "mkdir -p $DST_DATA_DIR/val                           && \\\n",
    "head -n 500000 day_$1 > $DST_DATA_DIR/day_$1_small\n",
    "if [ $? -ne 0 ]; then\n",
    "\techo \"Warning: fallback to find original compressed data day_$1.gz...\"\n",
    "\techo \"Decompressing day_$1.gz...\"\n",
    "\tgzip -d -c day_$1.gz > day_$1\n",
    "\tif [ $? -ne 0 ]; then\n",
    "\t\techo \"Error: failed to decompress the file.\"\n",
    "\t\texit 2\n",
    "\tfi\n",
    "\thead -n 500000 day_$1 > $DST_DATA_DIR/day_$1_small\n",
    "\tif [ $? -ne 0 ]; then\n",
    "\t\techo \"Error: day_$1 file\"\n",
    "\t\texit 2\n",
    "\tfi\n",
    "fi\n",
    "\n",
    "echo \"Counting the number of samples in day_$1 dataset...\"\n",
    "total_count=$(wc -l $DST_DATA_DIR/day_$1_small)\n",
    "total_count=(${total_count})\n",
    "echo \"The first $total_count examples will be used in day_$1 dataset.\"\n",
    "\n",
    "echo \"Shuffling dataset...\"\n",
    "shuf $DST_DATA_DIR/day_$1_small > $DST_DATA_DIR/day_$1_shuf\n",
    "\n",
    "train_count=$(( total_count * 8 / 10))\n",
    "valtest_count=$(( total_count - train_count ))\n",
    "val_count=$(( valtest_count * 5 / 10 ))\n",
    "test_count=$(( valtest_count - val_count  ))\n",
    "\n",
    "split_dataset()\n",
    "{\n",
    "\techo \"Splitting into $train_count-sample training, $val_count-sample val, and $test_count-sample test datasets...\"\n",
    "\thead -n $train_count $DST_DATA_DIR/$1 > $DST_DATA_DIR/train/train.txt          && \\\n",
    "\ttail -n $valtest_count $DST_DATA_DIR/$1 > $DST_DATA_DIR/val/valtest.txt        && \\\n",
    "\thead -n $val_count $DST_DATA_DIR/val/valtest.txt > $DST_DATA_DIR/val/val.txt   && \\\n",
    "\ttail -n $test_count $DST_DATA_DIR/val/valtest.txt > $DST_DATA_DIR/val/test.txt\n",
    "\n",
    "\tif [ $? -ne 0 ]; then\n",
    "\t\texit 2\n",
    "\tfi\n",
    "}\n",
    "\n",
    "echo \"Preprocessing...\"\n",
    "if [[ $SCRIPT_TYPE == \"nvt\" ]]; then\n",
    "\tIS_PARQUET_FORMAT=$4\n",
    "\tIS_CRITEO_MODE=$5\n",
    "\tFEATURE_CROSS_LIST_OPTION=\"\"\n",
    "\tif [[ ( $IS_CRITEO_MODE -eq 0 ) && ( $6 -eq 1 ) ]]; then\n",
    "\t\tFEATURE_CROSS_LIST_OPTION=\"--feature_cross_list C1_C2,C3_C4\"\n",
    "\t\techo $FEATURE_CROSS_LIST_OPTION\n",
    "\tfi\n",
    "  split_dataset day_$1_shuf\n",
    "  python3 criteo_script/preprocess_nvt.py \\\n",
    "\t\t--data_path $DST_DATA_DIR             \\\n",
    "\t\t--out_path $DST_DATA_DIR              \\\n",
    "\t\t--freq_limit 6                        \\\n",
    "\t\t--device_limit_frac 0.5               \\\n",
    "\t\t--device_pool_frac 0.5                \\\n",
    "\t\t--out_files_per_proc 8                \\\n",
    "\t\t--devices \"0\"                         \\\n",
    "\t\t--num_io_threads 2                    \\\n",
    "        --parquet_format=$IS_PARQUET_FORMAT   \\\n",
    "\t\t--criteo_mode=$IS_CRITEO_MODE         \\\n",
    "\t\t$FEATURE_CROSS_LIST_OPTION\n",
    "\n",
    "elif [[ $SCRIPT_TYPE == \"perl\" ]]; then\n",
    "\tNUM_SLOT=$4\n",
    "  split_dataset day_$1_shuf\n",
    "\tperl criteo_script_legacy/preprocess.pl $DST_DATA_DIR/train/train.txt $DST_DATA_DIR/val/val.txt $DST_DATA_DIR/val/test.txt                      && \\\n",
    "\tcriteo2hugectr_legacy $NUM_SLOT $DST_DATA_DIR/train/train.txt.out $DST_DATA_DIR/train/sparse_embedding $DST_DATA_DIR/file_list.txt && \\\n",
    "\tcriteo2hugectr_legacy $NUM_SLOT $DST_DATA_DIR/val/test.txt.out $DST_DATA_DIR/val/sparse_embedding $DST_DATA_DIR/file_list_test.txt\n",
    "\n",
    "elif [[ $SCRIPT_TYPE == \"pandas\" ]]; then\n",
    "\tpython3 criteo_script/preprocess.py                 \\\n",
    "\t\t--src_csv_path=$DST_DATA_DIR/day_$1_shuf          \\\n",
    "\t\t--dst_csv_path=$DST_DATA_DIR/day_$1_shuf.out      \\\n",
    "\t\t--normalize_dense=$4 --feature_cross=$5      &&   \\\n",
    "  split_dataset day_$1_shuf.out\n",
    "\tNUM_WIDE_KEYS=\"\"\n",
    "\tif [[ $5 -ne 0 ]]; then\n",
    "\t\tNUM_WIDE_KEYS=2\n",
    "\tfi\n",
    "\n",
    "  FILE_LIST_LENGTH=\"\"\n",
    "  if [[ $# -gt 5 ]]; then\n",
    "    FILE_LIST_LENGTH=$6\n",
    "\tfi\n",
    "\n",
    "\tcriteo2hugectr $DST_DATA_DIR/train/train.txt $DST_DATA_DIR/train/sparse_embedding $DST_DATA_DIR/file_list.txt $NUM_WIDE_KEYS $FILE_LIST_LENGTH && \\\n",
    "\tcriteo2hugectr $DST_DATA_DIR/val/test.txt $DST_DATA_DIR/val/sparse_embedding $DST_DATA_DIR/file_list_test.txt $NUM_WIDE_KEYS $FILE_LIST_LENGTH\n",
    "fi\n",
    "\n",
    "if [ $? -ne 0 ]; then\n",
    "\texit 2\n",
    "fi\n",
    "\n",
    "echo \"All done!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f406798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing criteo_script/preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile criteo_script/preprocess.py\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import tempfile\n",
    "\n",
    "from six.moves import urllib\n",
    "import urllib.request \n",
    "\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import logging\n",
    "import concurrent.futures as cf\n",
    "from traceback import print_exc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing as skp\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(message)s')\n",
    "logging.root.setLevel(logging.NOTSET)\n",
    "\n",
    "NUM_INTEGER_COLUMNS = 13\n",
    "NUM_CATEGORICAL_COLUMNS = 26\n",
    "NUM_TOTAL_COLUMNS = 1 + NUM_INTEGER_COLUMNS + NUM_CATEGORICAL_COLUMNS\n",
    "\n",
    "MAX_NUM_WORKERS = NUM_TOTAL_COLUMNS\n",
    "\n",
    "INT_NAN_VALUE = np.iinfo(np.int32).min\n",
    "CAT_NAN_VALUE = '80000000'\n",
    "\n",
    "def idx2key(idx):\n",
    "    if idx == 0:\n",
    "        return 'label'\n",
    "    return 'I' + str(idx) if idx <= NUM_INTEGER_COLUMNS else 'C' + str(idx - NUM_INTEGER_COLUMNS)\n",
    "\n",
    "def _fill_missing_features_and_split(chunk, series_list_dict):\n",
    "    for cid, col in enumerate(chunk.columns):\n",
    "        NAN_VALUE = INT_NAN_VALUE if cid <= NUM_INTEGER_COLUMNS else CAT_NAN_VALUE\n",
    "        result_series = chunk[col].fillna(NAN_VALUE)\n",
    "        series_list_dict[col].append(result_series)\n",
    "\n",
    "def _merge_and_transform_series(src_series_list, col, dense_cols,\n",
    "                                normalize_dense):\n",
    "    result_series = pd.concat(src_series_list)\n",
    "\n",
    "    if col != 'label':\n",
    "        unique_value_counts = result_series.value_counts()\n",
    "        unique_value_counts = unique_value_counts.loc[unique_value_counts >= 6]\n",
    "        unique_value_counts = set(unique_value_counts.index.values)\n",
    "        NAN_VALUE = INT_NAN_VALUE if col.startswith('I') else CAT_NAN_VALUE\n",
    "        result_series = result_series.apply(\n",
    "                lambda x: x if x in unique_value_counts else NAN_VALUE)\n",
    "\n",
    "    if col == 'label' or col in dense_cols:\n",
    "        result_series = result_series.astype(np.int64)\n",
    "        le = skp.LabelEncoder()\n",
    "        result_series = pd.DataFrame(le.fit_transform(result_series))\n",
    "        if col != 'label':\n",
    "            result_series = result_series + 1\n",
    "    else:\n",
    "        oe = skp.OrdinalEncoder(dtype=np.int64)\n",
    "        result_series = pd.DataFrame(oe.fit_transform(pd.DataFrame(result_series)))\n",
    "        result_series = result_series + 1\n",
    "\n",
    "\n",
    "    if normalize_dense != 0:\n",
    "        if col in dense_cols:\n",
    "            mms = skp.MinMaxScaler(feature_range=(0,1))\n",
    "            result_series = pd.DataFrame(mms.fit_transform(result_series))\n",
    "\n",
    "    result_series.columns = [col]\n",
    "\n",
    "    min_max = (np.int64(result_series[col].min()), np.int64(result_series[col].max()))\n",
    "    if col != 'label':\n",
    "        logging.info('column {} [{}, {}]'.format(col, str(min_max[0]),str(min_max[1])))\n",
    "\n",
    "    return [result_series, min_max]\n",
    "\n",
    "def _convert_to_string(series):\n",
    "    return series.astype(str)\n",
    "\n",
    "def _merge_columns_and_feature_cross(series_list, min_max, feature_pairs,\n",
    "                                     feature_cross):\n",
    "    name_to_series = dict()\n",
    "    for series in series_list:\n",
    "        name_to_series[series.columns[0]] = series.iloc[:,0]\n",
    "    df = pd.DataFrame(name_to_series)\n",
    "    cols = [idx2key(idx) for idx in range(0, NUM_TOTAL_COLUMNS)]\n",
    "    df = df.reindex(columns=cols)\n",
    "\n",
    "    offset = np.int64(0)\n",
    "    for col in cols:\n",
    "        if col != 'label' and col.startswith('I') == False:\n",
    "            df[col] += offset\n",
    "            logging.info('column {} offset {}'.format(col, str(offset)))\n",
    "            offset += min_max[col][1]\n",
    "\n",
    "    if feature_cross != 0:\n",
    "        for idx, pair in enumerate(feature_pairs):\n",
    "            col0 = pair[0]\n",
    "            col1 = pair[1]\n",
    "\n",
    "            col1_width = int(min_max[col1][1] - min_max[col1][0] + 1)\n",
    "\n",
    "            crossed_column_series = df[col0] * col1_width + df[col1]\n",
    "            oe = skp.OrdinalEncoder(dtype=np.int64)\n",
    "            crossed_column_series = pd.DataFrame(oe.fit_transform(pd.DataFrame(crossed_column_series)))\n",
    "            crossed_column_series = crossed_column_series + 1\n",
    "\n",
    "            crossed_column = col0 + '_' + col1\n",
    "            df.insert(NUM_INTEGER_COLUMNS + 1 + idx, crossed_column, crossed_column_series)\n",
    "            crossed_column_max_val = np.int64(df[crossed_column].max())\n",
    "            logging.info('column {} [{}, {}]'.format(\n",
    "                crossed_column,\n",
    "                str(df[crossed_column].min()),\n",
    "                str(crossed_column_max_val)))\n",
    "            df[crossed_column] += offset\n",
    "            logging.info('column {} offset {}'.format(crossed_column, str(offset)))\n",
    "            offset += crossed_column_max_val\n",
    "\n",
    "    return df\n",
    "\n",
    "def _wait_futures_and_reset(futures):\n",
    "    for future in futures:\n",
    "        result = future.result()\n",
    "        if result:\n",
    "            print(result)\n",
    "    futures = list()\n",
    "\n",
    "def _process_chunks(executor, chunks_to_process, op, *argv):\n",
    "    futures = list()\n",
    "    for chunk in chunks_to_process:\n",
    "        argv_list = list(argv)\n",
    "        argv_list.insert(0, chunk)\n",
    "        new_argv = tuple(argv_list)\n",
    "        future = executor.submit(op, *new_argv)\n",
    "        futures.append(future)\n",
    "    _wait_futures_and_reset(futures)\n",
    "\n",
    "def preprocess(src_txt_name, dst_txt_name, normalize_dense, feature_cross):\n",
    "    cols = [idx2key(idx) for idx in range(0, NUM_TOTAL_COLUMNS)]\n",
    "    series_list_dict = dict()\n",
    "\n",
    "    with cf.ThreadPoolExecutor(max_workers=MAX_NUM_WORKERS) as executor:\n",
    "        logging.info('read a CSV file')\n",
    "        reader = pd.read_csv(src_txt_name, sep='\\t',\n",
    "                             names=cols,\n",
    "                             chunksize=131072)\n",
    "\n",
    "        logging.info('_fill_missing_features_and_split')\n",
    "        for col in cols:\n",
    "            series_list_dict[col] = list()\n",
    "        _process_chunks(executor, reader, _fill_missing_features_and_split,\n",
    "                        series_list_dict)\n",
    "\n",
    "    with cf.ProcessPoolExecutor(max_workers=MAX_NUM_WORKERS) as executor:\n",
    "        logging.info('_merge_and_transform_series')\n",
    "        futures = list()\n",
    "        dense_cols = [idx2key(idx+1) for idx in range(NUM_INTEGER_COLUMNS)]\n",
    "        dst_series_list = list()\n",
    "        min_max = dict()\n",
    "        for col, src_series_list in series_list_dict.items():\n",
    "            future = executor.submit(_merge_and_transform_series,\n",
    "                                     src_series_list, col, dense_cols,\n",
    "                                     normalize_dense)\n",
    "            futures.append(future)\n",
    "\n",
    "        for future in futures:\n",
    "            col = None\n",
    "            for idx, ret in enumerate(future.result()):\n",
    "                try:\n",
    "                    if idx == 0:\n",
    "                        col = ret.columns[0]\n",
    "                        dst_series_list.append(ret)\n",
    "                    else:\n",
    "                        min_max[col] = ret\n",
    "                except:\n",
    "                    print_exc()\n",
    "        futures = list()\n",
    "\n",
    "        logging.info('_merge_columns_and_feature_cross')\n",
    "        feature_pairs = [('C1', 'C2'), ('C3', 'C4')]\n",
    "        df = _merge_columns_and_feature_cross(dst_series_list, min_max, feature_pairs,\n",
    "                                              feature_cross)\n",
    "\n",
    "        \n",
    "        logging.info('_convert_to_string')\n",
    "        futures = dict()\n",
    "        for col in cols:\n",
    "            future = executor.submit(_convert_to_string, df[col])\n",
    "            futures[col] = future\n",
    "        if feature_cross != 0:\n",
    "            for pair in feature_pairs:\n",
    "                col = pair[0] + '_' + pair[1]\n",
    "                future = executor.submit(_convert_to_string, df[col])\n",
    "                futures[col] = future\n",
    "\n",
    "        logging.info('_store_to_df')\n",
    "        for col, future in futures.items():\n",
    "            ret = future.result()\n",
    "            try:\n",
    "                df[col] = ret\n",
    "            except:\n",
    "                print_exc()\n",
    "        futures = dict()\n",
    "\n",
    "        logging.info('write to a CSV file')\n",
    "        df.to_csv(dst_txt_name, sep=' ', header=False, index=False)\n",
    "\n",
    "        logging.info('done!')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    arg_parser = argparse.ArgumentParser(description='Preprocssing Criteo Dataset')\n",
    "\n",
    "    arg_parser.add_argument('--src_csv_path', type=str, required=True)\n",
    "    arg_parser.add_argument('--dst_csv_path', type=str, required=True)\n",
    "    arg_parser.add_argument('--normalize_dense', type=int, default=1)\n",
    "    arg_parser.add_argument('--feature_cross', type=int, default=1)\n",
    "\n",
    "    args = arg_parser.parse_args()\n",
    "\n",
    "    src_csv_path = args.src_csv_path\n",
    "    dst_csv_path = args.dst_csv_path\n",
    "\n",
    "    normalize_dense = args.normalize_dense\n",
    "    feature_cross = args.feature_cross\n",
    "\n",
    "    if os.path.exists(src_csv_path) == False:\n",
    "        sys.exit('ERROR: the file \\'{}\\' doesn\\'t exist'.format(src_csv_path))\n",
    "\n",
    "    if os.path.exists(dst_csv_path) == True:\n",
    "        sys.exit('ERROR: the file \\'{}\\' exists'.format(dst_csv_path))\n",
    "\n",
    "    preprocess(src_csv_path, dst_csv_path, normalize_dense, feature_cross)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2169ef",
   "metadata": {},
   "source": [
    "4. Run the preprocess script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42feed25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: existing criteo_data is erased\n",
      "Preprocessing script: Pandas\n",
      "Getting the first few examples from the uncompressed dataset...\n",
      "Counting the number of samples in day_0 dataset...\n",
      "The first 500000 examples will be used in day_0 dataset.\n",
      "Shuffling dataset...\n",
      "Preprocessing...\n",
      "2021-12-01 02:50:01,561 read a CSV file\n",
      "2021-12-01 02:50:01,563 _fill_missing_features_and_split\n",
      "2021-12-01 02:50:04,952 _merge_and_transform_series\n",
      "2021-12-01 02:50:05,560 column I6 [0, 1]\n",
      "2021-12-01 02:50:05,562 column I4 [0, 1]\n",
      "2021-12-01 02:50:05,587 column I1 [0, 1]\n",
      "2021-12-01 02:50:05,601 column I11 [0, 1]\n",
      "2021-12-01 02:50:05,604 column I3 [0, 1]\n",
      "2021-12-01 02:50:05,620 column I5 [0, 1]\n",
      "2021-12-01 02:50:05,620 column I7 [0, 0]\n",
      "2021-12-01 02:50:05,626 column I2 [0, 1]\n",
      "2021-12-01 02:50:05,645 column I10 [0, 1]\n",
      "2021-12-01 02:50:05,688 column I13 [0, 1]\n",
      "2021-12-01 02:50:05,752 column I8 [0, 1]\n",
      "2021-12-01 02:50:05,775 column I12 [0, 1]\n",
      "2021-12-01 02:50:05,803 column I9 [0, 0]\n",
      "2021-12-01 02:50:06,004 column C2 [1, 3830]\n",
      "2021-12-01 02:50:06,021 column C1 [1, 3559]\n",
      "2021-12-01 02:50:06,050 column C4 [1, 1843]\n",
      "2021-12-01 02:50:06,106 column C6 [1, 3]\n",
      "2021-12-01 02:50:06,149 column C3 [1, 5986]\n",
      "2021-12-01 02:50:06,168 column C5 [1, 4105]\n",
      "2021-12-01 02:50:06,227 column C9 [1, 25]\n",
      "2021-12-01 02:50:06,244 column C8 [1, 810]\n",
      "2021-12-01 02:50:06,262 column C7 [1, 3770]\n",
      "2021-12-01 02:50:06,416 column C14 [1, 954]\n",
      "2021-12-01 02:50:06,437 column C11 [1, 3575]\n",
      "2021-12-01 02:50:06,458 column C13 [1, 10]\n",
      "2021-12-01 02:50:06,489 column C10 [1, 4203]\n",
      "2021-12-01 02:50:06,561 column C16 [1, 42]\n",
      "2021-12-01 02:50:06,591 column C17 [1, 4]\n",
      "2021-12-01 02:50:06,607 column C15 [1, 2210]\n",
      "2021-12-01 02:50:06,635 column C12 [1, 6806]\n",
      "2021-12-01 02:50:06,639 column C18 [1, 338]\n",
      "2021-12-01 02:50:06,660 column C19 [1, 14]\n",
      "2021-12-01 02:50:06,903 column C20 [1, 3473]\n",
      "2021-12-01 02:50:06,917 column C21 [1, 4463]\n",
      "2021-12-01 02:50:06,961 column C23 [1, 3414]\n",
      "2021-12-01 02:50:06,972 column C25 [1, 38]\n",
      "2021-12-01 02:50:06,983 column C22 [1, 3745]\n",
      "2021-12-01 02:50:07,009 column C26 [1, 29]\n",
      "2021-12-01 02:50:07,049 column C24 [1, 4277]\n",
      "2021-12-01 02:50:07,055 _merge_columns_and_feature_cross\n",
      "2021-12-01 02:50:07,253 column C1 offset 0\n",
      "2021-12-01 02:50:07,255 column C2 offset 3559\n",
      "2021-12-01 02:50:07,257 column C3 offset 7389\n",
      "2021-12-01 02:50:07,259 column C4 offset 13375\n",
      "2021-12-01 02:50:07,260 column C5 offset 15218\n",
      "2021-12-01 02:50:07,262 column C6 offset 19323\n",
      "2021-12-01 02:50:07,264 column C7 offset 19326\n",
      "2021-12-01 02:50:07,265 column C8 offset 23096\n",
      "2021-12-01 02:50:07,267 column C9 offset 23906\n",
      "2021-12-01 02:50:07,269 column C10 offset 23931\n",
      "2021-12-01 02:50:07,271 column C11 offset 28134\n",
      "2021-12-01 02:50:07,272 column C12 offset 31709\n",
      "2021-12-01 02:50:07,274 column C13 offset 38515\n",
      "2021-12-01 02:50:07,276 column C14 offset 38525\n",
      "2021-12-01 02:50:07,277 column C15 offset 39479\n",
      "2021-12-01 02:50:07,279 column C16 offset 41689\n",
      "2021-12-01 02:50:07,281 column C17 offset 41731\n",
      "2021-12-01 02:50:07,282 column C18 offset 41735\n",
      "2021-12-01 02:50:07,284 column C19 offset 42073\n",
      "2021-12-01 02:50:07,286 column C20 offset 42087\n",
      "2021-12-01 02:50:07,287 column C21 offset 45560\n",
      "2021-12-01 02:50:07,289 column C22 offset 50023\n",
      "2021-12-01 02:50:07,291 column C23 offset 53768\n",
      "2021-12-01 02:50:07,292 column C24 offset 57182\n",
      "2021-12-01 02:50:07,294 column C25 offset 61459\n",
      "2021-12-01 02:50:07,296 column C26 offset 61497\n",
      "2021-12-01 02:50:07,394 column C1_C2 [1, 16580]\n",
      "2021-12-01 02:50:07,396 column C1_C2 offset 61526\n",
      "2021-12-01 02:50:07,661 column C3_C4 [1, 129558]\n",
      "2021-12-01 02:50:07,662 column C3_C4 offset 78106\n",
      "2021-12-01 02:50:07,662 _convert_to_string\n",
      "2021-12-01 02:50:07,665 _store_to_df\n",
      "2021-12-01 02:50:12,650 write to a CSV file\n",
      "2021-12-01 02:50:19,519 done!\n",
      "Splitting into 400000-sample training, 50000-sample val, and 50000-sample test datasets...\n",
      "slot_num for w&D is:27\n",
      "1\n",
      "criteo_data/train exist\n",
      "criteo_data/train/sparse_embedding0.data\n",
      "Opening criteo_data/file_list.txt\n",
      "0 keyset size is: 71154\n",
      "reopen file_list_tmp\n",
      "criteo_data/train/sparse_embedding1.data\n",
      "Opening criteo_data/file_list.txt\n",
      "1 keyset size is: 71125\n",
      "reopen file_list_tmp\n",
      "criteo_data/train/sparse_embedding2.data\n",
      "Opening criteo_data/file_list.txt\n",
      "2 keyset size is: 71269\n",
      "reopen file_list_tmp\n",
      "criteo_data/train/sparse_embedding3.data\n",
      "Opening criteo_data/file_list.txt\n",
      "3 keyset size is: 71020\n",
      "reopen file_list_tmp\n",
      "criteo_data/train/sparse_embedding4.data\n",
      "Opening criteo_data/file_list.txt\n",
      "4 keyset size is: 71051\n",
      "reopen file_list_tmp\n",
      "criteo_data/train/sparse_embedding5.data\n",
      "Opening criteo_data/file_list.txt\n",
      "5 keyset size is: 71219\n",
      "reopen file_list_tmp\n",
      "criteo_data/train/sparse_embedding6.data\n",
      "Opening criteo_data/file_list.txt\n",
      "6 keyset size is: 71313\n",
      "reopen file_list_tmp\n",
      "criteo_data/train/sparse_embedding7.data\n",
      "Opening criteo_data/file_list.txt\n",
      "7 keyset size is: 71484\n",
      "reopen file_list_tmp\n",
      "criteo_data/train/sparse_embedding8.data\n",
      "Opening criteo_data/file_list.txt\n",
      "8 keyset size is: 71590\n",
      "reopen file_list_tmp\n",
      "criteo_data/train/sparse_embedding9.data\n",
      "last keyset size is:62251\n",
      "Opening criteo_data/file_list.txt\n",
      "slot_num for w&D is:27\n",
      "1\n",
      "criteo_data/val exist\n",
      "criteo_data/val/sparse_embedding0.data\n",
      "Opening criteo_data/file_list_test.txt\n",
      "0 keyset size is: 71085\n",
      "reopen file_list_tmp\n",
      "criteo_data/val/sparse_embedding1.data\n",
      "last keyset size is:30047\n",
      "Opening criteo_data/file_list_test.txt\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "!bash preprocess.sh 0 criteo_data pandas 1 1 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdaba61",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTES**: \n",
    "- The first argument represents the dataset postfix.  For instance, if `day_1` is used, the postfix is `1`.\n",
    "- The second argument, `criteo_data`, is where the preprocessed data is stored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf30a009",
   "metadata": {},
   "source": [
    "## Start the Kafka Broker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514e3b1e",
   "metadata": {},
   "source": [
    "**Please refer to the README to start the Kafka Broker properly.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb585af1",
   "metadata": {},
   "source": [
    "## Wide&Deep Model Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "400f4d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wdl_demo.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wdl_demo.py\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "solver = hugectr.CreateSolver(model_name = \"wdl\",\n",
    "                              max_eval_batches = 5000,\n",
    "                              batchsize_eval = 1024,\n",
    "                              batchsize = 1024,\n",
    "                              lr = 0.001,\n",
    "                              vvgpu = [[0]],\n",
    "                              i64_input_key = False,\n",
    "                              use_mixed_precision = False,\n",
    "                              repeat_dataset = False,\n",
    "                              use_cuda_graph = True,\n",
    "                              kafka_brockers = \"10.23.137.25:9093\")\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Norm,\n",
    "                          source = [\"criteo_data/file_list.\"+str(i)+\".txt\" for i in range(5)],\n",
    "                          keyset = [\"criteo_data/file_list.\"+str(i)+\".keyset\" for i in range(5)],\n",
    "                          eval_source = \"criteo_data/file_list.5.txt\",\n",
    "                          check_type = hugectr.Check_t.Sum)\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.Adam)\n",
    "hc_config = hugectr.CreateHMemCache(2, 0.5, 0)\n",
    "etc = hugectr.CreateETC(ps_types = [hugectr.TrainPSType_t.Staged, hugectr.TrainPSType_t.Cached],\\\n",
    "                        sparse_models = [\"./wdl_0_sparse_model\", \"./wdl_1_sparse_model\"],\\\n",
    "                        local_paths = [\"./\"], hmem_cache_configs = [hc_config])\n",
    "model = hugectr.Model(solver, reader, optimizer, etc)\n",
    "model.add(hugectr.Input(label_dim = 1, label_name = \"label\",\n",
    "                        dense_dim = 13, dense_name = \"dense\",\n",
    "                        data_reader_sparse_param_array = \n",
    "                        [hugectr.DataReaderSparseParam(\"wide_data\", 2, True, 1),\n",
    "                        hugectr.DataReaderSparseParam(\"deep_data\", 1, True, 26)]))\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash, \n",
    "                            workspace_size_per_gpu_in_mb = 23,\n",
    "                            embedding_vec_size = 1,\n",
    "                            combiner = \"sum\",\n",
    "                            sparse_embedding_name = \"sparse_embedding0\",\n",
    "                            bottom_name = \"wide_data\",\n",
    "                            optimizer = optimizer))\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash, \n",
    "                            workspace_size_per_gpu_in_mb = 358,\n",
    "                            embedding_vec_size = 16,\n",
    "                            combiner = \"sum\",\n",
    "                            sparse_embedding_name = \"sparse_embedding1\",\n",
    "                            bottom_name = \"deep_data\",\n",
    "                            optimizer = optimizer))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding1\"],\n",
    "                            top_names = [\"reshape1\"],\n",
    "                            leading_dim=416))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding0\"],\n",
    "                            top_names = [\"reshape2\"],\n",
    "                            leading_dim=1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Concat,\n",
    "                            bottom_names = [\"reshape1\", \"dense\"], top_names = [\"concat1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"concat1\"],\n",
    "                            top_names = [\"fc1\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc1\"],\n",
    "                            top_names = [\"relu1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu1\"],\n",
    "                            top_names = [\"dropout1\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dropout1\"],\n",
    "                            top_names = [\"fc2\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc2\"],\n",
    "                            top_names = [\"relu2\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu2\"],\n",
    "                            top_names = [\"dropout2\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dropout2\"],\n",
    "                            top_names = [\"fc3\"],\n",
    "                            num_output=1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Add,\n",
    "                            bottom_names = [\"fc3\", \"reshape2\"],\n",
    "                            top_names = [\"add1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "                            bottom_names = [\"add1\", \"label\"],\n",
    "                            top_names = [\"loss\"]))\n",
    "model.compile()\n",
    "model.summary()\n",
    "model.graph_to_json(graph_config_file = \"wdl.json\")\n",
    "#model.save_params_to_files(\"wdl\")\n",
    "model.fit(num_epochs = 1, display = 500, eval_interval = 1000)\n",
    "\n",
    "# Get the updated embedding features in model.fit()\n",
    "# updated_model = model.get_incremental_model()\n",
    "# User defined operations to the updated_model\n",
    "# ...+\n",
    "model.set_source(source = [\"criteo_data/file_list.\"+str(i)+\".txt\" for i in range(5, 10)], \\\n",
    "                 keyset = [\"criteo_data/file_list.\"+str(i)+\".keyset\" for i in range(5, 10)], \\\n",
    "                 eval_source = \"criteo_data/file_list.9.txt\")\n",
    "\n",
    "# model.fit(num_epochs = 1, display = 500, eval_interval = 1000)\n",
    "# Get the updated embedding features in model.fit()\n",
    "# updated_model = model.get_incremental_model()\n",
    "# User defined operations to the updated_model\n",
    "# ...\n",
    "#model.dump_incremental_model_2kafka()\n",
    "model.save_params_to_files(\"wdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d9b49d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r *model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "45fff9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HUGECTR][11:22:11][INFO][RANK0]: Empty embedding, trained table will be stored in ./wdl_0_sparse_model\n",
      "[HUGECTR][11:22:11][INFO][RANK0]: Empty embedding, trained table will be stored in ./wdl_1_sparse_model\n",
      "HugeCTR Version: 3.2\n",
      "====================================================Model Init=====================================================\n",
      "[HUGECTR][11:22:11][INFO][RANK0]: Initialize model: wdl\n",
      "[HUGECTR][11:22:11][INFO][RANK0]: Global seed is 2046214884\n",
      "[HUGECTR][11:22:11][INFO][RANK0]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 0\n",
      "\n",
      "[HUGECTR][11:22:13][WARNING][RANK0]: Peer-to-peer access cannot be fully enabled.\n",
      "[HUGECTR][11:22:13][INFO][RANK0]: Start all2all warmup\n",
      "[HUGECTR][11:22:13][INFO][RANK0]: End all2all warmup\n",
      "[HUGECTR][11:22:13][INFO][RANK0]: Using All-reduce algorithm: NCCL\n",
      "[HUGECTR][11:22:13][INFO][RANK0]: Device 0: Tesla V100-SXM2-32GB\n",
      "[HUGECTR][11:22:13][DEBUG][RANK0]: Creating Kafka lifetime service.\n",
      "[HUGECTR][11:22:13][INFO][RANK0]: num of DataReader workers: 12\n",
      "[HUGECTR][11:22:13][INFO][RANK0]: max_vocabulary_size_per_gpu_=6029312\n",
      "[HUGECTR][11:22:13][INFO][RANK0]: max_vocabulary_size_per_gpu_=5865472\n",
      "[HUGECTR][11:22:13][INFO][RANK0]: Graph analysis to resolve tensor dependency\n",
      "===================================================Model Compile===================================================\n",
      "[HUGECTR][11:22:16][INFO][RANK0]: gpu0 start to init embedding\n",
      "[HUGECTR][11:22:16][INFO][RANK0]: gpu0 init embedding done\n",
      "[HUGECTR][11:22:16][INFO][RANK0]: gpu0 start to init embedding\n",
      "[HUGECTR][11:22:16][INFO][RANK0]: gpu0 init embedding done\n",
      "[HUGECTR][11:22:16][INFO][RANK0]: Enable HMEM-Based Parameter Server\n",
      "[HUGECTR][11:22:16][INFO][RANK0]: ./wdl_0_sparse_model not exist, create and train from scratch\n",
      "[HUGECTR][11:22:16][INFO][RANK0]: Enable HMemCache-Based Parameter Server\n",
      "[HUGECTR][11:22:16][INFO][RANK0]: ./wdl_1_sparse_model/key doesn't exist, created\n",
      "[HUGECTR][11:22:16][INFO][RANK0]: ./wdl_1_sparse_model/emb_vector doesn't exist, created\n",
      "[HUGECTR][11:22:16][INFO][RANK0]: ./wdl_1_sparse_model/Adam.m doesn't exist, created\n",
      "[HUGECTR][11:22:16][INFO][RANK0]: ./wdl_1_sparse_model/Adam.v doesn't exist, created\n",
      "[HUGECTR][11:22:17][INFO][RANK0]: Starting AUC NCCL warm-up\n",
      "[HUGECTR][11:22:17][INFO][RANK0]: Warm-up done\n",
      "===================================================Model Summary===================================================\n",
      "label                                   Dense                         Sparse                        \n",
      "label                                   dense                          wide_data,deep_data           \n",
      "(None, 1)                               (None, 13)                              \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "DistributedSlotSparseEmbeddingHash      wide_data                     sparse_embedding0             (None, 1, 1)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "DistributedSlotSparseEmbeddingHash      deep_data                     sparse_embedding1             (None, 26, 16)                \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Reshape                                 sparse_embedding1             reshape1                      (None, 416)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Reshape                                 sparse_embedding0             reshape2                      (None, 1)                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Concat                                  reshape1                      concat1                       (None, 429)                   \n",
      "                                        dense                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            concat1                       fc1                           (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc1                           relu1                         (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Dropout                                 relu1                         dropout1                      (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            dropout1                      fc2                           (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc2                           relu2                         (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Dropout                                 relu2                         dropout2                      (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            dropout2                      fc3                           (None, 1)                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Add                                     fc3                           add1                          (None, 1)                     \n",
      "                                        reshape2                                                                                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "BinaryCrossEntropyLoss                  add1                          loss                                                        \n",
      "                                        label                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "[HUGECTR][11:22:17][INFO][RANK0]: Save the model graph to wdl.json successfully\n",
      "=====================================================Model Fit=====================================================\n",
      "[HUGECTR][11:22:17][INFO][RANK0]: Use embedding training cache mode with number of training sources: 5, number of epochs: 1\n",
      "[HUGECTR][11:22:17][INFO][RANK0]: Training batchsize: 1024, evaluation batchsize: 1024\n",
      "[HUGECTR][11:22:17][INFO][RANK0]: Evaluation interval: 1000, snapshot interval: 10000\n",
      "[HUGECTR][11:22:17][INFO][RANK0]: Dense network trainable: True\n",
      "[HUGECTR][11:22:17][INFO][RANK0]: Sparse embedding sparse_embedding0 trainable: True\n",
      "[HUGECTR][11:22:17][INFO][RANK0]: Sparse embedding sparse_embedding1 trainable: True\n",
      "[HUGECTR][11:22:17][INFO][RANK0]: Use mixed precision: False, scaler: 1.000000, use cuda graph: True\n",
      "[HUGECTR][11:22:17][INFO][RANK0]: lr: 0.001000, warmup_steps: 1, end_lr: 0.000000\n",
      "[HUGECTR][11:22:17][INFO][RANK0]: decay_start: 0, decay_steps: 1, decay_power: 2.000000\n",
      "[HUGECTR][11:22:17][INFO][RANK0]: Evaluation source file: criteo_data/file_list.5.txt\n",
      "[HUGECTR][11:22:17][INFO][RANK0]: --------------------Epoch 0, source file: criteo_data/file_list.0.txt--------------------\n",
      "[HUGECTR][11:22:17][INFO][RANK0]: Preparing embedding table for next pass\n",
      "[HUGECTR][11:22:17][INFO][RANK0]: HMEM-Cache PS: Hit rate [load]: 0 %\n",
      "[HUGECTR][11:22:18][INFO][RANK0]: --------------------Epoch 0, source file: criteo_data/file_list.1.txt--------------------\n",
      "[HUGECTR][11:22:18][INFO][RANK0]: Preparing embedding table for next pass\n",
      "[HUGECTR][11:22:18][INFO][RANK0]: HMEM-Cache PS: Hit rate [dump]: 0 %\n",
      "[HUGECTR][11:22:18][INFO][RANK0]: HMEM-Cache PS: Hit rate [load]: 0 %\n",
      "[HUGECTR][11:22:19][INFO][RANK0]: --------------------Epoch 0, source file: criteo_data/file_list.2.txt--------------------\n",
      "[HUGECTR][11:22:19][INFO][RANK0]: Preparing embedding table for next pass\n",
      "[HUGECTR][11:22:19][INFO][RANK0]: HMEM-Cache PS: Hit rate [dump]: 76.51 %\n",
      "[HUGECTR][11:22:19][INFO][RANK0]: HMEM-Cache PS: Hit rate [load]: 69.58 %\n",
      "[HUGECTR][11:22:20][INFO][RANK0]: --------------------Epoch 0, source file: criteo_data/file_list.3.txt--------------------\n",
      "[HUGECTR][11:22:20][INFO][RANK0]: Preparing embedding table for next pass\n",
      "[HUGECTR][11:22:20][INFO][RANK0]: HMEM-Cache PS: Hit rate [dump]: 63.23 %\n",
      "[HUGECTR][11:22:20][INFO][RANK0]: HMEM-Cache PS: Hit rate [load]: 65.85 %\n",
      "[HUGECTR][11:22:21][INFO][RANK0]: --------------------Epoch 0, source file: criteo_data/file_list.4.txt--------------------\n",
      "[HUGECTR][11:22:21][INFO][RANK0]: Preparing embedding table for next pass\n",
      "[HUGECTR][11:22:21][INFO][RANK0]: HMEM-Cache PS: Hit rate [dump]: 63.22 %\n",
      "[HUGECTR][11:22:21][INFO][RANK0]: HMEM-Cache PS: Hit rate [load]: 64.53 %\n",
      "[HUGECTR][11:22:22][INFO][RANK0]: HMEM-Cache PS: Hit rate [dump]: 63.28 %\n",
      "[HUGECTR][11:22:22][INFO][RANK0]: Updating sparse model in SSD [DONE]\n",
      "[HUGECTR][11:22:22][INFO][RANK0]: Sync blocks from HMEM-Cache to SSD\n",
      " \u001b[38;2;89;255;89m ████████████████████████████████████████▏ \u001b[1m\u001b[31m100.0% \u001b[34m[   2/   2 | 83.2 Hz | 0s<0s]  \u001b[0m\u001b[32m\u001b[0mm\n",
      "[HUGECTR][11:22:22][INFO][RANK0]: Dumping dense weights to file, successful\n",
      "[HUGECTR][11:22:22][INFO][RANK0]: Dumping dense optimizer states to file, successful\n",
      "[HUGECTR][11:22:22][INFO][RANK0]: Dumping untrainable weights to file, successful\n",
      "[HUGECTR][11:22:24][DEBUG][RANK0]: Destroying Kafka lifetime service.\n"
     ]
    }
   ],
   "source": [
    "!python wdl_demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101fed17",
   "metadata": {},
   "source": [
    "## WDL Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202d558b",
   "metadata": {},
   "source": [
    "### Inference using HugeCTR python API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "537e62a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/jershi/rocksdb’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir /jershi/rocksdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "02a76b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wdl_predict.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 'wdl_predict.py'\n",
    "from hugectr.inference import InferenceParams, CreateInferenceSession\n",
    "import hugectr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from mpi4py import MPI\n",
    "def wdl_inference(model_name='wdl', network_file='wdl.json', dense_file='wdl_dense_0.model', \\\n",
    "                  embedding_file_list=['wdl_0_sparse_model', 'wdl_1_sparse_model'], data_file='infer_data.csv',\\\n",
    "                  enable_cache=False, rocksdb_path=\"\"):\n",
    "    CATEGORICAL_COLUMNS=[\"C1_C2\",\"C3_C4\"] + [\"C\" + str(x) for x in range(1, 27)]\n",
    "    CONTINUOUS_COLUMNS=[\"I\" + str(x) for x in range(1, 14)]\n",
    "    LABEL_COLUMNS = ['label']\n",
    "    test_df=pd.read_csv(data_file,sep=',')\n",
    "    config_file = network_file\n",
    "    row_ptrs = list(range(0, 11, 2)) + list(range(0, 131))\n",
    "    #row_ptrs = [0, 2] + list(range(0,27))\n",
    "    #print(row_ptrs)\n",
    "    dense_features =  list(test_df[CONTINUOUS_COLUMNS].values.flatten())\n",
    "    #dense_features = list(np.zeros(13, dtype=\"float32\"))\n",
    "    test_df[CATEGORICAL_COLUMNS].astype(np.int64)\n",
    "    embedding_columns = list((test_df[CATEGORICAL_COLUMNS]).values.flatten())\n",
    "    #embedding_columns = list(np.array([74153]*2 + [41979]*26, dtype='int64'))\n",
    "    #print(embedding_columns)\n",
    "\n",
    "    cpuMemoryDatabase=hugectr.inference.CPUMemoryDatabaseParams(hugectr.DatabaseType_t.parallel_hash_map)\n",
    "    rocksdbdatabase=hugectr.inference.PersistentDatabaseParams(hugectr.DatabaseType_t.disabled,path=\"/jershi/rocksdb/\")\n",
    "    redisdatabase=hugectr.inference.DistributedDatabaseParams(hugectr.DatabaseType_t.disabled,address=\"127.0.0.1:7000,127.0.0.1:7001,127.0.0.1:7002\")\n",
    "    \n",
    "    # create parameter server, embedding cache and inference session\n",
    "    inference_params = InferenceParams(model_name = model_name,\n",
    "                                max_batchsize = 64,\n",
    "                                hit_rate_threshold = 0.5,\n",
    "                                dense_model_file = dense_file,\n",
    "                                sparse_model_files = embedding_file_list,\n",
    "                                device_id = 0,\n",
    "                                use_gpu_embedding_cache = enable_cache,\n",
    "                                cache_size_percentage = 0.9,\n",
    "                                i64_input_key = True,\n",
    "                                use_mixed_precision = False,\n",
    "                                cpu_memory_db=cpuMemoryDatabase,\n",
    "                                persistent_db=rocksdbdatabase,\n",
    "                                distributed_db=redisdatabase)\n",
    "    inference_session = CreateInferenceSession(config_file, inference_params)\n",
    "    output = inference_session.predict(dense_features, embedding_columns, row_ptrs)\n",
    "    print(\"WDL multi-embedding table inference result is {}\".format(output))\n",
    "\n",
    "wdl_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8147fe24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HUGECTR][11:22:37][INFO][RANK0]: default_emb_vec_value is not specified using default: 0.000000\n",
      "[HUGECTR][11:22:37][INFO][RANK0]: default_emb_vec_value is not specified using default: 0.000000\n",
      "[HUGECTR][11:22:37][INFO][RANK0]: Creating ParallelHashMap CPU database backend...\n",
      "[HUGECTR][11:22:37][INFO][RANK0]: Created parallel (16 partitions) blank database backend in local memory!\n",
      "[HUGECTR][11:22:37][DEBUG][RANK0]: ParallelHashMap backend. Table: hctr_et.wdl.sparse_embedding0. Inserted 82424 / 82424 pairs.\n",
      "[HUGECTR][11:22:37][INFO][RANK0]: Table: hctr_et.wdl.sparse_embedding0; cached 82424 / 82424 embeddings in CPU memory database!\n",
      "[HUGECTR][11:22:37][DEBUG][RANK0]: ParallelHashMap backend. Table: hctr_et.wdl.sparse_embedding1. Inserted 60909 / 60909 pairs.\n",
      "[HUGECTR][11:22:37][INFO][RANK0]: Table: hctr_et.wdl.sparse_embedding1; cached 60909 / 60909 embeddings in CPU memory database!\n",
      "[HUGECTR][11:22:37][DEBUG][RANK0]: Real-time subscribers created!\n",
      "[HUGECTR][11:22:37][INFO][RANK0]: Create embedding cache in device 0.\n",
      "[HUGECTR][11:22:37][INFO][RANK0]: Use GPU embedding cache: False, cache size percentage: 0.900000\n",
      "[HUGECTR][11:22:37][INFO][RANK0]: Configured cache hit rate threshold: 0.500000\n",
      "[HUGECTR][11:22:37][INFO][RANK0]: Global seed is 2682958633\n",
      "[HUGECTR][11:22:37][INFO][RANK0]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 0\n",
      "\n",
      "[HUGECTR][11:22:39][WARNING][RANK0]: Peer-to-peer access cannot be fully enabled.\n",
      "[HUGECTR][11:22:39][INFO][RANK0]: Start all2all warmup\n",
      "[HUGECTR][11:22:39][INFO][RANK0]: End all2all warmup\n",
      "[HUGECTR][11:22:39][INFO][RANK0]: Model name: wdl\n",
      "[HUGECTR][11:22:39][INFO][RANK0]: Use mixed precision: False\n",
      "[HUGECTR][11:22:39][INFO][RANK0]: Use cuda graph: True\n",
      "[HUGECTR][11:22:39][INFO][RANK0]: Max batchsize: 64\n",
      "[HUGECTR][11:22:39][INFO][RANK0]: Use I64 input key: True\n",
      "[HUGECTR][11:22:39][INFO][RANK0]: start create embedding for inference\n",
      "[HUGECTR][11:22:39][INFO][RANK0]: sparse_input name wide_data\n",
      "[HUGECTR][11:22:39][INFO][RANK0]: sparse_input name deep_data\n",
      "[HUGECTR][11:22:39][INFO][RANK0]: create embedding for inference success\n",
      "[HUGECTR][11:22:39][INFO][RANK0]: Inference stage skip BinaryCrossEntropyLoss layer, replaced by Sigmoid layer\n",
      "[HUGECTR][11:22:39][INFO][RANK0]: Looking up 10 embeddings (each with 1 values)...\n",
      "[HUGECTR][11:22:39][DEBUG][RANK0]: ParallelHashMap backend. Table: hctr_et.wdl.sparse_embedding0. Fetched 9 / 10 values.\n",
      "[HUGECTR][11:22:39][DEBUG][RANK0]: ParallelHashMap: 9 hits, 1 missing!\n",
      "[HUGECTR][11:22:39][INFO][RANK0]: Parameter server lookup of 9 / 10 embeddings took 140 us.\n",
      "[HUGECTR][11:22:39][INFO][RANK0]: Looking up 130 embeddings (each with 16 values)...\n",
      "[HUGECTR][11:22:39][DEBUG][RANK0]: ParallelHashMap backend. Table: hctr_et.wdl.sparse_embedding1. Fetched 130 / 130 values.\n",
      "[HUGECTR][11:22:39][DEBUG][RANK0]: ParallelHashMap: 130 hits, 0 missing!\n",
      "[HUGECTR][11:22:39][INFO][RANK0]: Parameter server lookup of 130 / 130 embeddings took 753 us.\n",
      "WDL multi-embedding table inference result is [0.016208581626415253, 0.00580210005864501, 0.009868821129202843, 0.010509094223380089, 0.022976448759436607]\n"
     ]
    }
   ],
   "source": [
    "!python wdl_predict.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552b9cf4",
   "metadata": {},
   "source": [
    "### Inference using Triton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40cdc3d",
   "metadata": {},
   "source": [
    "Please refer to the HugeCTR_Continuous_Training_and_Inference(Part2) notebook to start Triton and do the inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1063f2a",
   "metadata": {},
   "source": [
    "## Continue Training WDL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1fe87d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r wdl_continue*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ebb53441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wdl_continue.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wdl_continue.py\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "solver = hugectr.CreateSolver(model_name = \"wdl\",\n",
    "                              max_eval_batches = 5000,\n",
    "                              batchsize_eval = 1024,\n",
    "                              batchsize = 1024,\n",
    "                              lr = 0.001,\n",
    "                              vvgpu = [[0]],\n",
    "                              i64_input_key = False,\n",
    "                              use_mixed_precision = False,\n",
    "                              repeat_dataset = False,\n",
    "                              use_cuda_graph = True,\n",
    "                              kafka_brockers = \"10.23.137.25:9093\")\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Norm,\n",
    "                          source = [\"criteo_data/file_list.\"+str(i)+\".txt\" for i in range(6, 9)],\n",
    "                          keyset = [\"criteo_data/file_list.\"+str(i)+\".keyset\" for i in range(6, 9)],\n",
    "                          eval_source = \"criteo_data/file_list.9.txt\",\n",
    "                          check_type = hugectr.Check_t.Sum)\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.Adam)\n",
    "hc_config = hugectr.CreateHMemCache(2, 0.5, 0)\n",
    "etc = hugectr.CreateETC(ps_types = [hugectr.TrainPSType_t.Staged, hugectr.TrainPSType_t.Cached],\\\n",
    "                        sparse_models = [\"./wdl_0_sparse_model\", \"./wdl_1_sparse_model\"],\\\n",
    "                        local_paths = [\"./\"], hmem_cache_configs = [hc_config])\n",
    "model = hugectr.Model(solver, reader, optimizer, etc)\n",
    "model.construct_from_json(graph_config_file = \"wdl.json\", include_dense_network = True)\n",
    "model.compile()\n",
    "model.load_dense_weights(\"wdl_dense_0_model\")\n",
    "model.load_dense_optimizer_states(\"dcn_opt_dense_1000.model\")\n",
    "\n",
    "model.summary()\n",
    "model.graph_to_json(graph_config_file = \"wdl.json\")\n",
    "model.fit(num_epochs = 1, display = 500, eval_interval = 1000)\n",
    "model.dump_incremental_model_2kafka()\n",
    "model.save_params_to_files(\"wdl_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f3ef5331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HUGECTR][11:23:51][INFO][RANK0]: Use existing embedding: ./wdl_0_sparse_model\n",
      "[HUGECTR][11:23:51][INFO][RANK0]: Use existing embedding: ./wdl_1_sparse_model\n",
      "HugeCTR Version: 3.2\n",
      "====================================================Model Init=====================================================\n",
      "[HUGECTR][11:23:51][INFO][RANK0]: Initialize model: wdl\n",
      "[HUGECTR][11:23:51][INFO][RANK0]: Global seed is 2875343134\n",
      "[HUGECTR][11:23:51][INFO][RANK0]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 0\n",
      "\n",
      "[HUGECTR][11:23:52][WARNING][RANK0]: Peer-to-peer access cannot be fully enabled.\n",
      "[HUGECTR][11:23:52][INFO][RANK0]: Start all2all warmup\n",
      "[HUGECTR][11:23:52][INFO][RANK0]: End all2all warmup\n",
      "[HUGECTR][11:23:52][INFO][RANK0]: Using All-reduce algorithm: NCCL\n",
      "[HUGECTR][11:23:52][INFO][RANK0]: Device 0: Tesla V100-SXM2-32GB\n",
      "[HUGECTR][11:23:52][DEBUG][RANK0]: Creating Kafka lifetime service.\n",
      "[HUGECTR][11:23:52][INFO][RANK0]: num of DataReader workers: 12\n",
      "[HUGECTR][11:23:52][INFO][RANK0]: max_num_frequent_categories is not specified using default: 1\n",
      "[HUGECTR][11:23:52][INFO][RANK0]: max_num_infrequent_samples is not specified using default: -1\n",
      "[HUGECTR][11:23:52][INFO][RANK0]: p_dup_max is not specified using default: 0.010000\n",
      "[HUGECTR][11:23:52][INFO][RANK0]: max_all_reduce_bandwidth is not specified using default: 130000000000.000000\n",
      "[HUGECTR][11:23:52][INFO][RANK0]: max_all_to_all_bandwidth is not specified using default: 190000000000.000000\n",
      "[HUGECTR][11:23:52][INFO][RANK0]: efficiency_bandwidth_ratio is not specified using default: 1.000000\n",
      "[HUGECTR][11:23:52][INFO][RANK0]: use_train_precompute_indices is not specified using default: 0\n",
      "[HUGECTR][11:23:52][INFO][RANK0]: use_eval_precompute_indices is not specified using default: 0\n",
      "[HUGECTR][11:23:52][INFO][RANK0]: communication_type is not specified using default: IB_NVLink\n",
      "[HUGECTR][11:23:52][INFO][RANK0]: hybrid_embedding_type is not specified using default: Distributed\n",
      "[HUGECTR][11:23:52][INFO][RANK0]: max_vocabulary_size_per_gpu_=6029312\n",
      "[HUGECTR][11:23:52][INFO][RANK0]: max_num_frequent_categories is not specified using default: 1\n",
      "[HUGECTR][11:23:52][INFO][RANK0]: max_num_infrequent_samples is not specified using default: -1\n",
      "[HUGECTR][11:23:52][INFO][RANK0]: p_dup_max is not specified using default: 0.010000\n",
      "[HUGECTR][11:23:52][INFO][RANK0]: max_all_reduce_bandwidth is not specified using default: 130000000000.000000\n",
      "[HUGECTR][11:23:52][INFO][RANK0]: max_all_to_all_bandwidth is not specified using default: 190000000000.000000\n",
      "[HUGECTR][11:23:52][INFO][RANK0]: efficiency_bandwidth_ratio is not specified using default: 1.000000\n",
      "[HUGECTR][11:23:52][INFO][RANK0]: use_train_precompute_indices is not specified using default: 0\n",
      "[HUGECTR][11:23:52][INFO][RANK0]: use_eval_precompute_indices is not specified using default: 0\n",
      "[HUGECTR][11:23:52][INFO][RANK0]: communication_type is not specified using default: IB_NVLink\n",
      "[HUGECTR][11:23:52][INFO][RANK0]: hybrid_embedding_type is not specified using default: Distributed\n",
      "[HUGECTR][11:23:52][INFO][RANK0]: max_vocabulary_size_per_gpu_=5865472\n",
      "[HUGECTR][11:23:52][INFO][RANK0]: Load the model graph from wdl.json successfully\n",
      "[HUGECTR][11:23:52][INFO][RANK0]: Graph analysis to resolve tensor dependency\n",
      "===================================================Model Compile===================================================\n",
      "[HUGECTR][11:23:56][INFO][RANK0]: gpu0 start to init embedding\n",
      "[HUGECTR][11:23:56][INFO][RANK0]: gpu0 init embedding done\n",
      "[HUGECTR][11:23:56][INFO][RANK0]: gpu0 start to init embedding\n",
      "[HUGECTR][11:23:56][INFO][RANK0]: gpu0 init embedding done\n",
      "[HUGECTR][11:23:56][INFO][RANK0]: Enable HMEM-Based Parameter Server\n",
      "[HUGECTR][11:23:56][INFO][RANK0]: Enable HMemCache-Based Parameter Server\n",
      "[HUGECTR][11:23:57][INFO][RANK0]: Starting AUC NCCL warm-up\n",
      "[HUGECTR][11:23:57][INFO][RANK0]: Warm-up done\n",
      "0. Runtime error: Cannot open dense model file /jershi/HugeCTR_gitlab/hugectr/HugeCTR/pybind/model.cpp:1983 \n",
      "\n",
      "0. Runtime error: Cannot open dense opt states file /jershi/HugeCTR_gitlab/hugectr/HugeCTR/pybind/model.cpp:1934 \n",
      "\n",
      "===================================================Model Summary===================================================\n",
      "label                                   Dense                         Sparse                        \n",
      "label                                   dense                          wide_data,deep_data           \n",
      "(None, 1)                               (None, 13)                              \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "DistributedSlotSparseEmbeddingHash      wide_data                     sparse_embedding0             (None, 1, 1)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "DistributedSlotSparseEmbeddingHash      deep_data                     sparse_embedding1             (None, 26, 16)                \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Reshape                                 sparse_embedding1             reshape1                      (None, 416)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Reshape                                 sparse_embedding0             reshape2                      (None, 1)                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Concat                                  reshape1                      concat1                       (None, 429)                   \n",
      "                                        dense                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            concat1                       fc1                           (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc1                           relu1                         (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Dropout                                 relu1                         dropout1                      (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            dropout1                      fc2                           (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc2                           relu2                         (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Dropout                                 relu2                         dropout2                      (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            dropout2                      fc3                           (None, 1)                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Add                                     fc3                           add1                          (None, 1)                     \n",
      "                                        reshape2                                                                                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "BinaryCrossEntropyLoss                  add1                          loss                                                        \n",
      "                                        label                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "[HUGECTR][11:23:57][INFO][RANK0]: Save the model graph to wdl.json successfully\n",
      "=====================================================Model Fit=====================================================\n",
      "[HUGECTR][11:23:57][INFO][RANK0]: Use embedding training cache mode with number of training sources: 3, number of epochs: 1\n",
      "[HUGECTR][11:23:57][INFO][RANK0]: Training batchsize: 1024, evaluation batchsize: 1024\n",
      "[HUGECTR][11:23:57][INFO][RANK0]: Evaluation interval: 1000, snapshot interval: 10000\n",
      "[HUGECTR][11:23:57][INFO][RANK0]: Dense network trainable: True\n",
      "[HUGECTR][11:23:57][INFO][RANK0]: Sparse embedding sparse_embedding0 trainable: True\n",
      "[HUGECTR][11:23:57][INFO][RANK0]: Sparse embedding sparse_embedding1 trainable: True\n",
      "[HUGECTR][11:23:57][INFO][RANK0]: Use mixed precision: False, scaler: 1.000000, use cuda graph: True\n",
      "[HUGECTR][11:23:57][INFO][RANK0]: lr: 0.001000, warmup_steps: 1, end_lr: 0.000000\n",
      "[HUGECTR][11:23:57][INFO][RANK0]: decay_start: 0, decay_steps: 1, decay_power: 2.000000\n",
      "[HUGECTR][11:23:57][INFO][RANK0]: Evaluation source file: criteo_data/file_list.9.txt\n",
      "[HUGECTR][11:23:57][INFO][RANK0]: --------------------Epoch 0, source file: criteo_data/file_list.6.txt--------------------\n",
      "[HUGECTR][11:23:57][INFO][RANK0]: Preparing embedding table for next pass\n",
      "[HUGECTR][11:23:57][INFO][RANK0]: HMEM-Cache PS: Hit rate [load]: 0 %\n",
      "[HUGECTR][11:23:58][INFO][RANK0]: --------------------Epoch 0, source file: criteo_data/file_list.7.txt--------------------\n",
      "[HUGECTR][11:23:58][INFO][RANK0]: Preparing embedding table for next pass\n",
      "[HUGECTR][11:23:58][INFO][RANK0]: HMEM-Cache PS: Hit rate [dump]: 99.05 %\n",
      "[HUGECTR][11:23:58][INFO][RANK0]: HMEM-Cache PS: Hit rate [load]: 76.65 %\n",
      "[HUGECTR][11:23:59][INFO][RANK0]: --------------------Epoch 0, source file: criteo_data/file_list.8.txt--------------------\n",
      "[HUGECTR][11:23:59][INFO][RANK0]: Preparing embedding table for next pass\n",
      "[HUGECTR][11:23:59][INFO][RANK0]: HMEM-Cache PS: Hit rate [dump]: 99.7 %\n",
      "[HUGECTR][11:23:59][INFO][RANK0]: HMEM-Cache PS: Hit rate [load]: 90.3 %\n",
      "[HUGECTR][11:24:00][INFO][RANK0]: HMEM-Cache PS: Hit rate [dump]: 90.2 %\n",
      "[HUGECTR][11:24:00][INFO][RANK0]: HMEM-Cache PS: Hit rate [load]: 92.37 %\n",
      "[HUGECTR][11:24:00][INFO][RANK0]: Get updated portion of embedding table [DONE}\n",
      "embedding_size1\n",
      "table namesparse_embedding0\n",
      "taghctr_et.wdl.sparse_embedding0\n",
      "key size58853\n",
      "vector size58853\n",
      "[HUGECTR][11:24:00][INFO][RANK0]: Creating new Kafka topic \"hctr_et.wdl.sparse_embedding0\".\n",
      "0done\n",
      "embedding_size16\n",
      "table namesparse_embedding1\n",
      "taghctr_et.wdl.sparse_embedding1\n",
      "key size58383\n",
      "vector size934128\n",
      "[HUGECTR][11:24:00][INFO][RANK0]: Creating new Kafka topic \"hctr_et.wdl.sparse_embedding1\".\n",
      "1done\n",
      "[HUGECTR][11:24:08][INFO][RANK0]: HMEM-Cache PS: Hit rate [dump]: 90.2 %\n",
      "[HUGECTR][11:24:08][INFO][RANK0]: Updating sparse model in SSD [DONE]\n",
      "[HUGECTR][11:24:08][INFO][RANK0]: Sync blocks from HMEM-Cache to SSD\n",
      " \u001b[38;2;89;255;89m ████████████████████████████████████████▏ \u001b[1m\u001b[31m100.0% \u001b[34m[   2/   2 | 59.7 Hz | 0s<0s]  \u001b[0m\u001b[32m\u001b[0mm\n",
      "[HUGECTR][11:24:08][INFO][RANK0]: Dumping dense weights to file, successful\n",
      "[HUGECTR][11:24:08][INFO][RANK0]: Dumping dense optimizer states to file, successful\n",
      "[HUGECTR][11:24:08][INFO][RANK0]: Dumping untrainable weights to file, successful\n",
      "[HUGECTR][11:24:21][DEBUG][RANK0]: Destroying Kafka lifetime service.\n"
     ]
    }
   ],
   "source": [
    "!python wdl_continue.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feae14e",
   "metadata": {},
   "source": [
    "## Inference with new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bc659f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wdl_continue_predict.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 'wdl_continue_predict.py'\n",
    "from hugectr.inference import InferenceParams, CreateInferenceSession\n",
    "import hugectr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from mpi4py import MPI\n",
    "def wdl_inference(model_name='wdl', network_file='wdl.json', dense_file='wdl_dense_0.model', \\\n",
    "                  embedding_file_list=['wdl_0_sparse_model', 'wdl_1_sparse_model'], data_file='infer_data.csv',\\\n",
    "                  enable_cache=False, rocksdb_path=\"\"):\n",
    "    CATEGORICAL_COLUMNS=[\"C1_C2\",\"C3_C4\"] + [\"C\" + str(x) for x in range(1, 27)]\n",
    "    CONTINUOUS_COLUMNS=[\"I\" + str(x) for x in range(1, 14)]\n",
    "    LABEL_COLUMNS = ['label']\n",
    "    test_df=pd.read_csv(data_file,sep=',')\n",
    "    config_file = network_file\n",
    "    row_ptrs = list(range(0, 11, 2)) + list(range(0, 131))\n",
    "    dense_features =  list(test_df[CONTINUOUS_COLUMNS].values.flatten())\n",
    "    test_df[CATEGORICAL_COLUMNS].astype(np.int64)\n",
    "    embedding_columns = list((test_df[CATEGORICAL_COLUMNS]).values.flatten())\n",
    "    \n",
    "    cpuMemoryDatabase=hugectr.inference.CPUMemoryDatabaseParams(hugectr.DatabaseType_t.disabled)\n",
    "    rocksdbdatabase=hugectr.inference.PersistentDatabaseParams(hugectr.DatabaseType_t.disabled,path=\"/jershi/rocksdb/\")\n",
    "    redisdatabase=hugectr.inference.DistributedDatabaseParams(hugectr.DatabaseType_t.redis_cluster,address=\"10.31.241.169:7000,10.31.241.169:7001,10.31.241.169:7002\")\n",
    "\n",
    "    # create parameter server, embedding cache and inference session\n",
    "    inference_params = InferenceParams(model_name = model_name,\n",
    "                                max_batchsize = 64,\n",
    "                                hit_rate_threshold = 0.5,\n",
    "                                dense_model_file = dense_file,\n",
    "                                sparse_model_files = embedding_file_list,\n",
    "                                device_id = 0,\n",
    "                                use_gpu_embedding_cache = enable_cache,\n",
    "                                cache_size_percentage = 0.9,\n",
    "                                i64_input_key = True,\n",
    "                                use_mixed_precision = False,\n",
    "                                cpu_memory_db=cpuMemoryDatabase,\n",
    "                                persistent_db=rocksdbdatabase,\n",
    "                                distributed_db=redisdatabase)\n",
    "    inference_session = CreateInferenceSession(config_file, inference_params)\n",
    "    output = inference_session.predict(dense_features, embedding_columns, row_ptrs)\n",
    "    print(\"WDL multi-embedding table inference result is {}\".format(output))\n",
    "\n",
    "wdl_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8cb257cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HUGECTR][11:32:49][INFO][RANK0]: default_emb_vec_value is not specified using default: 0.000000\n",
      "[HUGECTR][11:32:49][INFO][RANK0]: default_emb_vec_value is not specified using default: 0.000000\n",
      "[HUGECTR][11:32:49][INFO][RANK0]: Creating RedisCluster backend...\n",
      "[HUGECTR][11:32:49][INFO][RANK0]: Connecting to Redis cluster via 10.31.241.169:7000 ...\n",
      "Traceback (most recent call last):\n",
      "  File \"wdl_continue_predict.py\", line 42, in <module>\n",
      "    wdl_inference()\n",
      "  File \"wdl_continue_predict.py\", line 38, in wdl_inference\n",
      "    inference_session = CreateInferenceSession(config_file, inference_params)\n",
      "RuntimeError: Failed to connect to Redis: No route to host\n"
     ]
    }
   ],
   "source": [
    "!python wdl_continue_predict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417b6b71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
