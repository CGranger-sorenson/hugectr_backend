{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5addfd4e",
   "metadata": {},
   "source": [
    "# Configuration System- Model Mangement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56050d44",
   "metadata": {},
   "source": [
    "Model management is a part of MLOps. ML models should be consistent, and meet all business requirements at scale. To make this happen, a logical, easy-to-follow policy for model management is essential. Model management is responsible for development, training, validation and deployment of ML/DNN models.\n",
    "\n",
    "* model training: Here we take care of model training, model compression, model validation, model deployment and model retraining (happens when the deployed model’s performance drops below a set threshold).\n",
    "* **model deployment** perfomance/state monitoring,  deployment strategies (A/B testing and etc), model rollback\n",
    "* **Resource scheduling** Support allocation and task management based on real-time resource status of heterogeneous platforms during training or inference\n",
    "* **Experiments**: Here we take of logging training metrics, loss, images, text or any other metadata you might have as well as code, data & pipeline versioning, \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9ae546",
   "metadata": {},
   "source": [
    "# Model Management workflow for deployment:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2416204f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Model Registry**:model registry, version management，elastic deployment and multi-model hybrid deployment.  \n",
    "\n",
    "**Data Versioning**: version control systems help developers manage changes to source code. While data version control is a set of tools and processes that tries to adapt the version control process to the data world to manage the changes of models in relationship to datasets and vice-versa.  \n",
    "\n",
    "**Traffic Management**: routing management，load balancing management  \n",
    "\n",
    "**Model Monitoring** The collected metrics are written and reported to stream platform, monitored b Prom and indicator query,It is used to track the models inference performance and identify any signs of Serving Skew which is when data changes cause the deployed model performance to degrade below the score/accuracy it displayed in the training environment. \n",
    "\n",
    "**Experiment Tracker**: It is used for collecting, organizing, and tracking model training/validation information/performance across multiple runs with different configurations (lr, epochs, optimizers, loss, batch size and so on) and datasets (train/val splits and transforms)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01264589",
   "metadata": {},
   "source": [
    "# Workflow for deploying a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62942c31",
   "metadata": {},
   "source": [
    "* Register the model.\n",
    "* Define deployment configuration\n",
    "* Define inference configuration.\n",
    "* Deploy your machine learning model.\n",
    "* Update model\n",
    "* Test the resulting web service.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a5bdc5",
   "metadata": {},
   "source": [
    "## Register the model\n",
    "\n",
    "When you register a model, make sure training side uploads the model to the distributed file system (or cloud) \n",
    "\n",
    "The following examples demonstrate how to register a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c0a321",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hps.cm import cm\n",
    "\n",
    "cm.initialize(server=\"localhost\",port=9008)\n",
    "\n",
    "# Set model download path\n",
    "model_path = cm.set_inference_(\"inference\",\"hdfs://model_repo/modelname\", \"modelfile\")\n",
    "\n",
    "# Register model\n",
    "model = cm.register(workspace, model_name=\"wdl\", model_path=model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461b69f0",
   "metadata": {},
   "source": [
    "## Define a deployment configuration\n",
    "A deployment configuration specifies the amount of hardware resource and deploy method(hierarchical or local) your model service needs in order to run. For example, a deployment configuration lets you specify that your service needs 2 devices, 2 CPU cores, 1 GPU core, and that you want to enable autoscaling.\n",
    "\n",
    "The options available for a deployment configuration differ depending on the compute target you choose. In a local deployment, all you can specify is which port your webservice will be served on.\n",
    "\n",
    "```json\n",
    "        {\n",
    "            \"model\":\"wdl\",\n",
    "            \"sparse_files\":[\"/model/wdl/1/0_sparse_2000.model\",\"/model/wdl/1/1_sparse_2000.model\"],\n",
    "            \"dense_file\":\"/model/wdl/1/_dense_2000.model\",\n",
    "            \"network_file\":\"/model/wdl/1/wdl_infer.json\",\n",
    "            \"num_of_worker_buffer_in_pool\": \"4\",\n",
    "            \"deployed_device_list\":[\"1\"],\n",
    "            \"max_batch_size\":\"1024\",\n",
    "            \"default_value_for_each_table\":[\"0.0\",\"0.0\"],\n",
    "            \"hit_rate_threshold\":\"0.9\",\n",
    "            \"gpucacheper\":\"0.5\",\n",
    "            \"gpucache\":\"true\"\n",
    "        }\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a9fbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hps.cm import cm\n",
    "\n",
    "# Set model deployment_config path\n",
    "deployment_config = model.deploy_configuration(\"inference\",model_name, config_file=\"./ps.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084b1331",
   "metadata": {},
   "source": [
    "## Define an inference configuration\n",
    "\n",
    "An inference configuration describes the model network to use when initializing model service. Configure resource and the model service-related parameters per model.\n",
    "\n",
    "```pbtxt\n",
    "name: \"wdl\"\n",
    "backend: \"hugectr\"\n",
    "max_batch_size:64,\n",
    "input [\n",
    "  {\n",
    "    name: \"DES\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ -1 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"CATCOLUMN\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [ -1 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"ROWINDEX\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ -1 ]\n",
    "  }\n",
    "]\n",
    "```\n",
    "\n",
    "```json\n",
    "\"layers\": [\n",
    "    {\n",
    "      \"name\": \"data\",\n",
    "      \"type\": \"Data\",\n",
    "      \"source\": \"./file_list.txt\",\n",
    "      \"eval_source\": \"./file_list_test.txt\",\n",
    "      \"check\": \"Sum\",\n",
    "      \"label\": {\n",
    "        \"top\": \"label\",\n",
    "        \"label_dim\": 1\n",
    "      },\n",
    "      \"dense\": {\n",
    "        \"top\": \"dense\",\n",
    "        \"dense_dim\": 13\n",
    "      },\n",
    "      \"sparse\": [\n",
    "        {\n",
    "          \"top\": \"data1\",\n",
    "          \"slot_num\": 26,\n",
    "          \"is_fixed_length\": false,\n",
    "          \"nnz_per_slot\": 1\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"sparse_embedding1\",\n",
    "      \"type\": \"LocalizedSlotSparseEmbeddingHash\",\n",
    "      \"bottom\": \"data1\",\n",
    "      \"top\": \"sparse_embedding1\",\n",
    "      \"sparse_embedding_hparam\": {\n",
    "        \"embedding_vec_size\": 128,\n",
    "        \"combiner\": \"sum\",\n",
    "        \"workspace_size_per_gpu_in_mb\": 2547\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"fc1\",\n",
    "      \"type\": \"InnerProduct\",\n",
    "      \"bottom\": \"dense\",\n",
    "      \"top\": \"fc1\",\n",
    "      \"fc_param\": {\n",
    "        \"num_output\": 512\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"relu1\",\n",
    "      \"type\": \"ReLU\",\n",
    "      \"bottom\": \"fc1\",\n",
    "      \"top\": \"relu1\"\n",
    "    },\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c97f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from hps.cm import cm\n",
    "\n",
    "inference_config = model.InferenceConfig(\n",
    "    modelname=wdl,\n",
    "    model_network=\"./wdl_network.json\",\n",
    "    running_config = \"./config.pbtxtx\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e061ba9",
   "metadata": {},
   "source": [
    "## Deploy model\n",
    "Lanch Triton Server and load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5e463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonhttpclient\n",
    "\n",
    "try:\n",
    "    triton_client = tritonhttpclient.InferenceServerClient(url=\"localhost:8000\", verbose=True)\n",
    "    print(\"client created.\")\n",
    "except Exception as e:\n",
    "    print(\"channel creation failed: \" + str(e))\n",
    "    \n",
    "triton_client.load_model(model_name=\"wdl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25b211f",
   "metadata": {},
   "source": [
    "## Update Model\n",
    "\n",
    "CM is capable of supporting the Model Repository Extension, such as Triton's Model Repository Extension allows you to query and control model repositories that are being served by Triton.\n",
    "\n",
    "* **Depoly new models online**: Follow the step as described above, CM will load not only the network dense weight as part of the HugeCTR model, but inserting the embedding table of new models to Hierarchical Inference Parameter Server and creating the embedding cache based on model definition in Independent Parameter Server Configuration, which means the Parameter server will independently provide an initialization mechanism for the new embedding table and embedding cache of new models.\n",
    "\n",
    "\n",
    "* **Update the deployed model online**: Just reset the inference and deployment configuration, CM will load the network dense weight as part of the HugeCTR model and updating the embedding tables of the latest model file to Inference Hierarchical Parameter Server and refreshing the embedding cache, which means the Parameter server will independently provide an updated mechanism for existing embedding tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6016286a",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_config = model.deploy_configuration(\"inference\",model_name, config_file=\"./ps.json\")\n",
    "\n",
    "inference_config = model.InferenceConfig(\n",
    "    modelname=wdl,\n",
    "    model_network=\"./wdl_network.json\",\n",
    "    running_config = \"./config.pbtxtx\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd1cee7",
   "metadata": {},
   "source": [
    "## Recycle Model\n",
    "\n",
    "* **Recycle old models**: Delete the inference and deployment configuration, CM will recycle the HugeCTR model network's weights  from Triton and release the corresponding embedded cache from devices, which means the embedding tables corresponding to the model will still remain in the Inference Hierarchical Parameter Server Database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7943657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "service.delete()\n",
    "model.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ae5c7f",
   "metadata": {},
   "source": [
    "# Swagger UI\n",
    "\n",
    "Users can configure specific configuration items through a UI to trigger changes to the model service in inference cluster. At the same time, these APIs ensure the service security of through Ouath authentication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d163a3",
   "metadata": {},
   "source": [
    "### Post API for set inference configuration "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef2011e",
   "metadata": {},
   "source": [
    "![POST API](./post_inference.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613e75ce",
   "metadata": {},
   "source": [
    "### Configuration System UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5bb518",
   "metadata": {},
   "source": [
    "![CM](./CM.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c95f5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
