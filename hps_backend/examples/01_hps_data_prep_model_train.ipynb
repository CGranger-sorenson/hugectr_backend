{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee445715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HCTR][05:04:36.012][INFO][RK0][main]: Generate Parquet dataset\n",
      "[HCTR][05:04:36.012][INFO][RK0][main]: train data folder: ./data_parquet, eval data folder: ./data_parquet, slot_size_array: 10000, 10000, 10000, 10000, nnz array: 1, 1, 1, 1, #files for train: 16, #files for eval: 4, #samples per file: 40960, Use power law distribution: 1, alpha of power law: 1.3\n",
      "[HCTR][05:04:36.012][INFO][RK0][main]: ./data_parquet exist\n",
      "[HCTR][05:04:36.012][INFO][RK0][main]: ./data_parquet exist\n",
      "[HCTR][05:04:36.012][INFO][RK0][main]: ./data_parquet/train exist\n",
      "[HCTR][05:04:36.012][INFO][RK0][main]: ./data_parquet/train/gen_0.parquet\n",
      "[HCTR][05:04:37.520][INFO][RK0][main]: ./data_parquet/train/gen_1.parquet\n",
      "[HCTR][05:04:37.638][INFO][RK0][main]: ./data_parquet/train/gen_2.parquet\n",
      "[HCTR][05:04:37.769][INFO][RK0][main]: ./data_parquet/train/gen_3.parquet\n",
      "[HCTR][05:04:37.931][INFO][RK0][main]: ./data_parquet/train/gen_4.parquet\n",
      "[HCTR][05:04:38.059][INFO][RK0][main]: ./data_parquet/train/gen_5.parquet\n",
      "[HCTR][05:04:38.191][INFO][RK0][main]: ./data_parquet/train/gen_6.parquet\n",
      "[HCTR][05:04:38.325][INFO][RK0][main]: ./data_parquet/train/gen_7.parquet\n",
      "[HCTR][05:04:38.446][INFO][RK0][main]: ./data_parquet/train/gen_8.parquet\n",
      "[HCTR][05:04:38.573][INFO][RK0][main]: ./data_parquet/train/gen_9.parquet\n",
      "[HCTR][05:04:38.709][INFO][RK0][main]: ./data_parquet/train/gen_10.parquet\n",
      "[HCTR][05:04:38.862][INFO][RK0][main]: ./data_parquet/train/gen_11.parquet\n",
      "[HCTR][05:04:38.980][INFO][RK0][main]: ./data_parquet/train/gen_12.parquet\n",
      "[HCTR][05:04:39.094][INFO][RK0][main]: ./data_parquet/train/gen_13.parquet\n",
      "[HCTR][05:04:39.228][INFO][RK0][main]: ./data_parquet/train/gen_14.parquet\n",
      "[HCTR][05:04:39.350][INFO][RK0][main]: ./data_parquet/train/gen_15.parquet\n",
      "[HCTR][05:04:39.474][INFO][RK0][main]: ./data_parquet/file_list.txt done!\n",
      "[HCTR][05:04:39.475][INFO][RK0][main]: ./data_parquet/val exist\n",
      "[HCTR][05:04:39.475][INFO][RK0][main]: ./data_parquet/val/gen_0.parquet\n",
      "[HCTR][05:04:39.590][INFO][RK0][main]: ./data_parquet/val/gen_1.parquet\n",
      "[HCTR][05:04:39.726][INFO][RK0][main]: ./data_parquet/val/gen_2.parquet\n",
      "[HCTR][05:04:39.875][INFO][RK0][main]: ./data_parquet/val/gen_3.parquet\n",
      "[HCTR][05:04:40.001][INFO][RK0][main]: ./data_parquet/file_list_test.txt done!\n"
     ]
    }
   ],
   "source": [
    "import hugectr\n",
    "from hugectr.tools import DataGeneratorParams, DataGenerator\n",
    "\n",
    "data_generator_params = DataGeneratorParams(\n",
    "  format = hugectr.DataReaderType_t.Parquet,\n",
    "  label_dim = 1,\n",
    "  dense_dim = 10,\n",
    "  num_slot = 4,\n",
    "  i64_input_key = True,\n",
    "  nnz_array = [1, 1, 1, 1],\n",
    "  source = \"./data_parquet/file_list.txt\",\n",
    "  eval_source = \"./data_parquet/file_list_test.txt\",\n",
    "  slot_size_array = [10000, 10000, 10000, 10000],\n",
    "  check_type = hugectr.Check_t.Non,\n",
    "  dist_type = hugectr.Distribution_t.PowerLaw,\n",
    "  power_law_type = hugectr.PowerLaw_t.Short,\n",
    "  num_files = 16,\n",
    "  eval_num_files = 4,\n",
    "  num_samples_per_file = 40960)\n",
    "data_generator = DataGenerator(data_generator_params)\n",
    "data_generator.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ada73016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/merlin/hugectr_inference_backend/hps_backend/examples\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72c2dda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir hps_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188016f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdad8404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting fix_meta_json_path.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile fix_meta_json_path.py\n",
    "\n",
    "import json\n",
    "file_path_train = './data_parquet/train/_metadata.json'\n",
    "file_path_val   = './data_parquet/val/_metadata.json'\n",
    "def fix_meta_json_path(file_path):\n",
    "    with open(file_path) as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    for item in data['file_stats']:\n",
    "        item['file_name'] = \"gen_{}\".format(item['file_name'])\n",
    "        print(item)\n",
    "    \n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "fix_meta_json_path(file_path_train)\n",
    "fix_meta_json_path(file_path_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "081e32f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file_name': 'gen_0.parquet', 'num_rows': 40960}\n",
      "{'file_name': 'gen_1.parquet', 'num_rows': 40960}\n",
      "{'file_name': 'gen_2.parquet', 'num_rows': 40960}\n",
      "{'file_name': 'gen_3.parquet', 'num_rows': 40960}\n",
      "{'file_name': 'gen_4.parquet', 'num_rows': 40960}\n",
      "{'file_name': 'gen_5.parquet', 'num_rows': 40960}\n",
      "{'file_name': 'gen_6.parquet', 'num_rows': 40960}\n",
      "{'file_name': 'gen_7.parquet', 'num_rows': 40960}\n",
      "{'file_name': 'gen_8.parquet', 'num_rows': 40960}\n",
      "{'file_name': 'gen_9.parquet', 'num_rows': 40960}\n",
      "{'file_name': 'gen_10.parquet', 'num_rows': 40960}\n",
      "{'file_name': 'gen_11.parquet', 'num_rows': 40960}\n",
      "{'file_name': 'gen_12.parquet', 'num_rows': 40960}\n",
      "{'file_name': 'gen_13.parquet', 'num_rows': 40960}\n",
      "{'file_name': 'gen_14.parquet', 'num_rows': 40960}\n",
      "{'file_name': 'gen_15.parquet', 'num_rows': 40960}\n",
      "{'file_name': 'gen_0.parquet', 'num_rows': 40960}\n",
      "{'file_name': 'gen_1.parquet', 'num_rows': 40960}\n",
      "{'file_name': 'gen_2.parquet', 'num_rows': 40960}\n",
      "{'file_name': 'gen_3.parquet', 'num_rows': 40960}\n"
     ]
    }
   ],
   "source": [
    "!python3 fix_meta_json_path.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829fcd0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a81cdf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c4354bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_col0</th>\n",
       "      <th>_col1</th>\n",
       "      <th>_col2</th>\n",
       "      <th>_col3</th>\n",
       "      <th>_col4</th>\n",
       "      <th>_col5</th>\n",
       "      <th>_col6</th>\n",
       "      <th>_col7</th>\n",
       "      <th>_col8</th>\n",
       "      <th>_col9</th>\n",
       "      <th>_col10</th>\n",
       "      <th>_col11</th>\n",
       "      <th>_col12</th>\n",
       "      <th>_col13</th>\n",
       "      <th>_col14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.814841</td>\n",
       "      <td>0.520009</td>\n",
       "      <td>0.797189</td>\n",
       "      <td>0.222827</td>\n",
       "      <td>0.078717</td>\n",
       "      <td>0.614582</td>\n",
       "      <td>0.395775</td>\n",
       "      <td>0.567154</td>\n",
       "      <td>0.466539</td>\n",
       "      <td>0.775148</td>\n",
       "      <td>0.139989</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.619443</td>\n",
       "      <td>0.167564</td>\n",
       "      <td>0.716365</td>\n",
       "      <td>0.586061</td>\n",
       "      <td>0.908427</td>\n",
       "      <td>0.058091</td>\n",
       "      <td>0.241871</td>\n",
       "      <td>0.683993</td>\n",
       "      <td>0.159354</td>\n",
       "      <td>0.260729</td>\n",
       "      <td>0.820689</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.554788</td>\n",
       "      <td>0.128592</td>\n",
       "      <td>0.356470</td>\n",
       "      <td>0.681377</td>\n",
       "      <td>0.321617</td>\n",
       "      <td>0.098580</td>\n",
       "      <td>0.840157</td>\n",
       "      <td>0.391385</td>\n",
       "      <td>0.724311</td>\n",
       "      <td>0.544364</td>\n",
       "      <td>2088</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.756215</td>\n",
       "      <td>0.546706</td>\n",
       "      <td>0.642247</td>\n",
       "      <td>0.640887</td>\n",
       "      <td>0.630262</td>\n",
       "      <td>0.664023</td>\n",
       "      <td>0.061477</td>\n",
       "      <td>0.012776</td>\n",
       "      <td>0.363288</td>\n",
       "      <td>0.551708</td>\n",
       "      <td>0.330753</td>\n",
       "      <td>0</td>\n",
       "      <td>9165</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.604962</td>\n",
       "      <td>0.065089</td>\n",
       "      <td>0.626631</td>\n",
       "      <td>0.364093</td>\n",
       "      <td>0.446031</td>\n",
       "      <td>0.244077</td>\n",
       "      <td>0.310744</td>\n",
       "      <td>0.514325</td>\n",
       "      <td>0.320860</td>\n",
       "      <td>0.329368</td>\n",
       "      <td>0.519373</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      _col0     _col1     _col2     _col3     _col4     _col5     _col6  \\\n",
       "0  0.814841  0.520009  0.797189  0.222827  0.078717  0.614582  0.395775   \n",
       "1  0.619443  0.167564  0.716365  0.586061  0.908427  0.058091  0.241871   \n",
       "2  0.027290  0.554788  0.128592  0.356470  0.681377  0.321617  0.098580   \n",
       "3  0.756215  0.546706  0.642247  0.640887  0.630262  0.664023  0.061477   \n",
       "4  0.604962  0.065089  0.626631  0.364093  0.446031  0.244077  0.310744   \n",
       "\n",
       "      _col7     _col8     _col9    _col10  _col11  _col12  _col13  _col14  \n",
       "0  0.567154  0.466539  0.775148  0.139989       9       1       2      52  \n",
       "1  0.683993  0.159354  0.260729  0.820689       1       7       0      87  \n",
       "2  0.840157  0.391385  0.724311  0.544364    2088       1       2       2  \n",
       "3  0.012776  0.363288  0.551708  0.330753       0    9165       0       3  \n",
       "4  0.514325  0.320860  0.329368  0.519373       1      39      39       2  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(\"./data_parquet/train/gen_0.parquet\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da1ab77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7b9c3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "import re\n",
    "import shutil\n",
    "import glob\n",
    "import warnings\n",
    "\n",
    "BASE_DIR = \"/hps_demo\"\n",
    "embedding_folder  = os.path.join(BASE_DIR, \"embedding\")\n",
    "wdl_embedding_repo= os.path.join(embedding_folder, \"hps_infer\")\n",
    "wdl_version =os.path.join(wdl_embedding_repo, \"1\")\n",
    "\n",
    "if os.path.isdir(embedding_folder):\n",
    "    shutil.rmtree(embedding_folder)\n",
    "os.makedirs(embedding_folder)\n",
    "\n",
    "if os.path.isdir(wdl_embedding_repo):\n",
    "    shutil.rmtree(wdl_embedding_repo)\n",
    "os.makedirs(wdl_embedding_repo)\n",
    "\n",
    "if os.path.isdir(wdl_version):\n",
    "    shutil.rmtree(wdl_version)\n",
    "os.makedirs(wdl_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12faf315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/hps_demo\u001b[00m\n",
      "└── \u001b[01;34membedding\u001b[00m\n",
      "    └── \u001b[01;34mhps_infer\u001b[00m\n",
      "        └── \u001b[01;34m1\u001b[00m\n",
      "\n",
      "3 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "!tree -l $BASE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58cf28a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d79e003",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93041d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hps_model_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hps_model_train.py\n",
    "\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "\n",
    "## typical DLRM architecture building\n",
    "## Bottom layer: bottom MLP layer for dense features(10) + embedding layer for sparse features(2+2)\n",
    "## Middle layer: concatenate 3 blocks\n",
    "## Top layer: top MLP layer to fully connect all inputs (FC twice + RELU + BinaryCrossEntropy)\n",
    "\n",
    "# construct model\n",
    "solver = hugectr.CreateSolver(model_name = \"hps_train\",\n",
    "                              max_eval_batches = 1,\n",
    "                              batchsize_eval = 1024,\n",
    "                              batchsize = 1024,\n",
    "                              lr = 0.001,\n",
    "                              vvgpu = [[0]],\n",
    "                              i64_input_key = True,\n",
    "                              repeat_dataset = True,\n",
    "                              use_cuda_graph = True)\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Parquet,\n",
    "                                  source = [\"./data_parquet/file_list.txt\"],\n",
    "                                  eval_source = \"./data_parquet/file_list_test.txt\",\n",
    "                                  check_type = hugectr.Check_t.Non,\n",
    "                                  slot_size_array = [10000, 10000, 10000, 10000])\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.Adam)\n",
    "model = hugectr.Model(solver, reader, optimizer)\n",
    "\n",
    "# model NN\n",
    "\n",
    "# https://nvidia-merlin.github.io/HugeCTR/master/api/python_interface.html?highlight=model#input-layer\n",
    "# check for \"data_reader_sparse_param_array\" parameter, 4 sparse feature in this case\n",
    "# assigned 2 sparse feat for slot1, 2 sparse feat for slot2\n",
    "model.add(hugectr.Input(label_dim = 1, label_name = \"label\",\n",
    "                        dense_dim = 10, dense_name = \"dense\",\n",
    "                        data_reader_sparse_param_array = \n",
    "                        [hugectr.DataReaderSparseParam(\"data1\", [1, 1], True, 2),\n",
    "                        hugectr.DataReaderSparseParam(\"data2\", [1, 1], True, 2)]))\n",
    "\n",
    "# sparse layer for categorical features\n",
    "# https://nvidia-merlin.github.io/HugeCTR/master/api/python_interface.html?highlight=model#sparseembedding\n",
    "# sparse layer should be defined after Input layer, but before Dense layer\n",
    "# for embedding_type, check https://nvidia-merlin.github.io/HugeCTR/master/api/hugectr_layer_book.html#embedding-types-detail\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash, \n",
    "                            workspace_size_per_gpu_in_mb = 4,\n",
    "                            embedding_vec_size = 16,\n",
    "                            combiner = \"sum\",\n",
    "                            sparse_embedding_name = \"sparse_embedding1\",\n",
    "                            bottom_name = \"data1\",\n",
    "                            optimizer = optimizer))\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash, \n",
    "                            workspace_size_per_gpu_in_mb = 8,\n",
    "                            embedding_vec_size = 32,\n",
    "                            combiner = \"sum\",\n",
    "                            sparse_embedding_name = \"sparse_embedding2\",\n",
    "                            bottom_name = \"data2\",\n",
    "                            optimizer = optimizer))\n",
    "# reshape\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding1\"],\n",
    "                            top_names = [\"reshape1\"],\n",
    "                            leading_dim=32))                            \n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding2\"],\n",
    "                            top_names = [\"reshape2\"],\n",
    "                            leading_dim=64))\n",
    "\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Concat,\n",
    "                            bottom_names = [\"reshape1\", \"reshape2\", \"dense\"], top_names = [\"concat1\"]))\n",
    "\n",
    "# FC layer + ReLU + FC + binary cross entropy\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"concat1\"],\n",
    "                            top_names = [\"fc1\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc1\"],\n",
    "                            top_names = [\"relu1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"relu1\"],\n",
    "                            top_names = [\"fc2\"],\n",
    "                            num_output=1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "                            bottom_names = [\"fc2\", \"label\"],\n",
    "                            top_names = [\"loss\"]))\n",
    "\n",
    "# model compile\n",
    "model.compile()\n",
    "model.summary()\n",
    "model.graph_to_json(\"./hps_model/hps_train.json\")\n",
    "model.fit(max_iter = 1100, display = 200, eval_interval = 1000, snapshot = 1000, snapshot_prefix = \"./hps_model/hps_train\")\n",
    "model.export_predictions(\"./hps_model/hps_train_pred_\" + str(1000), \"./hps_model/hps_train_label_\" + str(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ba5a706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HugeCTR Version: 3.6\n",
      "====================================================Model Init=====================================================\n",
      "[HCTR][05:49:03.277][INFO][RK0][main]: Initialize model: hps_train\n",
      "[HCTR][05:49:03.277][WARNING][RK0][main]: MPI was already initialized somewhere elese. Lifetime service disabled.\n",
      "[HCTR][05:49:03.277][INFO][RK0][main]: Global seed is 4101819403\n",
      "[HCTR][05:49:03.443][INFO][RK0][main]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 3\n",
      "[HCTR][05:49:04.896][WARNING][RK0][main]: Peer-to-peer access cannot be fully enabled.\n",
      "[HCTR][05:49:04.896][INFO][RK0][main]: Start all2all warmup\n",
      "[HCTR][05:49:04.898][INFO][RK0][main]: End all2all warmup\n",
      "[HCTR][05:49:04.899][INFO][RK0][main]: Using All-reduce algorithm: NCCL\n",
      "[HCTR][05:49:04.900][INFO][RK0][main]: Device 0: NVIDIA A100-SXM4-80GB\n",
      "[HCTR][05:49:04.900][INFO][RK0][main]: num of DataReader workers: 1\n",
      "[HCTR][05:49:04.901][INFO][RK0][main]: Vocabulary size: 40000\n",
      "[HCTR][05:49:04.901][INFO][RK0][main]: max_vocabulary_size_per_gpu_=21845\n",
      "[HCTR][05:49:04.902][DEBUG][RK0][tid #140382482462464]: file_name_ ./data_parquet/train/gen_0.parquet file_total_rows_ 40960\n",
      "[HCTR][05:49:04.903][DEBUG][RK0][tid #140382461163264]: file_name_ ./data_parquet/val/gen_0.parquet file_total_rows_ 40960\n",
      "[HCTR][05:49:04.905][INFO][RK0][main]: max_vocabulary_size_per_gpu_=21845\n",
      "[HCTR][05:49:04.914][INFO][RK0][main]: Graph analysis to resolve tensor dependency\n",
      "===================================================Model Compile===================================================\n",
      "[HCTR][05:49:06.147][INFO][RK0][main]: gpu0 start to init embedding\n",
      "[HCTR][05:49:06.147][INFO][RK0][main]: gpu0 init embedding done\n",
      "[HCTR][05:49:06.147][INFO][RK0][main]: gpu0 start to init embedding\n",
      "[HCTR][05:49:06.147][INFO][RK0][main]: gpu0 init embedding done\n",
      "[HCTR][05:49:06.147][INFO][RK0][main]: AUC Init batch_size_per_gpu:1024 n_batches:1 num_classes:1\n",
      "num_local_gpus:1 num_global_gpus:1 num_bins:10000 num_partitions:1\n",
      "[HCTR][05:49:06.147][INFO][RK0][main]: Starting AUC NCCL warm-up\n",
      "[HCTR][05:49:06.148][INFO][RK0][main]: Warm-up done\n",
      "===================================================Model Summary===================================================\n",
      "[HCTR][05:49:06.148][INFO][RK0][main]: label                                   Dense                         Sparse                        \n",
      "label                                   dense                          data1,data2                   \n",
      "(None, 1)                               (None, 10)                              \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "DistributedSlotSparseEmbeddingHash      data1                         sparse_embedding1             (None, 2, 16)                 \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "DistributedSlotSparseEmbeddingHash      data2                         sparse_embedding2             (None, 2, 32)                 \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Reshape                                 sparse_embedding1             reshape1                      (None, 32)                    \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Reshape                                 sparse_embedding2             reshape2                      (None, 64)                    \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Concat                                  reshape1                      concat1                       (None, 106)                   \n",
      "                                        reshape2                                                                                  \n",
      "                                        dense                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            concat1                       fc1                           (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc1                           relu1                         (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            relu1                         fc2                           (None, 1)                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "BinaryCrossEntropyLoss                  fc2                           loss                                                        \n",
      "                                        label                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "[HCTR][05:49:06.148][INFO][RK0][main]: Save the model graph to ./hps_model/hps_train.json successfully\n",
      "=====================================================Model Fit=====================================================\n",
      "[HCTR][05:49:06.148][INFO][RK0][main]: Use non-epoch mode with number of iterations: 1100\n",
      "[HCTR][05:49:06.148][INFO][RK0][main]: Training batchsize: 1024, evaluation batchsize: 1024\n",
      "[HCTR][05:49:06.148][INFO][RK0][main]: Evaluation interval: 1000, snapshot interval: 1000\n",
      "[HCTR][05:49:06.148][INFO][RK0][main]: Dense network trainable: True\n",
      "[HCTR][05:49:06.148][INFO][RK0][main]: Sparse embedding sparse_embedding1 trainable: True\n",
      "[HCTR][05:49:06.148][INFO][RK0][main]: Sparse embedding sparse_embedding2 trainable: True\n",
      "[HCTR][05:49:06.148][INFO][RK0][main]: Use mixed precision: False, scaler: 1.000000, use cuda graph: True\n",
      "[HCTR][05:49:06.148][INFO][RK0][main]: lr: 0.001000, warmup_steps: 1, end_lr: 0.000000\n",
      "[HCTR][05:49:06.148][INFO][RK0][main]: decay_start: 0, decay_steps: 1, decay_power: 2.000000\n",
      "[HCTR][05:49:06.148][INFO][RK0][main]: Training source file: ./data_parquet/file_list.txt\n",
      "[HCTR][05:49:06.148][INFO][RK0][main]: Evaluation source file: ./data_parquet/file_list_test.txt\n",
      "[HCTR][05:49:06.179][DEBUG][RK0][tid #140382482462464]: file_name_ ./data_parquet/train/gen_1.parquet file_total_rows_ 40960\n",
      "[HCTR][05:49:06.212][DEBUG][RK0][tid #140382482462464]: file_name_ ./data_parquet/train/gen_2.parquet file_total_rows_ 40960\n",
      "[HCTR][05:49:06.246][DEBUG][RK0][tid #140382482462464]: file_name_ ./data_parquet/train/gen_3.parquet file_total_rows_ 40960\n",
      "[HCTR][05:49:06.280][DEBUG][RK0][tid #140382482462464]: file_name_ ./data_parquet/train/gen_4.parquet file_total_rows_ 40960\n",
      "[HCTR][05:49:06.313][DEBUG][RK0][tid #140382482462464]: file_name_ ./data_parquet/train/gen_5.parquet file_total_rows_ 40960\n",
      "[HCTR][05:49:06.319][INFO][RK0][main]: Iter: 200 Time(200 iters): 0.170227s Loss: 0.69402 lr:0.001\n",
      "[HCTR][05:49:06.347][DEBUG][RK0][tid #140382482462464]: file_name_ ./data_parquet/train/gen_6.parquet file_total_rows_ 40960\n",
      "[HCTR][05:49:06.381][DEBUG][RK0][tid #140382482462464]: file_name_ ./data_parquet/train/gen_7.parquet file_total_rows_ 40960\n",
      "[HCTR][05:49:06.415][DEBUG][RK0][tid #140382482462464]: file_name_ ./data_parquet/train/gen_8.parquet file_total_rows_ 40960\n",
      "[HCTR][05:49:06.448][DEBUG][RK0][tid #140382482462464]: file_name_ ./data_parquet/train/gen_9.parquet file_total_rows_ 40960\n",
      "[HCTR][05:49:06.482][DEBUG][RK0][tid #140382482462464]: file_name_ ./data_parquet/train/gen_10.parquet file_total_rows_ 40960\n",
      "[HCTR][05:49:06.488][INFO][RK0][main]: Iter: 400 Time(200 iters): 0.169111s Loss: 0.693437 lr:0.001\n",
      "[HCTR][05:49:06.516][DEBUG][RK0][tid #140382482462464]: file_name_ ./data_parquet/train/gen_11.parquet file_total_rows_ 40960\n",
      "[HCTR][05:49:06.551][DEBUG][RK0][tid #140382482462464]: file_name_ ./data_parquet/train/gen_12.parquet file_total_rows_ 40960\n",
      "[HCTR][05:49:06.585][DEBUG][RK0][tid #140382482462464]: file_name_ ./data_parquet/train/gen_13.parquet file_total_rows_ 40960\n",
      "[HCTR][05:49:06.619][DEBUG][RK0][tid #140382482462464]: file_name_ ./data_parquet/train/gen_14.parquet file_total_rows_ 40960\n",
      "[HCTR][05:49:06.653][DEBUG][RK0][tid #140382482462464]: file_name_ ./data_parquet/train/gen_15.parquet file_total_rows_ 40960\n",
      "[HCTR][05:49:06.658][INFO][RK0][main]: Iter: 600 Time(200 iters): 0.170388s Loss: 0.692773 lr:0.001\n",
      "[HCTR][05:49:06.687][DEBUG][RK0][tid #140382482462464]: file_name_ ./data_parquet/train/gen_0.parquet file_total_rows_ 40960\n",
      "[HCTR][05:49:06.721][DEBUG][RK0][tid #140382482462464]: file_name_ ./data_parquet/train/gen_1.parquet file_total_rows_ 40960\n",
      "[HCTR][05:49:06.756][DEBUG][RK0][tid #140382482462464]: file_name_ ./data_parquet/train/gen_2.parquet file_total_rows_ 40960\n",
      "[HCTR][05:49:06.790][DEBUG][RK0][tid #140382482462464]: file_name_ ./data_parquet/train/gen_3.parquet file_total_rows_ 40960\n",
      "[HCTR][05:49:06.824][DEBUG][RK0][tid #140382482462464]: file_name_ ./data_parquet/train/gen_4.parquet file_total_rows_ 40960\n",
      "[HCTR][05:49:06.829][INFO][RK0][main]: Iter: 800 Time(200 iters): 0.170977s Loss: 0.692469 lr:0.001\n",
      "[HCTR][05:49:06.858][DEBUG][RK0][tid #140382482462464]: file_name_ ./data_parquet/train/gen_5.parquet file_total_rows_ 40960\n",
      "[HCTR][05:49:06.892][DEBUG][RK0][tid #140382482462464]: file_name_ ./data_parquet/train/gen_6.parquet file_total_rows_ 40960\n",
      "[HCTR][05:49:06.927][DEBUG][RK0][tid #140382482462464]: file_name_ ./data_parquet/train/gen_7.parquet file_total_rows_ 40960\n",
      "[HCTR][05:49:06.961][DEBUG][RK0][tid #140382482462464]: file_name_ ./data_parquet/train/gen_8.parquet file_total_rows_ 40960\n",
      "[HCTR][05:49:06.995][DEBUG][RK0][tid #140382482462464]: file_name_ ./data_parquet/train/gen_9.parquet file_total_rows_ 40960\n",
      "[HCTR][05:49:07.000][INFO][RK0][main]: Iter: 1000 Time(200 iters): 0.170989s Loss: 0.692426 lr:0.001\n",
      "[HCTR][05:49:07.002][INFO][RK0][main]: Evaluation, AUC: 0.493831\n",
      "[HCTR][05:49:07.002][INFO][RK0][main]: Eval Time for 1 iters: 0.000593s\n",
      "[HCTR][05:49:07.005][INFO][RK0][main]: Rank0: Write hash table to file\n",
      "[HCTR][05:49:07.008][INFO][RK0][main]: Rank0: Write hash table to file\n",
      "[HCTR][05:49:07.010][INFO][RK0][main]: Dumping sparse weights to files, successful\n",
      "[HCTR][05:49:07.010][INFO][RK0][main]: Rank0: Write optimzer state to file\n",
      "[HCTR][05:49:07.011][INFO][RK0][main]: Done\n",
      "[HCTR][05:49:07.011][INFO][RK0][main]: Rank0: Write optimzer state to file\n",
      "[HCTR][05:49:07.012][INFO][RK0][main]: Done\n",
      "[HCTR][05:49:07.012][INFO][RK0][main]: Rank0: Write optimzer state to file\n",
      "[HCTR][05:49:07.014][INFO][RK0][main]: Done\n",
      "[HCTR][05:49:07.014][INFO][RK0][main]: Rank0: Write optimzer state to file\n",
      "[HCTR][05:49:07.016][INFO][RK0][main]: Done\n",
      "[HCTR][05:49:07.016][INFO][RK0][main]: Dumping sparse optimzer states to files, successful\n",
      "[HCTR][05:49:07.016][INFO][RK0][main]: Dumping dense weights to file, successful\n",
      "[HCTR][05:49:07.017][INFO][RK0][main]: Dumping dense optimizer states to file, successful\n",
      "[HCTR][05:49:07.047][DEBUG][RK0][tid #140382482462464]: file_name_ ./data_parquet/train/gen_10.parquet file_total_rows_ 40960\n",
      "[HCTR][05:49:07.082][DEBUG][RK0][tid #140382482462464]: file_name_ ./data_parquet/train/gen_11.parquet file_total_rows_ 40960\n",
      "[HCTR][05:49:07.103][INFO][RK0][main]: Finish 1100 iterations with batchsize: 1024 in 0.95s.\n"
     ]
    }
   ],
   "source": [
    "!python3 hps_model_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2b766c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/hps_demo\u001b[00m\n",
      "└── \u001b[01;34membedding\u001b[00m\n",
      "    └── \u001b[01;34mhps_infer\u001b[00m\n",
      "        └── \u001b[01;34m1\u001b[00m\n",
      "\n",
      "3 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "!tree -l $BASE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13e46283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mhps_model\u001b[00m\n",
      "├── hps_train.json\n",
      "├── hps_train0_opt_sparse_1000.model\n",
      "├── \u001b[01;34mhps_train0_sparse_1000.model\u001b[00m\n",
      "│   ├── emb_vector\n",
      "│   └── key\n",
      "├── hps_train1_opt_sparse_1000.model\n",
      "├── \u001b[01;34mhps_train1_sparse_1000.model\u001b[00m\n",
      "│   ├── emb_vector\n",
      "│   └── key\n",
      "├── hps_train_dense_1000.model\n",
      "├── hps_train_label_1000\n",
      "├── hps_train_opt_dense_1000.model\n",
      "└── hps_train_pred_1000\n",
      "\n",
      "2 directories, 11 files\n"
     ]
    }
   ],
   "source": [
    "!tree -l hps_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d078e405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/hps_demo\u001b[00m\n",
      "└── \u001b[01;34membedding\u001b[00m\n",
      "    └── \u001b[01;34mhps_infer\u001b[00m\n",
      "        └── \u001b[01;34m1\u001b[00m\n",
      "            ├── \u001b[01;34mhps_train0_sparse_1000.model\u001b[00m\n",
      "            │   ├── emb_vector\n",
      "            │   └── key\n",
      "            └── \u001b[01;34mhps_train1_sparse_1000.model\u001b[00m\n",
      "                ├── emb_vector\n",
      "                └── key\n",
      "\n",
      "5 directories, 4 files\n"
     ]
    }
   ],
   "source": [
    "!cp -r ./hps_model/hps_train0_sparse_1000.model /hps_demo/embedding/hps_infer/1\n",
    "!cp -r ./hps_model/hps_train1_sparse_1000.model /hps_demo/embedding/hps_infer/1\n",
    "!tree -l /hps_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21befd9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15956bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "104d6dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hps_train2predict.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hps_train2predict.py\n",
    "\n",
    "# validation\n",
    "from hugectr.inference import InferenceParams, CreateInferenceSession\n",
    "import hugectr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from mpi4py import MPI\n",
    "\n",
    "def demo_inference(model_name, network_file, dense_file, embedding_file_list, data_file,enable_cache):\n",
    "    # CATEGORICAL_COLUMNS=[\"C1_C2\",\"C3_C4\"]+[\"C\" + str(x) for x in range(1, 5)]\n",
    "    CATEGORICAL_COLUMNS=[\"C\" + str(x) for x in range(1, 5)]\n",
    "    CONTINUOUS_COLUMNS=[\"I\" + str(x) for x in range(1, 11)]\n",
    "    LABEL_COLUMNS = ['label']\n",
    "    \n",
    "    emb_size = [10000, 10000, 10000, 10000]\n",
    "    shift = np.insert(np.cumsum(emb_size), 0, 0)[:-1]\n",
    "    \n",
    "    test_df = pd.read_csv(data_file,sep=',')\n",
    "    config_file = network_file\n",
    "    \n",
    "    # row_ptrs = list(range(0,21))+list(range(0,261))\n",
    "    row_ptrs = list([0,2,4])\n",
    "    \n",
    "    dense_features =  list(test_df[CONTINUOUS_COLUMNS].values.flatten())\n",
    "    test_df[CATEGORICAL_COLUMNS].astype(np.int64)\n",
    "    embedding_columns = list((test_df[CATEGORICAL_COLUMNS]+shift).values.flatten())\n",
    "\n",
    "    # create parameter server, embedding cache and inference session\n",
    "    inference_params = InferenceParams(model_name = model_name,\n",
    "                                max_batchsize = 64,\n",
    "                                hit_rate_threshold = 0.9,\n",
    "                                dense_model_file = dense_file,\n",
    "                                sparse_model_files = embedding_file_list,\n",
    "                                device_id = 0,\n",
    "                                use_gpu_embedding_cache = enable_cache,\n",
    "                                cache_size_percentage = 0.9,\n",
    "                                i64_input_key = True,\n",
    "                                use_mixed_precision = False\n",
    "                                )\n",
    "    inference_session = CreateInferenceSession(config_file, inference_params)\n",
    "    # TODO: check VSCR example for hugectr inference\n",
    "    # https://gitlab-master.nvidia.com/dl/hugectr/hugectr_inference_backend/-/blob/main/docs/architecture.md#vcsr-example\n",
    "    output = inference_session.predict(dense_features, embedding_columns, row_ptrs)\n",
    "    print(\"HPS demo multi-embedding table inference result is {}\".format(output))\n",
    "\n",
    "def demo_lookup(model_name, network_file, dense_file, embedding_file_list, data_file,enable_cache):\n",
    "    # CATEGORICAL_COLUMNS=[\"C1_C2\",\"C3_C4\"]+[\"C\" + str(x) for x in range(1, 5)]\n",
    "    CATEGORICAL_COLUMNS=[\"C\" + str(x) for x in range(1, 5)]\n",
    "    CONTINUOUS_COLUMNS=[\"I\" + str(x) for x in range(1, 11)]\n",
    "    LABEL_COLUMNS = ['label']\n",
    "    \n",
    "    emb_size = [10000, 10000, 10000, 10000]\n",
    "    shift = np.insert(np.cumsum(emb_size), 0, 0)[:-1]\n",
    "    test_df = pd.read_csv(data_file,sep=',')\n",
    "    config_file = network_file\n",
    "    \n",
    "#     row_ptrs = list(range(0,21))+list(range(0,261))\n",
    "    row_ptrs = list([0,2,4])\n",
    "    \n",
    "    dense_features =  list(test_df[CONTINUOUS_COLUMNS].values.flatten())\n",
    "    test_df[CATEGORICAL_COLUMNS].astype(np.int64)\n",
    "    embedding_columns = list((test_df[CATEGORICAL_COLUMNS]+shift).values.flatten())\n",
    "\n",
    "    # create parameter server, embedding cache and inference session\n",
    "    inference_params = InferenceParams(model_name = model_name,\n",
    "                                max_batchsize = 64,\n",
    "                                hit_rate_threshold = 0.9,\n",
    "                                dense_model_file = dense_file,\n",
    "                                sparse_model_files = embedding_file_list,\n",
    "                                device_id = 0,\n",
    "                                use_gpu_embedding_cache = enable_cache,\n",
    "                                cache_size_percentage = 0.9,\n",
    "                                i64_input_key = True,\n",
    "                                use_mixed_precision = False\n",
    "                                )\n",
    "    inference_session = CreateInferenceSession(config_file, inference_params)\n",
    "    # TODO: check VSCR example for hugectr inference\n",
    "    # https://gitlab-master.nvidia.com/dl/hugectr/hugectr_inference_backend/-/blob/main/docs/architecture.md#vcsr-example\n",
    "    output = inference_session.predict(dense_features, embedding_columns, row_ptrs)\n",
    "    print(\"HPS demo multi-embedding table inference result is {}\".format(output))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    model_name = sys.argv[1]\n",
    "    network_file = sys.argv[2]\n",
    "    dense_file = sys.argv[3]\n",
    "    embedding_file_list = str(sys.argv[4]).split(',')\n",
    "    print(embedding_file_list)\n",
    "    data_file = sys.argv[5]\n",
    "  \n",
    "\n",
    "    #demo_inference(model_name, network_file, dense_file, embedding_file_list, data_file, True,hugectr.Database_t.Redis)\n",
    "    demo_inference(model_name, network_file, dense_file, embedding_file_list, data_file, True)\n",
    "    #demo_inference(model_name, network_file, dense_file, embedding_file_list, data_file, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344c79f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9dc8173f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare infer_test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30d24e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_col0</th>\n",
       "      <th>_col1</th>\n",
       "      <th>_col2</th>\n",
       "      <th>_col3</th>\n",
       "      <th>_col4</th>\n",
       "      <th>_col5</th>\n",
       "      <th>_col6</th>\n",
       "      <th>_col7</th>\n",
       "      <th>_col8</th>\n",
       "      <th>_col9</th>\n",
       "      <th>_col10</th>\n",
       "      <th>_col11</th>\n",
       "      <th>_col12</th>\n",
       "      <th>_col13</th>\n",
       "      <th>_col14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.857220</td>\n",
       "      <td>0.776533</td>\n",
       "      <td>0.619641</td>\n",
       "      <td>0.513418</td>\n",
       "      <td>0.289615</td>\n",
       "      <td>0.244357</td>\n",
       "      <td>0.957589</td>\n",
       "      <td>0.764655</td>\n",
       "      <td>0.080346</td>\n",
       "      <td>0.247009</td>\n",
       "      <td>0.753515</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.120284</td>\n",
       "      <td>0.602983</td>\n",
       "      <td>0.954893</td>\n",
       "      <td>0.740286</td>\n",
       "      <td>0.731098</td>\n",
       "      <td>0.448716</td>\n",
       "      <td>0.557691</td>\n",
       "      <td>0.257979</td>\n",
       "      <td>0.871001</td>\n",
       "      <td>0.031287</td>\n",
       "      <td>0.210629</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.912180</td>\n",
       "      <td>0.090513</td>\n",
       "      <td>0.363640</td>\n",
       "      <td>0.750713</td>\n",
       "      <td>0.547341</td>\n",
       "      <td>0.031661</td>\n",
       "      <td>0.994557</td>\n",
       "      <td>0.702622</td>\n",
       "      <td>0.395151</td>\n",
       "      <td>0.520261</td>\n",
       "      <td>0.747921</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.189186</td>\n",
       "      <td>0.312756</td>\n",
       "      <td>0.235713</td>\n",
       "      <td>0.248728</td>\n",
       "      <td>0.056102</td>\n",
       "      <td>0.872351</td>\n",
       "      <td>0.658739</td>\n",
       "      <td>0.233019</td>\n",
       "      <td>0.186730</td>\n",
       "      <td>0.749987</td>\n",
       "      <td>0.343642</td>\n",
       "      <td>199</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.892929</td>\n",
       "      <td>0.740022</td>\n",
       "      <td>0.132553</td>\n",
       "      <td>0.956464</td>\n",
       "      <td>0.322804</td>\n",
       "      <td>0.746096</td>\n",
       "      <td>0.120569</td>\n",
       "      <td>0.745465</td>\n",
       "      <td>0.085630</td>\n",
       "      <td>0.608585</td>\n",
       "      <td>0.762991</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      _col0     _col1     _col2     _col3     _col4     _col5     _col6  \\\n",
       "0  0.857220  0.776533  0.619641  0.513418  0.289615  0.244357  0.957589   \n",
       "1  0.120284  0.602983  0.954893  0.740286  0.731098  0.448716  0.557691   \n",
       "2  0.912180  0.090513  0.363640  0.750713  0.547341  0.031661  0.994557   \n",
       "3  0.189186  0.312756  0.235713  0.248728  0.056102  0.872351  0.658739   \n",
       "4  0.892929  0.740022  0.132553  0.956464  0.322804  0.746096  0.120569   \n",
       "\n",
       "      _col7     _col8     _col9    _col10  _col11  _col12  _col13  _col14  \n",
       "0  0.764655  0.080346  0.247009  0.753515       0       5       0       7  \n",
       "1  0.257979  0.871001  0.031287  0.210629      20       2      18       2  \n",
       "2  0.702622  0.395151  0.520261  0.747921       6       5       8      22  \n",
       "3  0.233019  0.186730  0.749987  0.343642     199      28       2       0  \n",
       "4  0.745465  0.085630  0.608585  0.762991       9       5       7       1  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(\"./data_parquet/val/gen_0.parquet\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6bcddb3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label',\n",
       " 'I1',\n",
       " 'I2',\n",
       " 'I3',\n",
       " 'I4',\n",
       " 'I5',\n",
       " 'I6',\n",
       " 'I7',\n",
       " 'I8',\n",
       " 'I9',\n",
       " 'I10',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C3',\n",
       " 'C4']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CATEGORICAL_COLUMNS=[\"C\" + str(x) for x in range(1, 5)]\n",
    "CONTINUOUS_COLUMNS=[\"I\" + str(x) for x in range(1, 11)]\n",
    "LABEL_COLUMNS = ['label']\n",
    "cols = LABEL_COLUMNS + CONTINUOUS_COLUMNS + CATEGORICAL_COLUMNS\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44612685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>I1</th>\n",
       "      <th>I2</th>\n",
       "      <th>I3</th>\n",
       "      <th>I4</th>\n",
       "      <th>I5</th>\n",
       "      <th>I6</th>\n",
       "      <th>I7</th>\n",
       "      <th>I8</th>\n",
       "      <th>I9</th>\n",
       "      <th>I10</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.857220</td>\n",
       "      <td>0.776533</td>\n",
       "      <td>0.619641</td>\n",
       "      <td>0.513418</td>\n",
       "      <td>0.289615</td>\n",
       "      <td>0.244357</td>\n",
       "      <td>0.957589</td>\n",
       "      <td>0.764655</td>\n",
       "      <td>0.080346</td>\n",
       "      <td>0.247009</td>\n",
       "      <td>0.753515</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.120284</td>\n",
       "      <td>0.602983</td>\n",
       "      <td>0.954893</td>\n",
       "      <td>0.740286</td>\n",
       "      <td>0.731098</td>\n",
       "      <td>0.448716</td>\n",
       "      <td>0.557691</td>\n",
       "      <td>0.257979</td>\n",
       "      <td>0.871001</td>\n",
       "      <td>0.031287</td>\n",
       "      <td>0.210629</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.912180</td>\n",
       "      <td>0.090513</td>\n",
       "      <td>0.363640</td>\n",
       "      <td>0.750713</td>\n",
       "      <td>0.547341</td>\n",
       "      <td>0.031661</td>\n",
       "      <td>0.994557</td>\n",
       "      <td>0.702622</td>\n",
       "      <td>0.395151</td>\n",
       "      <td>0.520261</td>\n",
       "      <td>0.747921</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.189186</td>\n",
       "      <td>0.312756</td>\n",
       "      <td>0.235713</td>\n",
       "      <td>0.248728</td>\n",
       "      <td>0.056102</td>\n",
       "      <td>0.872351</td>\n",
       "      <td>0.658739</td>\n",
       "      <td>0.233019</td>\n",
       "      <td>0.186730</td>\n",
       "      <td>0.749987</td>\n",
       "      <td>0.343642</td>\n",
       "      <td>199</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.892929</td>\n",
       "      <td>0.740022</td>\n",
       "      <td>0.132553</td>\n",
       "      <td>0.956464</td>\n",
       "      <td>0.322804</td>\n",
       "      <td>0.746096</td>\n",
       "      <td>0.120569</td>\n",
       "      <td>0.745465</td>\n",
       "      <td>0.085630</td>\n",
       "      <td>0.608585</td>\n",
       "      <td>0.762991</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label        I1        I2        I3        I4        I5        I6  \\\n",
       "0  0.857220  0.776533  0.619641  0.513418  0.289615  0.244357  0.957589   \n",
       "1  0.120284  0.602983  0.954893  0.740286  0.731098  0.448716  0.557691   \n",
       "2  0.912180  0.090513  0.363640  0.750713  0.547341  0.031661  0.994557   \n",
       "3  0.189186  0.312756  0.235713  0.248728  0.056102  0.872351  0.658739   \n",
       "4  0.892929  0.740022  0.132553  0.956464  0.322804  0.746096  0.120569   \n",
       "\n",
       "         I7        I8        I9       I10   C1  C2  C3  C4  \n",
       "0  0.764655  0.080346  0.247009  0.753515    0   5   0   7  \n",
       "1  0.257979  0.871001  0.031287  0.210629   20   2  18   2  \n",
       "2  0.702622  0.395151  0.520261  0.747921    6   5   8  22  \n",
       "3  0.233019  0.186730  0.749987  0.343642  199  28   2   0  \n",
       "4  0.745465  0.085630  0.608585  0.762991    9   5   7   1  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.set_axis(cols, axis=1,inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "32f40523",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./hps_model/infer_test.csv', sep=',', index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c6ee08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d89eca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2af9feb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./hps_model/hps_train0_sparse_1000.model', './hps_model/hps_train1_sparse_1000.model']\n",
      "[HCTR][08:22:51.768][WARNING][RK0][main]: default_value_for_each_table.size() is not equal to the number of embedding tables\n",
      "[HCTR][08:22:51.768][INFO][RK0][main]: default_emb_vec_value is not specified using default: 0\n",
      "[HCTR][08:22:51.768][INFO][RK0][main]: default_emb_vec_value is not specified using default: 0\n",
      "[HCTR][08:22:51.768][INFO][RK0][main]: Creating ParallelHashMap CPU database backend...\n",
      "[HCTR][08:22:51.769][INFO][RK0][main]: Created parallel (16 partitions) blank database backend in local memory!\n",
      "[HCTR][08:22:51.769][INFO][RK0][main]: Volatile DB: initial cache rate = 1\n",
      "[HCTR][08:22:51.769][INFO][RK0][main]: Volatile DB: cache missed embeddings = 0\n",
      "[HCTR][08:22:51.782][INFO][RK0][main]: Table: hps_et.hps_train.sparse_embedding1; cached 18502 / 18502 embeddings in volatile database (ParallelHashMap); load: 18502 / 18446744073709551615 (0.00%).\n",
      "[HCTR][08:22:51.788][INFO][RK0][main]: Table: hps_et.hps_train.sparse_embedding2; cached 18471 / 18471 embeddings in volatile database (ParallelHashMap); load: 18471 / 18446744073709551615 (0.00%).\n",
      "[HCTR][08:22:51.788][DEBUG][RK0][main]: Real-time subscribers created!\n",
      "[HCTR][08:22:51.788][INFO][RK0][main]: Create embedding cache in device 0.\n",
      "[HCTR][08:22:51.789][INFO][RK0][main]: Use GPU embedding cache: True, cache size percentage: 0.900000\n",
      "[HCTR][08:22:51.789][INFO][RK0][main]: Configured cache hit rate threshold: 0.900000\n",
      "[HCTR][08:22:51.973][WARNING][RK0][main]: MPI was already initialized somewhere elese. Lifetime service disabled.\n",
      "[HCTR][08:22:51.973][INFO][RK0][main]: Global seed is 533563138\n",
      "[HCTR][08:22:52.132][INFO][RK0][main]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 3\n",
      "[HCTR][08:22:54.025][WARNING][RK0][main]: Peer-to-peer access cannot be fully enabled.\n",
      "[HCTR][08:22:54.025][INFO][RK0][main]: Start all2all warmup\n",
      "[HCTR][08:22:54.028][INFO][RK0][main]: End all2all warmup\n",
      "[HCTR][08:22:54.030][INFO][RK0][main]: Create inference session on device: 0\n",
      "[HCTR][08:22:54.030][INFO][RK0][main]: Model name: hps_train\n",
      "[HCTR][08:22:54.030][INFO][RK0][main]: Use mixed precision: False\n",
      "[HCTR][08:22:54.030][INFO][RK0][main]: Use cuda graph: True\n",
      "[HCTR][08:22:54.030][INFO][RK0][main]: Max batchsize: 1024\n",
      "[HCTR][08:22:54.030][INFO][RK0][main]: Use I64 input key: True\n",
      "[HCTR][08:22:54.030][INFO][RK0][main]: start create embedding for inference\n",
      "[HCTR][08:22:54.030][INFO][RK0][main]: sparse_input name data1\n",
      "[HCTR][08:22:54.030][INFO][RK0][main]: sparse_input name data2\n",
      "[HCTR][08:22:54.030][INFO][RK0][main]: create embedding for inference success\n",
      "[HCTR][08:22:54.030][INFO][RK0][main]: Inference stage skip BinaryCrossEntropyLoss layer, replaced by Sigmoid layer\n",
      "Traceback (most recent call last):\n",
      "  File \"hps_train2predict.py\", line 92, in <module>\n",
      "    demo_inference(model_name, network_file, dense_file, embedding_file_list, data_file, True)\n",
      "  File \"hps_train2predict.py\", line 43, in demo_inference\n",
      "    output = inference_session.predict(dense_features, embedding_columns, row_ptrs)\n",
      "RuntimeError: Runtime error: The dimension of dense features is not consistent\n",
      "\tError_t::WrongInput at predict_(/hugectr/HugeCTR/include/pybind/inference_wrapper.hpp:158)\n"
     ]
    }
   ],
   "source": [
    "!python hps_train2predict.py \\\n",
    "    \"hps_train\" \\\n",
    "    \"./hps_model/hps_train.json\" \\\n",
    "    \"./hps_model/hps_train_dense_1000.model\" \\\n",
    "    \"./hps_model/hps_train0_sparse_1000.model,./hps_model/hps_train1_sparse_1000.model\" \\\n",
    "    \"./hps_model/infer_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3f6510",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62243a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84460dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
