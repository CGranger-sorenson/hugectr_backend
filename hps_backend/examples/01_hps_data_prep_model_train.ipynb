{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70ffb5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HCTR][07:46:25.518][INFO][RK0][main]: Generate Parquet dataset\n",
      "[HCTR][07:46:25.518][INFO][RK0][main]: train data folder: ./data_parquet, eval data folder: ./data_parquet, slot_size_array: 10000, 10000, 10000, 10000, nnz array: 1, 1, 1, 1, #files for train: 16, #files for eval: 4, #samples per file: 40960, Use power law distribution: 1, alpha of power law: 1.3\n",
      "[HCTR][07:46:25.518][INFO][RK0][main]: ./data_parquet exist\n",
      "[HCTR][07:46:25.518][INFO][RK0][main]: ./data_parquet exist\n",
      "[HCTR][07:46:25.518][INFO][RK0][main]: ./data_parquet/train exist\n",
      "[HCTR][07:46:25.518][INFO][RK0][main]: ./data_parquet/train/gen_0.parquet\n",
      "[HCTR][07:46:25.663][INFO][RK0][main]: ./data_parquet/train/gen_1.parquet\n",
      "[HCTR][07:46:25.811][INFO][RK0][main]: ./data_parquet/train/gen_2.parquet\n",
      "[HCTR][07:46:25.937][INFO][RK0][main]: ./data_parquet/train/gen_3.parquet\n",
      "[HCTR][07:46:26.071][INFO][RK0][main]: ./data_parquet/train/gen_4.parquet\n",
      "[HCTR][07:46:26.194][INFO][RK0][main]: ./data_parquet/train/gen_5.parquet\n",
      "[HCTR][07:46:26.309][INFO][RK0][main]: ./data_parquet/train/gen_6.parquet\n",
      "[HCTR][07:46:26.434][INFO][RK0][main]: ./data_parquet/train/gen_7.parquet\n",
      "[HCTR][07:46:26.561][INFO][RK0][main]: ./data_parquet/train/gen_8.parquet\n",
      "[HCTR][07:46:26.688][INFO][RK0][main]: ./data_parquet/train/gen_9.parquet\n",
      "[HCTR][07:46:26.817][INFO][RK0][main]: ./data_parquet/train/gen_10.parquet\n",
      "[HCTR][07:46:26.970][INFO][RK0][main]: ./data_parquet/train/gen_11.parquet\n",
      "[HCTR][07:46:27.088][INFO][RK0][main]: ./data_parquet/train/gen_12.parquet\n",
      "[HCTR][07:46:27.222][INFO][RK0][main]: ./data_parquet/train/gen_13.parquet\n",
      "[HCTR][07:46:27.358][INFO][RK0][main]: ./data_parquet/train/gen_14.parquet\n",
      "[HCTR][07:46:27.484][INFO][RK0][main]: ./data_parquet/train/gen_15.parquet\n",
      "[HCTR][07:46:27.618][INFO][RK0][main]: ./data_parquet/file_list.txt done!\n",
      "[HCTR][07:46:27.618][INFO][RK0][main]: ./data_parquet/val exist\n",
      "[HCTR][07:46:27.618][INFO][RK0][main]: ./data_parquet/val/gen_0.parquet\n",
      "[HCTR][07:46:27.753][INFO][RK0][main]: ./data_parquet/val/gen_1.parquet\n",
      "[HCTR][07:46:27.873][INFO][RK0][main]: ./data_parquet/val/gen_2.parquet\n",
      "[HCTR][07:46:28.007][INFO][RK0][main]: ./data_parquet/val/gen_3.parquet\n",
      "[HCTR][07:46:28.140][INFO][RK0][main]: ./data_parquet/file_list_test.txt done!\n"
     ]
    }
   ],
   "source": [
    "import hugectr\n",
    "from hugectr.tools import DataGeneratorParams, DataGenerator\n",
    "\n",
    "data_generator_params = DataGeneratorParams(\n",
    "  format = hugectr.DataReaderType_t.Parquet,\n",
    "  label_dim = 1,\n",
    "  dense_dim = 10,\n",
    "  num_slot = 4,\n",
    "  i64_input_key = True,\n",
    "  nnz_array = [1, 1, 1, 1],\n",
    "  source = \"./data_parquet/file_list.txt\",\n",
    "  eval_source = \"./data_parquet/file_list_test.txt\",\n",
    "  slot_size_array = [10000, 10000, 10000, 10000],\n",
    "  check_type = hugectr.Check_t.Non,\n",
    "  dist_type = hugectr.Distribution_t.PowerLaw,\n",
    "  power_law_type = hugectr.PowerLaw_t.Short,\n",
    "  num_files = 16,\n",
    "  eval_num_files = 4,\n",
    "  num_samples_per_file = 40960)\n",
    "data_generator = DataGenerator(data_generator_params)\n",
    "data_generator.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a210c112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/merlin/hugectr_inference_backend/hps_backend/examples\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb7217ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘hps_model’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir hps_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e31dd36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "250afbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting fix_meta_json_path.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile fix_meta_json_path.py\n",
    "\n",
    "import json\n",
    "file_path_train = './data_parquet/train/_metadata.json'\n",
    "file_path_val   = './data_parquet/val/_metadata.json'\n",
    "def fix_meta_json_path(file_path):\n",
    "    with open(file_path) as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    for item in data['file_stats']:\n",
    "        item['file_name'] = \"gen_{}\".format(item['file_name'])\n",
    "        print(item)\n",
    "    \n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "fix_meta_json_path(file_path_train)\n",
    "fix_meta_json_path(file_path_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f519c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file_name': 'gen_0.parquet', 'num_rows': 40960}\n",
      "{'file_name': 'gen_1.parquet', 'num_rows': 40960}\n",
      "{'file_name': 'gen_2.parquet', 'num_rows': 40960}\n",
      "{'file_name': 'gen_3.parquet', 'num_rows': 40960}\n",
      "{'file_name': 'gen_4.parquet', 'num_rows': 40960}\n",
      "{'file_name': 'gen_5.parquet', 'num_rows': 40960}\n",
      "{'file_name': 'gen_6.parquet', 'num_rows': 40960}\n",
      "{'file_name': 'gen_7.parquet', 'num_rows': 40960}\n",
      "{'file_name': 'gen_8.parquet', 'num_rows': 40960}\n",
      "{'file_name': 'gen_9.parquet', 'num_rows': 40960}\n",
      "{'file_name': 'gen_10.parquet', 'num_rows': 40960}\n",
      "{'file_name': 'gen_11.parquet', 'num_rows': 40960}\n",
      "{'file_name': 'gen_12.parquet', 'num_rows': 40960}\n",
      "{'file_name': 'gen_13.parquet', 'num_rows': 40960}\n",
      "{'file_name': 'gen_14.parquet', 'num_rows': 40960}\n",
      "{'file_name': 'gen_15.parquet', 'num_rows': 40960}\n",
      "{'file_name': 'gen_0.parquet', 'num_rows': 40960}\n",
      "{'file_name': 'gen_1.parquet', 'num_rows': 40960}\n",
      "{'file_name': 'gen_2.parquet', 'num_rows': 40960}\n",
      "{'file_name': 'gen_3.parquet', 'num_rows': 40960}\n"
     ]
    }
   ],
   "source": [
    "!python3 fix_meta_json_path.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515ed1ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9727e6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfadcbff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_col0</th>\n",
       "      <th>_col1</th>\n",
       "      <th>_col2</th>\n",
       "      <th>_col3</th>\n",
       "      <th>_col4</th>\n",
       "      <th>_col5</th>\n",
       "      <th>_col6</th>\n",
       "      <th>_col7</th>\n",
       "      <th>_col8</th>\n",
       "      <th>_col9</th>\n",
       "      <th>_col10</th>\n",
       "      <th>_col11</th>\n",
       "      <th>_col12</th>\n",
       "      <th>_col13</th>\n",
       "      <th>_col14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.565468</td>\n",
       "      <td>0.552473</td>\n",
       "      <td>0.880794</td>\n",
       "      <td>0.419513</td>\n",
       "      <td>0.776548</td>\n",
       "      <td>0.606234</td>\n",
       "      <td>0.528292</td>\n",
       "      <td>0.978610</td>\n",
       "      <td>0.240545</td>\n",
       "      <td>0.944232</td>\n",
       "      <td>0.828531</td>\n",
       "      <td>135</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.965076</td>\n",
       "      <td>0.891845</td>\n",
       "      <td>0.294612</td>\n",
       "      <td>0.117228</td>\n",
       "      <td>0.919538</td>\n",
       "      <td>0.855993</td>\n",
       "      <td>0.228353</td>\n",
       "      <td>0.402679</td>\n",
       "      <td>0.377469</td>\n",
       "      <td>0.890093</td>\n",
       "      <td>0.428936</td>\n",
       "      <td>11</td>\n",
       "      <td>28</td>\n",
       "      <td>264</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.196433</td>\n",
       "      <td>0.943692</td>\n",
       "      <td>0.506197</td>\n",
       "      <td>0.306889</td>\n",
       "      <td>0.598248</td>\n",
       "      <td>0.853011</td>\n",
       "      <td>0.245981</td>\n",
       "      <td>0.503549</td>\n",
       "      <td>0.609469</td>\n",
       "      <td>0.741337</td>\n",
       "      <td>0.568564</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.718024</td>\n",
       "      <td>0.367026</td>\n",
       "      <td>0.873090</td>\n",
       "      <td>0.375533</td>\n",
       "      <td>0.101961</td>\n",
       "      <td>0.530518</td>\n",
       "      <td>0.467618</td>\n",
       "      <td>0.102497</td>\n",
       "      <td>0.451634</td>\n",
       "      <td>0.022515</td>\n",
       "      <td>0.689176</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>597</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.537616</td>\n",
       "      <td>0.227445</td>\n",
       "      <td>0.333675</td>\n",
       "      <td>0.550674</td>\n",
       "      <td>0.348952</td>\n",
       "      <td>0.707912</td>\n",
       "      <td>0.655634</td>\n",
       "      <td>0.638966</td>\n",
       "      <td>0.940576</td>\n",
       "      <td>0.218253</td>\n",
       "      <td>0.037498</td>\n",
       "      <td>3571</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      _col0     _col1     _col2     _col3     _col4     _col5     _col6  \\\n",
       "0  0.565468  0.552473  0.880794  0.419513  0.776548  0.606234  0.528292   \n",
       "1  0.965076  0.891845  0.294612  0.117228  0.919538  0.855993  0.228353   \n",
       "2  0.196433  0.943692  0.506197  0.306889  0.598248  0.853011  0.245981   \n",
       "3  0.718024  0.367026  0.873090  0.375533  0.101961  0.530518  0.467618   \n",
       "4  0.537616  0.227445  0.333675  0.550674  0.348952  0.707912  0.655634   \n",
       "\n",
       "      _col7     _col8     _col9    _col10  _col11  _col12  _col13  _col14  \n",
       "0  0.978610  0.240545  0.944232  0.828531     135      32       1       5  \n",
       "1  0.402679  0.377469  0.890093  0.428936      11      28     264       4  \n",
       "2  0.503549  0.609469  0.741337  0.568564       1      11      15       9  \n",
       "3  0.102497  0.451634  0.022515  0.689176      15       0     597       6  \n",
       "4  0.638966  0.940576  0.218253  0.037498    3571       0       1       9  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(\"./data_parquet/train/gen_0.parquet\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ad60d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c145a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "import re\n",
    "import shutil\n",
    "import glob\n",
    "import warnings\n",
    "\n",
    "BASE_DIR = \"/hps_demo\"\n",
    "embedding_folder  = os.path.join(BASE_DIR, \"embedding\")\n",
    "wdl_embedding_repo= os.path.join(embedding_folder, \"hps_infer\")\n",
    "wdl_version =os.path.join(wdl_embedding_repo, \"1\")\n",
    "\n",
    "if os.path.isdir(embedding_folder):\n",
    "    shutil.rmtree(embedding_folder)\n",
    "os.makedirs(embedding_folder)\n",
    "\n",
    "if os.path.isdir(wdl_embedding_repo):\n",
    "    shutil.rmtree(wdl_embedding_repo)\n",
    "os.makedirs(wdl_embedding_repo)\n",
    "\n",
    "if os.path.isdir(wdl_version):\n",
    "    shutil.rmtree(wdl_version)\n",
    "os.makedirs(wdl_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5010e004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/hps_demo\u001b[00m\n",
      "└── \u001b[01;34membedding\u001b[00m\n",
      "    └── \u001b[01;34mhps_infer\u001b[00m\n",
      "        └── \u001b[01;34m1\u001b[00m\n",
      "\n",
      "3 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "!tree -l $BASE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6aa15d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d75c59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "011cd3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hps_model_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hps_model_train.py\n",
    "\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "\n",
    "## typical DLRM architecture building\n",
    "## Bottom layer: bottom MLP layer for dense features(10) + embedding layer for sparse features(2+2)\n",
    "## Middle layer: concatenate 3 blocks\n",
    "## Top layer: top MLP layer to fully connect all inputs (FC twice + RELU + BinaryCrossEntropy)\n",
    "\n",
    "# construct model\n",
    "solver = hugectr.CreateSolver(model_name = \"hps_train\",\n",
    "                              max_eval_batches = 1,\n",
    "                              batchsize_eval = 1024,\n",
    "                              batchsize = 1024,\n",
    "                              lr = 0.001,\n",
    "                              vvgpu = [[0]],\n",
    "                              i64_input_key = True,\n",
    "                              repeat_dataset = True,\n",
    "                              use_cuda_graph = True)\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Parquet,\n",
    "                                  source = [\"./data_parquet/file_list.txt\"],\n",
    "                                  eval_source = \"./data_parquet/file_list_test.txt\",\n",
    "                                  check_type = hugectr.Check_t.Non,\n",
    "                                  slot_size_array = [10000, 10000, 10000, 10000])\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.Adam)\n",
    "model = hugectr.Model(solver, reader, optimizer)\n",
    "\n",
    "# model NN\n",
    "\n",
    "# https://nvidia-merlin.github.io/HugeCTR/master/api/python_interface.html?highlight=model#input-layer\n",
    "# check for \"data_reader_sparse_param_array\" parameter, 4 sparse feature in this case\n",
    "# assigned 2 sparse feat for slot1, 2 sparse feat for slot2\n",
    "model.add(hugectr.Input(label_dim = 1, label_name = \"label\",\n",
    "                        dense_dim = 10, dense_name = \"dense\",\n",
    "                        data_reader_sparse_param_array = \n",
    "                        [hugectr.DataReaderSparseParam(\"data1\", [1, 1], True, 2),\n",
    "                        hugectr.DataReaderSparseParam(\"data2\", [1, 1], True, 2)]))\n",
    "\n",
    "# sparse layer for categorical features\n",
    "# https://nvidia-merlin.github.io/HugeCTR/master/api/python_interface.html?highlight=model#sparseembedding\n",
    "# sparse layer should be defined after Input layer, but before Dense layer\n",
    "# for embedding_type, check https://nvidia-merlin.github.io/HugeCTR/master/api/hugectr_layer_book.html#embedding-types-detail\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash, \n",
    "                            workspace_size_per_gpu_in_mb = 4,\n",
    "                            embedding_vec_size = 16,\n",
    "                            combiner = \"sum\",\n",
    "                            sparse_embedding_name = \"sparse_embedding1\",\n",
    "                            bottom_name = \"data1\",\n",
    "                            optimizer = optimizer))\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash, \n",
    "                            workspace_size_per_gpu_in_mb = 8,\n",
    "                            embedding_vec_size = 32,\n",
    "                            combiner = \"sum\",\n",
    "                            sparse_embedding_name = \"sparse_embedding2\",\n",
    "                            bottom_name = \"data2\",\n",
    "                            optimizer = optimizer))\n",
    "# reshape\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding1\"],\n",
    "                            top_names = [\"reshape1\"],\n",
    "                            leading_dim=32))                            \n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding2\"],\n",
    "                            top_names = [\"reshape2\"],\n",
    "                            leading_dim=64))\n",
    "\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Concat,\n",
    "                            bottom_names = [\"reshape1\", \"reshape2\", \"dense\"], top_names = [\"concat1\"]))\n",
    "\n",
    "# FC layer + ReLU + FC + binary cross entropy\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"concat1\"],\n",
    "                            top_names = [\"fc1\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc1\"],\n",
    "                            top_names = [\"relu1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"relu1\"],\n",
    "                            top_names = [\"fc2\"],\n",
    "                            num_output=1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "                            bottom_names = [\"fc2\", \"label\"],\n",
    "                            top_names = [\"loss\"]))\n",
    "\n",
    "# model compile\n",
    "model.compile()\n",
    "model.summary()\n",
    "model.graph_to_json(\"./hps_model/hps_train.json\")\n",
    "model.fit(max_iter = 1100, display = 200, eval_interval = 1000, snapshot = 1000, snapshot_prefix = \"./hps_model/hps_train\")\n",
    "model.export_predictions(\"./hps_model/hps_train_pred_\" + str(1000), \"./hps_model/hps_train_label_\" + str(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59b7632b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HugeCTR Version: 3.6\n",
      "====================================================Model Init=====================================================\n",
      "[HCTR][07:48:08.807][INFO][RK0][main]: Initialize model: hps_train\n",
      "[HCTR][07:48:08.807][WARNING][RK0][main]: MPI was already initialized somewhere elese. Lifetime service disabled.\n",
      "[HCTR][07:48:08.807][INFO][RK0][main]: Global seed is 617217921\n",
      "[HCTR][07:48:08.968][INFO][RK0][main]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 3\n",
      "[HCTR][07:48:10.454][WARNING][RK0][main]: Peer-to-peer access cannot be fully enabled.\n",
      "[HCTR][07:48:10.454][INFO][RK0][main]: Start all2all warmup\n",
      "[HCTR][07:48:10.456][INFO][RK0][main]: End all2all warmup\n",
      "[HCTR][07:48:10.457][INFO][RK0][main]: Using All-reduce algorithm: NCCL\n",
      "[HCTR][07:48:10.458][INFO][RK0][main]: Device 0: NVIDIA A100-SXM4-80GB\n",
      "[HCTR][07:48:10.458][INFO][RK0][main]: num of DataReader workers: 1\n",
      "[HCTR][07:48:10.459][INFO][RK0][main]: Vocabulary size: 40000\n",
      "[HCTR][07:48:10.459][INFO][RK0][main]: max_vocabulary_size_per_gpu_=21845\n",
      "[HCTR][07:48:10.461][DEBUG][RK0][tid #140051467990784]: file_name_ ./data_parquet/val/gen_0.parquet file_total_rows_ 40960\n",
      "[HCTR][07:48:10.461][DEBUG][RK0][tid #140051581523712]: file_name_ ./data_parquet/train/gen_0.parquet file_total_rows_ 40960\n",
      "[HCTR][07:48:10.465][INFO][RK0][main]: max_vocabulary_size_per_gpu_=21845\n",
      "[HCTR][07:48:10.468][INFO][RK0][main]: Graph analysis to resolve tensor dependency\n",
      "===================================================Model Compile===================================================\n",
      "[HCTR][07:48:11.700][INFO][RK0][main]: gpu0 start to init embedding\n",
      "[HCTR][07:48:11.701][INFO][RK0][main]: gpu0 init embedding done\n",
      "[HCTR][07:48:11.701][INFO][RK0][main]: gpu0 start to init embedding\n",
      "[HCTR][07:48:11.701][INFO][RK0][main]: gpu0 init embedding done\n",
      "[HCTR][07:48:11.701][INFO][RK0][main]: AUC Init batch_size_per_gpu:1024 n_batches:1 num_classes:1\n",
      "num_local_gpus:1 num_global_gpus:1 num_bins:10000 num_partitions:1\n",
      "[HCTR][07:48:11.701][INFO][RK0][main]: Starting AUC NCCL warm-up\n",
      "[HCTR][07:48:11.702][INFO][RK0][main]: Warm-up done\n",
      "===================================================Model Summary===================================================\n",
      "[HCTR][07:48:11.702][INFO][RK0][main]: label                                   Dense                         Sparse                        \n",
      "label                                   dense                          data1,data2                   \n",
      "(None, 1)                               (None, 10)                              \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "DistributedSlotSparseEmbeddingHash      data1                         sparse_embedding1             (None, 2, 16)                 \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "DistributedSlotSparseEmbeddingHash      data2                         sparse_embedding2             (None, 2, 32)                 \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Reshape                                 sparse_embedding1             reshape1                      (None, 32)                    \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Reshape                                 sparse_embedding2             reshape2                      (None, 64)                    \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Concat                                  reshape1                      concat1                       (None, 106)                   \n",
      "                                        reshape2                                                                                  \n",
      "                                        dense                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            concat1                       fc1                           (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc1                           relu1                         (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            relu1                         fc2                           (None, 1)                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "BinaryCrossEntropyLoss                  fc2                           loss                                                        \n",
      "                                        label                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "[HCTR][07:48:11.703][INFO][RK0][main]: Save the model graph to ./hps_model/hps_train.json successfully\n",
      "=====================================================Model Fit=====================================================\n",
      "[HCTR][07:48:11.703][INFO][RK0][main]: Use non-epoch mode with number of iterations: 1100\n",
      "[HCTR][07:48:11.703][INFO][RK0][main]: Training batchsize: 1024, evaluation batchsize: 1024\n",
      "[HCTR][07:48:11.703][INFO][RK0][main]: Evaluation interval: 1000, snapshot interval: 1000\n",
      "[HCTR][07:48:11.703][INFO][RK0][main]: Dense network trainable: True\n",
      "[HCTR][07:48:11.703][INFO][RK0][main]: Sparse embedding sparse_embedding1 trainable: True\n",
      "[HCTR][07:48:11.703][INFO][RK0][main]: Sparse embedding sparse_embedding2 trainable: True\n",
      "[HCTR][07:48:11.703][INFO][RK0][main]: Use mixed precision: False, scaler: 1.000000, use cuda graph: True\n",
      "[HCTR][07:48:11.703][INFO][RK0][main]: lr: 0.001000, warmup_steps: 1, end_lr: 0.000000\n",
      "[HCTR][07:48:11.703][INFO][RK0][main]: decay_start: 0, decay_steps: 1, decay_power: 2.000000\n",
      "[HCTR][07:48:11.703][INFO][RK0][main]: Training source file: ./data_parquet/file_list.txt\n",
      "[HCTR][07:48:11.703][INFO][RK0][main]: Evaluation source file: ./data_parquet/file_list_test.txt\n",
      "[HCTR][07:48:11.734][DEBUG][RK0][tid #140051581523712]: file_name_ ./data_parquet/train/gen_1.parquet file_total_rows_ 40960\n",
      "[HCTR][07:48:11.768][DEBUG][RK0][tid #140051581523712]: file_name_ ./data_parquet/train/gen_2.parquet file_total_rows_ 40960\n",
      "[HCTR][07:48:11.802][DEBUG][RK0][tid #140051581523712]: file_name_ ./data_parquet/train/gen_3.parquet file_total_rows_ 40960\n",
      "[HCTR][07:48:11.837][DEBUG][RK0][tid #140051581523712]: file_name_ ./data_parquet/train/gen_4.parquet file_total_rows_ 40960\n",
      "[HCTR][07:48:11.871][DEBUG][RK0][tid #140051581523712]: file_name_ ./data_parquet/train/gen_5.parquet file_total_rows_ 40960\n",
      "[HCTR][07:48:11.877][INFO][RK0][main]: Iter: 200 Time(200 iters): 0.17414s Loss: 0.692902 lr:0.001\n",
      "[HCTR][07:48:11.906][DEBUG][RK0][tid #140051581523712]: file_name_ ./data_parquet/train/gen_6.parquet file_total_rows_ 40960\n",
      "[HCTR][07:48:11.941][DEBUG][RK0][tid #140051581523712]: file_name_ ./data_parquet/train/gen_7.parquet file_total_rows_ 40960\n",
      "[HCTR][07:48:11.976][DEBUG][RK0][tid #140051581523712]: file_name_ ./data_parquet/train/gen_8.parquet file_total_rows_ 40960\n",
      "[HCTR][07:48:12.010][DEBUG][RK0][tid #140051581523712]: file_name_ ./data_parquet/train/gen_9.parquet file_total_rows_ 40960\n",
      "[HCTR][07:48:12.046][DEBUG][RK0][tid #140051581523712]: file_name_ ./data_parquet/train/gen_10.parquet file_total_rows_ 40960\n",
      "[HCTR][07:48:12.051][INFO][RK0][main]: Iter: 400 Time(200 iters): 0.174422s Loss: 0.693527 lr:0.001\n",
      "[HCTR][07:48:12.081][DEBUG][RK0][tid #140051581523712]: file_name_ ./data_parquet/train/gen_11.parquet file_total_rows_ 40960\n",
      "[HCTR][07:48:12.116][DEBUG][RK0][tid #140051581523712]: file_name_ ./data_parquet/train/gen_12.parquet file_total_rows_ 40960\n",
      "[HCTR][07:48:12.151][DEBUG][RK0][tid #140051581523712]: file_name_ ./data_parquet/train/gen_13.parquet file_total_rows_ 40960\n",
      "[HCTR][07:48:12.187][DEBUG][RK0][tid #140051581523712]: file_name_ ./data_parquet/train/gen_14.parquet file_total_rows_ 40960\n",
      "[HCTR][07:48:12.222][DEBUG][RK0][tid #140051581523712]: file_name_ ./data_parquet/train/gen_15.parquet file_total_rows_ 40960\n",
      "[HCTR][07:48:12.227][INFO][RK0][main]: Iter: 600 Time(200 iters): 0.175765s Loss: 0.693337 lr:0.001\n",
      "[HCTR][07:48:12.257][DEBUG][RK0][tid #140051581523712]: file_name_ ./data_parquet/train/gen_0.parquet file_total_rows_ 40960\n",
      "[HCTR][07:48:12.292][DEBUG][RK0][tid #140051581523712]: file_name_ ./data_parquet/train/gen_1.parquet file_total_rows_ 40960\n",
      "[HCTR][07:48:12.327][DEBUG][RK0][tid #140051581523712]: file_name_ ./data_parquet/train/gen_2.parquet file_total_rows_ 40960\n",
      "[HCTR][07:48:12.362][DEBUG][RK0][tid #140051581523712]: file_name_ ./data_parquet/train/gen_3.parquet file_total_rows_ 40960\n",
      "[HCTR][07:48:12.397][DEBUG][RK0][tid #140051581523712]: file_name_ ./data_parquet/train/gen_4.parquet file_total_rows_ 40960\n",
      "[HCTR][07:48:12.403][INFO][RK0][main]: Iter: 800 Time(200 iters): 0.175601s Loss: 0.691377 lr:0.001\n",
      "[HCTR][07:48:12.432][DEBUG][RK0][tid #140051581523712]: file_name_ ./data_parquet/train/gen_5.parquet file_total_rows_ 40960\n",
      "[HCTR][07:48:12.468][DEBUG][RK0][tid #140051581523712]: file_name_ ./data_parquet/train/gen_6.parquet file_total_rows_ 40960\n",
      "[HCTR][07:48:12.503][DEBUG][RK0][tid #140051581523712]: file_name_ ./data_parquet/train/gen_7.parquet file_total_rows_ 40960\n",
      "[HCTR][07:48:12.538][DEBUG][RK0][tid #140051581523712]: file_name_ ./data_parquet/train/gen_8.parquet file_total_rows_ 40960\n",
      "[HCTR][07:48:12.573][DEBUG][RK0][tid #140051581523712]: file_name_ ./data_parquet/train/gen_9.parquet file_total_rows_ 40960\n",
      "[HCTR][07:48:12.578][INFO][RK0][main]: Iter: 1000 Time(200 iters): 0.175362s Loss: 0.691771 lr:0.001\n",
      "[HCTR][07:48:12.579][INFO][RK0][main]: Evaluation, AUC: 0.505952\n",
      "[HCTR][07:48:12.580][INFO][RK0][main]: Eval Time for 1 iters: 0.000628s\n",
      "[HCTR][07:48:12.589][INFO][RK0][main]: Rank0: Write hash table to file\n",
      "[HCTR][07:48:12.592][INFO][RK0][main]: Rank0: Write hash table to file\n",
      "[HCTR][07:48:12.594][INFO][RK0][main]: Dumping sparse weights to files, successful\n",
      "[HCTR][07:48:12.594][INFO][RK0][main]: Rank0: Write optimzer state to file\n",
      "[HCTR][07:48:12.595][INFO][RK0][main]: Done\n",
      "[HCTR][07:48:12.595][INFO][RK0][main]: Rank0: Write optimzer state to file\n",
      "[HCTR][07:48:12.596][INFO][RK0][main]: Done\n",
      "[HCTR][07:48:12.596][INFO][RK0][main]: Rank0: Write optimzer state to file\n",
      "[HCTR][07:48:12.598][INFO][RK0][main]: Done\n",
      "[HCTR][07:48:12.598][INFO][RK0][main]: Rank0: Write optimzer state to file\n",
      "[HCTR][07:48:12.600][INFO][RK0][main]: Done\n",
      "[HCTR][07:48:12.600][INFO][RK0][main]: Dumping sparse optimzer states to files, successful\n",
      "[HCTR][07:48:12.601][INFO][RK0][main]: Dumping dense weights to file, successful\n",
      "[HCTR][07:48:12.601][INFO][RK0][main]: Dumping dense optimizer states to file, successful\n",
      "[HCTR][07:48:12.632][DEBUG][RK0][tid #140051581523712]: file_name_ ./data_parquet/train/gen_10.parquet file_total_rows_ 40960\n",
      "[HCTR][07:48:12.667][DEBUG][RK0][tid #140051581523712]: file_name_ ./data_parquet/train/gen_11.parquet file_total_rows_ 40960\n",
      "[HCTR][07:48:12.689][INFO][RK0][main]: Finish 1100 iterations with batchsize: 1024 in 0.99s.\n"
     ]
    }
   ],
   "source": [
    "!python3 hps_model_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67739460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/hps_demo\u001b[00m\n",
      "└── \u001b[01;34membedding\u001b[00m\n",
      "    └── \u001b[01;34mhps_infer\u001b[00m\n",
      "        └── \u001b[01;34m1\u001b[00m\n",
      "\n",
      "3 directories, 0 files\n"
     ]
    }
   ],
   "source": [
    "!tree -l $BASE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d44a727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mhps_model\u001b[00m\n",
      "├── hps_train.json\n",
      "├── hps_train0_opt_sparse_1000.model\n",
      "├── \u001b[01;34mhps_train0_sparse_1000.model\u001b[00m\n",
      "│   ├── emb_vector\n",
      "│   └── key\n",
      "├── hps_train1_opt_sparse_1000.model\n",
      "├── \u001b[01;34mhps_train1_sparse_1000.model\u001b[00m\n",
      "│   ├── emb_vector\n",
      "│   └── key\n",
      "├── hps_train_dense_1000.model\n",
      "├── hps_train_label_1000\n",
      "├── hps_train_opt_dense_1000.model\n",
      "├── hps_train_pred_1000\n",
      "└── infer_test.csv\n",
      "\n",
      "2 directories, 12 files\n"
     ]
    }
   ],
   "source": [
    "!tree -l hps_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "886da1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/hps_demo\u001b[00m\n",
      "└── \u001b[01;34membedding\u001b[00m\n",
      "    └── \u001b[01;34mhps_infer\u001b[00m\n",
      "        └── \u001b[01;34m1\u001b[00m\n",
      "            ├── \u001b[01;34mhps_train0_sparse_1000.model\u001b[00m\n",
      "            │   ├── emb_vector\n",
      "            │   └── key\n",
      "            └── \u001b[01;34mhps_train1_sparse_1000.model\u001b[00m\n",
      "                ├── emb_vector\n",
      "                └── key\n",
      "\n",
      "5 directories, 4 files\n"
     ]
    }
   ],
   "source": [
    "!cp -r ./hps_model/hps_train0_sparse_1000.model /hps_demo/embedding/hps_infer/1\n",
    "!cp -r ./hps_model/hps_train1_sparse_1000.model /hps_demo/embedding/hps_infer/1\n",
    "!tree -l /hps_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3474e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4866e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare infer_test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53abe291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_col0</th>\n",
       "      <th>_col1</th>\n",
       "      <th>_col2</th>\n",
       "      <th>_col3</th>\n",
       "      <th>_col4</th>\n",
       "      <th>_col5</th>\n",
       "      <th>_col6</th>\n",
       "      <th>_col7</th>\n",
       "      <th>_col8</th>\n",
       "      <th>_col9</th>\n",
       "      <th>_col10</th>\n",
       "      <th>_col11</th>\n",
       "      <th>_col12</th>\n",
       "      <th>_col13</th>\n",
       "      <th>_col14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.369659</td>\n",
       "      <td>0.562382</td>\n",
       "      <td>0.268092</td>\n",
       "      <td>0.270976</td>\n",
       "      <td>0.425515</td>\n",
       "      <td>0.436015</td>\n",
       "      <td>0.794521</td>\n",
       "      <td>0.969748</td>\n",
       "      <td>0.118850</td>\n",
       "      <td>0.317950</td>\n",
       "      <td>0.050502</td>\n",
       "      <td>1</td>\n",
       "      <td>434</td>\n",
       "      <td>1026</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.105705</td>\n",
       "      <td>0.476559</td>\n",
       "      <td>0.835602</td>\n",
       "      <td>0.103531</td>\n",
       "      <td>0.256015</td>\n",
       "      <td>0.835396</td>\n",
       "      <td>0.476131</td>\n",
       "      <td>0.923220</td>\n",
       "      <td>0.899870</td>\n",
       "      <td>0.008765</td>\n",
       "      <td>0.691802</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.818356</td>\n",
       "      <td>0.255720</td>\n",
       "      <td>0.896250</td>\n",
       "      <td>0.023801</td>\n",
       "      <td>0.751817</td>\n",
       "      <td>0.846724</td>\n",
       "      <td>0.261466</td>\n",
       "      <td>0.645097</td>\n",
       "      <td>0.173824</td>\n",
       "      <td>0.348452</td>\n",
       "      <td>0.533557</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>184</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.201031</td>\n",
       "      <td>0.303372</td>\n",
       "      <td>0.502298</td>\n",
       "      <td>0.366995</td>\n",
       "      <td>0.754150</td>\n",
       "      <td>0.270130</td>\n",
       "      <td>0.811643</td>\n",
       "      <td>0.322071</td>\n",
       "      <td>0.037592</td>\n",
       "      <td>0.338294</td>\n",
       "      <td>0.373525</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.923416</td>\n",
       "      <td>0.532531</td>\n",
       "      <td>0.791524</td>\n",
       "      <td>0.313665</td>\n",
       "      <td>0.763071</td>\n",
       "      <td>0.649208</td>\n",
       "      <td>0.176048</td>\n",
       "      <td>0.956767</td>\n",
       "      <td>0.219100</td>\n",
       "      <td>0.629382</td>\n",
       "      <td>0.667392</td>\n",
       "      <td>50</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      _col0     _col1     _col2     _col3     _col4     _col5     _col6  \\\n",
       "0  0.369659  0.562382  0.268092  0.270976  0.425515  0.436015  0.794521   \n",
       "1  0.105705  0.476559  0.835602  0.103531  0.256015  0.835396  0.476131   \n",
       "2  0.818356  0.255720  0.896250  0.023801  0.751817  0.846724  0.261466   \n",
       "3  0.201031  0.303372  0.502298  0.366995  0.754150  0.270130  0.811643   \n",
       "4  0.923416  0.532531  0.791524  0.313665  0.763071  0.649208  0.176048   \n",
       "\n",
       "      _col7     _col8     _col9    _col10  _col11  _col12  _col13  _col14  \n",
       "0  0.969748  0.118850  0.317950  0.050502       1     434    1026      13  \n",
       "1  0.923220  0.899870  0.008765  0.691802       0       0      23       9  \n",
       "2  0.645097  0.173824  0.348452  0.533557       2      11     184       7  \n",
       "3  0.322071  0.037592  0.338294  0.373525       2       0      10       0  \n",
       "4  0.956767  0.219100  0.629382  0.667392      50       9       1      46  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(\"./data_parquet/val/gen_0.parquet\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bae8840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label',\n",
       " 'I1',\n",
       " 'I2',\n",
       " 'I3',\n",
       " 'I4',\n",
       " 'I5',\n",
       " 'I6',\n",
       " 'I7',\n",
       " 'I8',\n",
       " 'I9',\n",
       " 'I10',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C3',\n",
       " 'C4']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CATEGORICAL_COLUMNS=[\"C\" + str(x) for x in range(1, 5)]\n",
    "CONTINUOUS_COLUMNS=[\"I\" + str(x) for x in range(1, 11)]\n",
    "LABEL_COLUMNS = ['label']\n",
    "cols = LABEL_COLUMNS + CONTINUOUS_COLUMNS + CATEGORICAL_COLUMNS\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b82e2eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>I1</th>\n",
       "      <th>I2</th>\n",
       "      <th>I3</th>\n",
       "      <th>I4</th>\n",
       "      <th>I5</th>\n",
       "      <th>I6</th>\n",
       "      <th>I7</th>\n",
       "      <th>I8</th>\n",
       "      <th>I9</th>\n",
       "      <th>I10</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.369659</td>\n",
       "      <td>0.562382</td>\n",
       "      <td>0.268092</td>\n",
       "      <td>0.270976</td>\n",
       "      <td>0.425515</td>\n",
       "      <td>0.436015</td>\n",
       "      <td>0.794521</td>\n",
       "      <td>0.969748</td>\n",
       "      <td>0.118850</td>\n",
       "      <td>0.317950</td>\n",
       "      <td>0.050502</td>\n",
       "      <td>1</td>\n",
       "      <td>434</td>\n",
       "      <td>1026</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.105705</td>\n",
       "      <td>0.476559</td>\n",
       "      <td>0.835602</td>\n",
       "      <td>0.103531</td>\n",
       "      <td>0.256015</td>\n",
       "      <td>0.835396</td>\n",
       "      <td>0.476131</td>\n",
       "      <td>0.923220</td>\n",
       "      <td>0.899870</td>\n",
       "      <td>0.008765</td>\n",
       "      <td>0.691802</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.818356</td>\n",
       "      <td>0.255720</td>\n",
       "      <td>0.896250</td>\n",
       "      <td>0.023801</td>\n",
       "      <td>0.751817</td>\n",
       "      <td>0.846724</td>\n",
       "      <td>0.261466</td>\n",
       "      <td>0.645097</td>\n",
       "      <td>0.173824</td>\n",
       "      <td>0.348452</td>\n",
       "      <td>0.533557</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>184</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.201031</td>\n",
       "      <td>0.303372</td>\n",
       "      <td>0.502298</td>\n",
       "      <td>0.366995</td>\n",
       "      <td>0.754150</td>\n",
       "      <td>0.270130</td>\n",
       "      <td>0.811643</td>\n",
       "      <td>0.322071</td>\n",
       "      <td>0.037592</td>\n",
       "      <td>0.338294</td>\n",
       "      <td>0.373525</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.923416</td>\n",
       "      <td>0.532531</td>\n",
       "      <td>0.791524</td>\n",
       "      <td>0.313665</td>\n",
       "      <td>0.763071</td>\n",
       "      <td>0.649208</td>\n",
       "      <td>0.176048</td>\n",
       "      <td>0.956767</td>\n",
       "      <td>0.219100</td>\n",
       "      <td>0.629382</td>\n",
       "      <td>0.667392</td>\n",
       "      <td>50</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label        I1        I2        I3        I4        I5        I6  \\\n",
       "0  0.369659  0.562382  0.268092  0.270976  0.425515  0.436015  0.794521   \n",
       "1  0.105705  0.476559  0.835602  0.103531  0.256015  0.835396  0.476131   \n",
       "2  0.818356  0.255720  0.896250  0.023801  0.751817  0.846724  0.261466   \n",
       "3  0.201031  0.303372  0.502298  0.366995  0.754150  0.270130  0.811643   \n",
       "4  0.923416  0.532531  0.791524  0.313665  0.763071  0.649208  0.176048   \n",
       "\n",
       "         I7        I8        I9       I10  C1   C2    C3  C4  \n",
       "0  0.969748  0.118850  0.317950  0.050502   1  434  1026  13  \n",
       "1  0.923220  0.899870  0.008765  0.691802   0    0    23   9  \n",
       "2  0.645097  0.173824  0.348452  0.533557   2   11   184   7  \n",
       "3  0.322071  0.037592  0.338294  0.373525   2    0    10   0  \n",
       "4  0.956767  0.219100  0.629382  0.667392  50    9     1  46  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.set_axis(cols, axis=1,inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4504a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./hps_model/infer_test.csv', sep=',', index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecf4d38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5b8fd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hps_train2predict.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hps_train2predict.py\n",
    "\n",
    "# validation\n",
    "from hugectr.inference import InferenceParams, CreateInferenceSession\n",
    "import hugectr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from mpi4py import MPI\n",
    "\n",
    "def demo_inference(model_name, network_file, dense_file, embedding_file_list, data_file,enable_cache):\n",
    "    # CATEGORICAL_COLUMNS=[\"C1_C2\",\"C3_C4\"]+[\"C\" + str(x) for x in range(1, 5)]\n",
    "    CATEGORICAL_COLUMNS=[\"C\" + str(x) for x in range(1, 5)]\n",
    "    CONTINUOUS_COLUMNS=[\"I\" + str(x) for x in range(1, 11)]\n",
    "    LABEL_COLUMNS = ['label']\n",
    "    \n",
    "    emb_size = [10000, 10000, 10000, 10000]\n",
    "    shift = np.insert(np.cumsum(emb_size), 0, 0)[:-1]\n",
    "    \n",
    "    test_df = pd.read_csv(data_file,sep=',')\n",
    "    config_file = network_file\n",
    "    \n",
    "    # row_ptrs = list(range(0,21))+list(range(0,261))\n",
    "    row_ptrs = list(range(0,15))\n",
    "    \n",
    "    dense_features =  list(test_df[CONTINUOUS_COLUMNS].values.flatten())\n",
    "    test_df[CATEGORICAL_COLUMNS].astype(np.int64)\n",
    "    embedding_columns = list((test_df[CATEGORICAL_COLUMNS]+shift).values.flatten())\n",
    "\n",
    "    # create parameter server, embedding cache and inference session\n",
    "    inference_params = InferenceParams(model_name = model_name,\n",
    "                                max_batchsize = 64,\n",
    "                                hit_rate_threshold = 0.9,\n",
    "                                dense_model_file = dense_file,\n",
    "                                sparse_model_files = embedding_file_list,\n",
    "                                device_id = 0,\n",
    "                                use_gpu_embedding_cache = enable_cache,\n",
    "                                cache_size_percentage = 0.9,\n",
    "                                i64_input_key = True,\n",
    "                                use_mixed_precision = False\n",
    "                                )\n",
    "    inference_session = CreateInferenceSession(config_file, inference_params)\n",
    "    # TODO: check VSCR example for hugectr inference\n",
    "    # https://gitlab-master.nvidia.com/dl/hugectr/hugectr_inference_backend/-/blob/main/docs/architecture.md#vcsr-example\n",
    "    output = inference_session.predict(dense_features, embedding_columns, row_ptrs)\n",
    "    print(\"HPS demo multi-embedding table inference result is {}\".format(output))\n",
    "\n",
    "# def demo_lookup(model_name, network_file, dense_file, embedding_file_list, data_file,enable_cache):\n",
    "#     # CATEGORICAL_COLUMNS=[\"C1_C2\",\"C3_C4\"]+[\"C\" + str(x) for x in range(1, 5)]\n",
    "#     CATEGORICAL_COLUMNS=[\"C\" + str(x) for x in range(1, 5)]\n",
    "#     CONTINUOUS_COLUMNS=[\"I\" + str(x) for x in range(1, 11)]\n",
    "#     LABEL_COLUMNS = ['label']\n",
    "    \n",
    "#     emb_size = [10000, 10000, 10000, 10000]\n",
    "#     shift = np.insert(np.cumsum(emb_size), 0, 0)[:-1]\n",
    "#     test_df = pd.read_csv(data_file,sep=',')\n",
    "#     config_file = network_file\n",
    "    \n",
    "# #     row_ptrs = list(range(0,21))+list(range(0,261))\n",
    "#     row_ptrs = list([0,2,4])\n",
    "    \n",
    "#     dense_features =  list(test_df[CONTINUOUS_COLUMNS].values.flatten())\n",
    "#     test_df[CATEGORICAL_COLUMNS].astype(np.int64)\n",
    "#     embedding_columns = list((test_df[CATEGORICAL_COLUMNS]+shift).values.flatten())\n",
    "\n",
    "#     # create parameter server, embedding cache and inference session\n",
    "#     inference_params = InferenceParams(model_name = model_name,\n",
    "#                                 max_batchsize = 64,\n",
    "#                                 hit_rate_threshold = 0.9,\n",
    "#                                 dense_model_file = dense_file,\n",
    "#                                 sparse_model_files = embedding_file_list,\n",
    "#                                 device_id = 0,\n",
    "#                                 use_gpu_embedding_cache = enable_cache,\n",
    "#                                 cache_size_percentage = 0.9,\n",
    "#                                 i64_input_key = True,\n",
    "#                                 use_mixed_precision = False\n",
    "#                                 )\n",
    "#     inference_session = CreateInferenceSession(config_file, inference_params)\n",
    "#     # TODO: check VSCR example for hugectr inference\n",
    "#     # https://gitlab-master.nvidia.com/dl/hugectr/hugectr_inference_backend/-/blob/main/docs/architecture.md#vcsr-example\n",
    "#     output = inference_session.predict(dense_features, embedding_columns, row_ptrs)\n",
    "#     print(\"HPS demo multi-embedding table inference result is {}\".format(output))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    model_name = sys.argv[1]\n",
    "    network_file = sys.argv[2]\n",
    "    dense_file = sys.argv[3]\n",
    "    embedding_file_list = str(sys.argv[4]).split(',')\n",
    "    print(embedding_file_list)\n",
    "    data_file = sys.argv[5]\n",
    "  \n",
    "\n",
    "    #demo_inference(model_name, network_file, dense_file, embedding_file_list, data_file, True,hugectr.Database_t.Redis)\n",
    "    demo_inference(model_name, network_file, dense_file, embedding_file_list, data_file, True)\n",
    "    #demo_inference(model_name, network_file, dense_file, embedding_file_list, data_file, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "271aa40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./hps_model/hps_train0_sparse_1000.model', './hps_model/hps_train1_sparse_1000.model']\n",
      "[HCTR][08:00:50.397][WARNING][RK0][main]: default_value_for_each_table.size() is not equal to the number of embedding tables\n",
      "[HCTR][08:00:50.397][INFO][RK0][main]: default_emb_vec_value is not specified using default: 0\n",
      "[HCTR][08:00:50.397][INFO][RK0][main]: default_emb_vec_value is not specified using default: 0\n",
      "[HCTR][08:00:50.397][INFO][RK0][main]: Creating ParallelHashMap CPU database backend...\n",
      "[HCTR][08:00:50.397][INFO][RK0][main]: Created parallel (16 partitions) blank database backend in local memory!\n",
      "[HCTR][08:00:50.397][INFO][RK0][main]: Volatile DB: initial cache rate = 1\n",
      "[HCTR][08:00:50.397][INFO][RK0][main]: Volatile DB: cache missed embeddings = 0\n",
      "[HCTR][08:00:50.406][INFO][RK0][main]: Table: hps_et.hps_train.sparse_embedding1; cached 18502 / 18502 embeddings in volatile database (ParallelHashMap); load: 18403 / 18446744073709551615 (0.00%).\n",
      "[HCTR][08:00:50.410][INFO][RK0][main]: Table: hps_et.hps_train.sparse_embedding2; cached 18471 / 18471 embeddings in volatile database (ParallelHashMap); load: 18432 / 18446744073709551615 (0.00%).\n",
      "[HCTR][08:00:50.410][DEBUG][RK0][main]: Real-time subscribers created!\n",
      "[HCTR][08:00:50.410][INFO][RK0][main]: Create embedding cache in device 0.\n",
      "[HCTR][08:00:50.418][INFO][RK0][main]: Use GPU embedding cache: True, cache size percentage: 0.900000\n",
      "[HCTR][08:00:50.418][INFO][RK0][main]: Configured cache hit rate threshold: 0.900000\n",
      "[HCTR][08:00:50.568][WARNING][RK0][main]: MPI was already initialized somewhere elese. Lifetime service disabled.\n",
      "[HCTR][08:00:50.568][INFO][RK0][main]: Global seed is 542666420\n",
      "[HCTR][08:00:50.719][INFO][RK0][main]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 3\n",
      "[HCTR][08:00:51.929][WARNING][RK0][main]: Peer-to-peer access cannot be fully enabled.\n",
      "[HCTR][08:00:51.929][INFO][RK0][main]: Start all2all warmup\n",
      "[HCTR][08:00:51.931][INFO][RK0][main]: End all2all warmup\n",
      "[HCTR][08:00:51.932][INFO][RK0][main]: Create inference session on device: 0\n",
      "[HCTR][08:00:51.932][INFO][RK0][main]: Model name: hps_train\n",
      "[HCTR][08:00:51.932][INFO][RK0][main]: Use mixed precision: False\n",
      "[HCTR][08:00:51.932][INFO][RK0][main]: Use cuda graph: True\n",
      "[HCTR][08:00:51.932][INFO][RK0][main]: Max batchsize: 64\n",
      "[HCTR][08:00:51.932][INFO][RK0][main]: Use I64 input key: True\n",
      "[HCTR][08:00:51.932][INFO][RK0][main]: start create embedding for inference\n",
      "[HCTR][08:00:51.932][INFO][RK0][main]: sparse_input name data1\n",
      "[HCTR][08:00:51.932][INFO][RK0][main]: sparse_input name data2\n",
      "[HCTR][08:00:51.932][INFO][RK0][main]: create embedding for inference success\n",
      "[HCTR][08:00:51.932][INFO][RK0][main]: Inference stage skip BinaryCrossEntropyLoss layer, replaced by Sigmoid layer\n",
      "Traceback (most recent call last):\n",
      "  File \"hps_train2predict.py\", line 93, in <module>\n",
      "    demo_inference(model_name, network_file, dense_file, embedding_file_list, data_file, True)\n",
      "  File \"hps_train2predict.py\", line 44, in demo_inference\n",
      "    output = inference_session.predict(dense_features, embedding_columns, row_ptrs)\n",
      "RuntimeError: Runtime error: The dimension of dense features is not consistent\n",
      "\tError_t::WrongInput at predict_(/hugectr/HugeCTR/include/pybind/inference_wrapper.hpp:158)\n"
     ]
    }
   ],
   "source": [
    "!python hps_train2predict.py \\\n",
    "    \"hps_train\" \\\n",
    "    \"./hps_model/hps_train.json\" \\\n",
    "    \"./hps_model/hps_train_dense_1000.model\" \\\n",
    "    \"./hps_model/hps_train0_sparse_1000.model,./hps_model/hps_train1_sparse_1000.model\" \\\n",
    "    \"./hps_model/infer_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72933e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4037ac20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fc54c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
